
Type,Requirement PE,The system shall refresh the display every 60 seconds.LF,The application shall match the color of the schema set forth by Department of Homeland Security US, If projected the data must be readable.On a 10x10 projection screen 90% of viewers must be able to read Event / Activity data from a viewing distance of 30 A, The product shall be available during normal business hours.As long as the user has access to the client PC the system will be available 99% of the time during the first six months of operation.US, If projected the data must be understandable.On a 10x10 projection screen 90% of viewers must be able to determine that Events or Activities are occuring in current time from a viewing distance of 100 SE,The product shall ensure that it can only be accessed by authorized users.The product will be able to distinguish between authorized and unauthorized users in all access attempts US,The product shall be intuitive and self-explanatory.
PE,The product shall respond fast to keep up-to-date data in the display. F,The system shall have a MDI form that allows for the viewing of the graph and the data table.F,The system shall display Events in a vertical table by time.F,The system shall display the Events in a graph by time.L,All business rules specified in the Disputes System shall be in compliance to the guidelines of Regulation E and Regulation Z.L,The Disputes application must maintain a detailed history of every action that a user takes on a dispute case.This ensures a complete audit trail if questions arise later on with regard to a particular dispute case.L,All actions that modify an existing dispute case must be recorded in the case history.F,The Disputes System must be accessible by both internal and external users.F,The Disputes System must prevent users from accessing any dispute cases that do not belong to their cardholder base.
F,The Disputes System will facilitate direct data entry of a dispute case via a user interface that supports real time responses to the users. F,The Disputes System must provide different levels of access with regard to disputes case initiation and follow-up actions.F,The Disputes System shall provide view access capability for authorized users of the application.F,The Disputes System shall provide update access capability for authorized users of the application.F,The Disputes System must allow the users to select disputable transactions (based on the age of the transaction) from a user interface and initiate a dispute (ticket retrieval request or chargeback notification) on the selected transaction.F,The Disputes System must provide the user the ability to initiate a single dispute case on multiple transactions that belong to a single merchant.F,The Disputes System will provide the user the ability to create or initiate a ticket retrieval request.
As part of ticket retrieval creation process the system must prompt the user to enter all the required information to create the ticket retrieval request. The ticket retrieval request is a document that is sent to merchant inquiring the validity of a transaction.F,The Disputes System must allow the user to create three unique types of ticket retrieval requests.The three types of ticket retrieval requests are (1) Request for original receipt (2) Request for a copy of the receipt or (3) Request for a portfolio.A portfolio consists of documentation that would provide proof of a purchase such as the documentation that is received from a car rental agency that is more than a sales receipt.F,The Disputes System must prevent external users from requesting original receipts.Requests for original receipts are restricted to internal users.
F, The Disputes System must provide a confirmation to the user upon the creation of ticket retrieval request that contains the following information; the dispute case number the type of retrieval requested (copy original or portfolio) and the date that the merchant response is due. F,The Disputes System shall allow the user to create or initiate a chargeback request.The system must prompt the user to enter the required information to initiate a chargeback request.The chargeback request results in a printed chargeback notification that is sent to merchant indicating that their account will be charged back if no proof is provided that the disputed transaction is valid.F, The Disputes System must provide a confirmation to the user upon the creation of a chargeback request.The confirmation must contain the following information; the dispute case number the type of chargeback requested (pending or immediate) and the date that the merchant response is due.
F,The Disputes System shall prevent the creation of duplicate dispute requests 100% of the time. F, The Disputes System shall accurately provide a list of possible reason codes for a given transaction.The list of possible reason codes must be derived based on the type of transaction and the industry that the merchant belongs to according to the business rules defined for the Disputes System.F,The Disputes System must provide the ability to view existing dispute cases.F, The Disputes System must provide Search functionality.The search method must include the ability to search by (1) the dispute case number (2) the merchant account number (3) the cardmember account number and (4) the issuer number.In addition to the above criteria the search functionality must further allow the user to limit the results of the search by a date range the type of dispute (ticket retrieval request or chargeback notification) the case status (open closed or all) and the dispute reason code.
F,A list of all disputes cases that meet the entered search criteria must be provided to the user. The list of cases must differentiate the type and/or status of the case by color -coding the line item in the list.For example all open cases will be yellow and all closed cases will be gray.F,The Disputes System shall provide the users the ability to view the history of all activities that have occurred on the dispute case.F,The Dispute System shall provide the users with the ability to follow up on a dispute case via follow-up actions.The list of available follow up actions for a given dispute case must be filtered by the status of the case and the access level of the user.F, The Disputes System shall record the name of the user and the date for any activity that creates or modifies the disputes case in the system.A detailed history of the actions taken on the case including the date and the user that performed the action must be maintained for auditing purposes.
F, For any systematic (non-user initiated) action that occurs on a case such as the disputes aging process a case activity will be generated. Upon the next logon the user that initiated the dispute case must be notified that a systematic action has occurred on the dispute case.F,All letter requests must be formatted according to guidelines specified by the Print Letter Utility system.F,Any disputes cases that have been closed for over 6 months must be purged from the online disputes database.O,The product must support Internet Explorer 5.5 and above.O,The product must support Mozilla Firefox 1.0 and above.O,The product shall interface with the Choice Parts System.This provides the feed of recycled parts data.O,The product shall run on the existing hardware for all environments.
O,The product shall adhere to the corporate Architecture guidelines LF,The product shall comply with corporate User Interface Guidelines LF,The product shall comply with corporate color scheme LF,The appearance of the product shall appear professional PO,The software product is expected to run on Windows or Linux platforms. US,The product shall be easy to use by Adjusters and Collision Estimators.95% of Adjusters and Collision Estimators shall find the product easy to use.US,The product shall increase productivity of Collision Estimators.80% of the Collision Estimators shall agree their productivity has increase within 1 month of using the product.US,Users shall feel satisfied using the product.85% of all users will be satisfied with the product.US,The product shall be easy to learn by Adjusters and Collision Estimators.The product shall be learned with two days onsite training US,The user shall easily locate instructions while using the product.
User help can be found within 90% of the system. PE,The search for recycled parts shall take no longer than 15 seconds.The search results shall be returned in under 15 seconds.PE,The search for the preferred repair facility shall take no longer than 8 seconds.The preferred repair facility is returned within 8 seconds PE,The recycled parts audit report shall be returned to the user within 10 seconds.The audit report shall be returned within 10 seconds.PE,The preferred repair facility ratings shall be saved within 5 seconds.The save shall occur within 5 seconds.A,The product shall adhere to the corporate online availability schedule.The application is brought down only within 98% of the scheduled outages per the availability schedule.A,The product shall achieve a 98% uptime.The product shall not fail more than 2% of the available online time.SC,The product shall be able to handle up to 1500 simultaneous users.The system shall perform with 1500 users as a time.
SC, The average number of recycled parts records per day shall be 50 000. SC, The product shall be able to handle 10 000 concurrent users within 2 years of the initial launch.O, For estimators the product shall be able to be operated in a repair facility during dirty and noisy conditions.O,The product shall interface with the ChoiceParts system once per day at approximately 1 O,The maintenance of the product will fall within the established corporate maintenance windows.O,The product shall be supported using the corporate support center.O,Product installations and upgrades shall be handled by the middleware technology team within established launch time frames.SE,Only adjusters can request recycled parts audit reports.No users without an adjuster role shall request recycled parts audits.SE,Only adjusters with a supervisor role can update preferred repair facility ratings.Users without the supervisor role cannot access the repair facility ratings.
SE,Only collision estimators shall search for recycled parts. Users without the collision estimator role shall not have access to the search for recycled parts.SE,Only valid data shall be entered into the system.No invalid data shall be entered into the system.SE,One insurance company shall not be able to view another insurance companys claim data.SE,The product shall free of computer viruses.SE,The system shall prevent malicious attacks including denial of service.L,The product must comply with Sarbanes-Oxley.L,The product shall comply with the estimatics laws relating to recycled parts usage.L,The product shall comply with insurance regulations regarding claims processing.
F, The user shall search for the preferred repair facility using vehicle vehicle location and radius in miles F, The vehicle data shall include vehicle year make and model F, The vehicle location shall include street address city state and zipcode F, The system shall resolve the zipcode for the street address city and state if zipcode is unknown. F,The search radius shall be between 1 and 30 miles F,The system shall locate the preferred repair facility with the highest ratings for the input criteria F,The adjuster shall enter the preferred repair facility on the estimate assignment.F,The adjuster shall be able to override the preferred repair facitlity on the estimate assignment.F,The preferred repair facility shall be returned to the user.F,The system shall return a list of repair facilities within the radius if the preferred repair facility cannot be determined.F,The estimator shall search for available recycled parts using damaged vehicle parts information.
F,The estimator shall search for available recycled parts using a list of preferred parts suppliers. F,The system shall search for available recycled parts for the supplied vehicle parts and suppliers.F,The estimator shall search by zipcode with a radius of 30 miles if no parts are found for the preferred parts suppliers F,The system shall retain the available recycled parts and suppliers returned from the search.F,The available recycled parts information and their suppliers shall be returned to the user.F,The estimator shall apply selected recycled parts to the collision estimate.F,The estimator shall not apply recycled parts to the collision estimate if no available parts are returned.F,The adjuster shall review the collision estimate.F,The adjuster shall request a recycled parts audit of the collision estimate.F,The recycled parts search results provided to the estimator shall be retrieved by the system.
F,The system shall generate an audit report based on the available recycled parts and the collision estimate. F,The audit report shall include the total number of recycled parts used in the estimate.F,The audit report shall include the number of available recycled parts from the original search results.F, The audit report shall include the percentage of available recycled parts used in the estimate F,The audit report shall include the total score of the audit which sums the indivual line items.F,The audit report shall be displayed to the user.F,The audit report shall be available to other adjusters at later points in time.F,The user shall select to view the preferred repair facility ratings.F,The current repair facility ratings shall be displayed to the user.F,The system will display a blank set of ratings if there are not ratings yet defined.
F,The user shall enter new ratings based on the audit report F,The ratings shall be from a scale of 1-10 F,The ratings shall include categories for attempted use of recycled parts and actual use of recycled parts F,The user shall select to save the preferred repair facility ratings. F,The preferred repair facility ratings shall be saved by the system.O, The product must work with most database management systems (DBMS) on the market whether the DBMS is colocated with the product on the same machine or is located on a different machine on the computer network.O,The product will function alongside server software on any operating system where the Java runtime can be installed.O,The product will require collaboration with a database management system (DBMS).The DBMS may be located on the same machine as the product or on a separate machine residing on the same computer network.O, The product must make use of web/application server technology.
Open source examples include Apache web server Tomcat and the JBoss application server. O, A database management system such as Oracle DB2 MySql or HSQL will need to be integrated with the product.LF, The product s Look and Feel shall be able to incorporate aspects of the customer s organization such as logo branding and identity LF,The product shall have a conservative and professional appearance.US,The product shall make the users want to use it.80% of the users surveyed report they are regularly using the product after the first 2 weeks post-launch.US,The product shall be easy to use.90% of users will be able to successfully reserve a conference room within 5 minutes of product use.US,The product shall give users feedback when necessary.80% of the users surveyed report that the product accurately confirms their actions.US,The product shall allow the user to select a chosen language from one of the target market countries.
US,The product shall allow for customization of start page and views preferences. US, An employee will be able to successfully use the product within a few minutes.After informally navigating the product for less than 15 minutes users shall be able to successfully setup meetings and reserve conference rooms.US,The product shall use a standard navigation menu familiar to most web users.US,The product shall allow for intuitive searching of available conference rooms.US,The product shall conform to the Americans with Disabilities Act.PE,The response shall be fast enough to avoid interrupting the user s flow of thought.90% of tasks shall complete within 5 seconds.98% of tasks shall complete within 8 seconds.A, Aside from server failure the software product shall achieve 99.99% up time.FT,The product shall create an exception log of problems encountered within the product for transmission to our company for analysis and resolution.
SC, The product shall be capable of handling up to 1 000 concurrent requests. This number will increase to 2 000 by Release 2.The concurrency capacity must be able to handle peak scheduling times such as early morning and late afternoon hours.SC, The product shall be able to process 10 000 transactions per hour within two years of its launch.This number will increase to 20 000 by Release 2.SC,The product shall be expected to operate for at least 5 years for each customer installation.O,The product shall be used in office environments.O,The product must be able to interface with any HTML browser.The product shall transmit data between the user and the product without problems.The product shall display HTML properly in 80% of all HTML browsers tested (minimum is to test 8 browsers).O,The product must be able to interface with various database management systems.The product shall communicate successfully with the database management system on 100% of all transactions.
O,The product must be able to interface with various email servers. The product shall be able to send email.O,The product shall be available for distribution via the Internet as a binary or ZIP file.O,The product shall be available for distribution as a packaged CD.O, The product will be available for licensing as a one-server two-five servers or five-or-more servers license.US,A new user must be able to be added to the system within 10 minutes.90% of new users are able to log into the system within 10 minutes.MN,The product must be designed using Design Patterns and coding best practices.90% of maintenance software developers are able to integrate new functionality into the product with 2 working days.MN,The product must be highly configurable for use with various database management systems for the end users.80% of end users are able to integrate new database management systems with the product without changing the product s software code.
MN,Maintenance releases will be offered to customers once a year. The releases shall take no longer than 5 minutes for 80% of the customer base to install.US,Every registered user will have access to the product s support site via the Internet.70% of registered users shall find a solution to their problem within 5 minutes of using the support site.MN,The customer shall be able to easily integrate new building maps with the product throughout the product s lifecycle.Integration of new maps with the product shall be possible with little to no support from product support staff.US, The product shall be translated into foreign languages other than the target market countries languages in future releases.For each emerging target market it shall take no more than 5 days to configure the product for that market s language.O, The product is expected to integrate with multiple database management systems.The product will operate with Oracle SQL Server DB2 MySQL HSQL and MS Access.
O,The product shall be able to be installed in any operating environment within 2 days. SE,The product shall ensure that only company employees or external users with company-approved user IDs may have product access.100% of all logons shall be by either company employees or external users with company-approved user IDs.SE,Only managers are able to perform search queries for reservations by user.100% of all search queries for reservations by user shall be from logons of only management logons.SE,The product shall ensure that the database s data corresponds to the data entered by the user.Each conference room reservation in the system will contain the same information as was entered by the user.SE,The product shall prevent the input of malicious data.The product and/or adjacent hardware/software systems data shall remain 100% uncorrupted each time malicious data is input into the product.
FT,The product and/or dependent databases/filesystems shall remain operational as a result of the input of malicious data. The product and/or dependent databases/filesystems shall not crash 100% of the time that malicious data is input into the product.SE,The product shall have the ability to receive automatic software updates as new threats emerge.100% of customers will be able to receive automatic software updates transmitted to the installed product.SC,The language of the product shall accommodate all of the target market countries.90% of survey respondents from each target market country will find that the language of the product is acceptable.US,The product shall be able to display calendar dates and times according to the user s locale.90% of survey respondents from each target market country will find that the date and time formatting of the product is correct.
O,The product must be developed with the J2SE/J2EE programming language libraries F,The product shall record meeting entries. F,The product will notify employees of meeting invitations.F,The product shall have the ability to send meeting reminders to employees.F,The product shall assign the organizers contact information to each meeting they create.F,The product will record meeting acknowledgements.F,The product shall store new conference rooms.F,The product shall update existing conference rooms.F,The product will be able to delete conference rooms.F,The product shall be able to store new room equipment.F,The product will update existing room equipment.F,The product shall be able to delete room equipment.F,The product shall allow an organizer to invite other employees to meetings.F, Each time a conference room is reserved the conference room schedule shall be updated to reflect the time and date of the reservation.
F,The product shall record the transportation status of equipment reserved. F,The product shall display a map of the company building showing conference room locations.F,The product shall record updated meeting agendas.F,The product shall send a meeting confirmation to the meeting organizer.F,The product shall display room equipment according to search parameters.F,The product shall display conference rooms according to search parameters.F,The product shall record different meeting types.F,The product shall record all the equipment that has been reserved.F,The product shall notify building personnel of equipment transport requests.F,The product will allow priviledged users to view meeting schedules in multiple reporting views.F,The product shall be able to send meeting notifications via different kinds of end-user specified methods.F,The product shall have a customizable Look and Feel.F,The product shall have an intuitive user interface.
F,The product will display an available status for unreserved conference rooms. PE,Recreation of Inventory Quantity Adjustment for the period of previous 30 days shall take no more than 30min.PE,The System shall allow for a minimum of 6 users to work at the same time.O,The System shall utilize currently owned computer equipment.O,The System shall be able to automatically backup business data and restore them on demand.SE,The System shall maintain consistent security.Any changes in user passwords and/or permissions should take affect in all parts of the System.SE,The System shall granularly control access rights to all major subsystems.It should be possible to assign access rights to all subsystems independently.L,The System shall meet all applicable accounting standards.The final version of the System must successfully pass independent audit performed by a certified auditor.O,The System shall not require additional third party licenses resulting in royalty fees.
F,The System shall allow generation of Inventory Quantity Adjustment documents on demand. F,The System shall allow on demand generation of all Inventory Quantity Adjustment documents since certain point of time.F,The IQA subsystem shall take into account current inventory status and adjust inventory of substitutionary ingredients accordingly.F, The System shall allow entering storing and modifying product formulas.F, Product formulas shall allow use of semi-products which have own formulas.F,Product formula ingredients shall allow defining substitutionary ingredients.F,Product formula substitutionary ingredients shall be entered automatically by the PFE subsystem.F,Formula substitutionary ingredients shall be kept in persisted order F, Product shall allow entering storing and modifying product menus used by POS terminals.F,PME Subsystem shall allow building product menus based on the list of products kept in the accounting system.
F,PME Subsystem shall allow keeping submenus within menus and products within submenus in a managed and persisted order. F,PME Subsystem shall allow for a product to exist in more than one submenu F,The RMS System shall have Windows XP look and feel and use GUI futures making its use intuitive.F,The System shall generate Inventory Quantity Adjustment document based on product daily sales data and product formulas.F,The System shall generate Inventory Quantity Adjustment document automatically when daily Product Sales data is available.PE,Administrator shall be able to activate a pre-paid card via the Administration section in under 5 seconds.PE, Customer shall be able to check the status of their prepaid card by entering in the PIN number in under 5 seconds.PE,System shall let customers register on the website as a pay as you go user in under 5 minutes.PE,System shall let customers register on the website as a unlimited movie subscriber user in under 10 minutes.
PE,System shall let existing customers log into the website with their email address and password in under 5 seconds. PE, System shall let customers cancel their account process shall take no longer than 2 minutes.Database will mark the account as inactive and user will not be able to log into the website.PE,System shall let administrator de-activate a customer account in under 1 minute.Customer will no longer be able to access the website.PE,System shall let administrator add/remove movies on the website in under 5 minutes.Entered movie information will be stored in the database and will now be available on the website.PE,System shall let the administrator add/remove categories on the website in under 5 minutes.PE,System shall let Izogn Manager access sales and usage statistics in under 2 minutes.PE,System shall let administrators generate unique pre-paid pins in under 2 minutes and generate an Excel file with the generated PINs.LF,The website shall be attractive to all audiences.
The website shall appear to be fun and the colors should be bright and vibrant. LF, The website should have an African feel but should not alienate non-Africans.The website should use animation on pages which are describing the services to grab the users attention and encourage them to sign up.LF, The website design should be modern clean and concise.US,The product should be able to be used by 90% of novice users on the Internet.US, Users should only have to navigate through a maximum of 4 pages to create an account with the website.US,Users should be able to access their streaming movies in under 2 clicks after logging into the website.US,An anonymous survey will show that 70% of customers are comfortable purchasing streaming movies after 2 weeks using the website.US,An anonymous survey will show that 95% of customers are comfortable using and purchasing pre-paid cards after a month of using the system.US,The product shall retain the customer s buying preferences and information.
US,The product shall be easy for customers with novice skills on the Internet to purchase and watch streaming movies. Customers should be able to purchase and watch a pre-determined movie in under 3 minutes.US,The product shall use symbols and words that are naturally understandable by users familiar with websites US,The website will use standard Internet conventions when interacting with the user.PE, Any interface between a user and the automated system shall have a maximum response time of 5 seconds unless noted by an exception below.PE, When purchasing a streaming movie or pre-paid card via credit card the processing time should have a maximum response time of 15 seconds.PE, When streaming a movie the buffering time should take no longer than 10 seconds (plus any latency on the user s connection.) PE, While streaming a movie buffering should only occur if there is latency in the user s connection.
SE, All credit card information will be secured on the server and only accessible by authorized Izogn administrators. Information will be encrypted in the database.A, The website shall be available for use 24 hours per day 365 days per year.A,The website shall achieve 99.5% up time.A, All movies shall be streamed on demand at any time of the day.FT,The website shall continue to operate if the streaming server goes down.FT,The website shall continue to operate if the payment gateway goes down.SC,The streaming server will support 50 simultaneous streaming connections.SC,The streaming server will support a maximum of 700 movies; new servers can be added to support more movies.SC, The server will support a maximum of 1 000 simultaneous users.SC, The website shall be capable of supporting 100 000 customers.MN,The product shall be expected to operate within the maximum maintenance budget for a minimum of 2 years.
O,All aspects of the website shall be accessed by a web browser over the Internet. O,The same hardware supporting the current website will be used to support the new system.O, We must be able to interface with the following browsers O,Website must interface with the CyberSource API to process credit card transactions.O,Website must be fully operational with MSN TV2.O,Movies shall be encoded and streamed via Windows Media Player 9.0.O,Movies shall be encoded for 56k and 256k streams.MN,Promotional updates to the website should take a day to update.PE,Izogn Administrator must be able to update the category listings on the website within 2 minutes.PE,Izogn Administrator must be able to add new products to the website within 2 minutes.A,The system will provide a 24 hour 800 toll free number for support to its customers.US,The website will provide a help section with answers and solutions to common problems.
A, Velkat staff will be available for phone support between 7am-7pm M-F to provide technical support to the Izogn administrators. O,The product shall be able to be installed in the specified environment within 1 working day.SE,Only registered customers can purchase streaming movies.SE, A customer can only stream a movie if they purchased it and are within the 2 day viewing period.SE,Only authorized personnel can access customer records in the database.SE,Only administrators can activate a pre-paid card.SE,Only authorized personnel can access sales information.SE,The website shall prevent its data from being corrupted when incorrect data is entered in.SE, The website shall protect itself from intentional abuse and notify the administrator at all occurrences.SE,Data integrity scripts will be run on a weekly basis to verify the integrity of the database.
SE, The website shall make its user aware of its information practices before collection data from them via a Privacy Policy accessible on all pages of the website. SE,The system shall notify customers of changes to its information policy.SE,The system shall reveal private information only in compliance with the organization s information policy.SE, All customer information will be stored on a secure database accessible only to authorized personnel.SE,All credit card information will be encrypted in the database.SE,The streaming server will have anti-virus software to prevent infection of malicious viruses.LF,The website should cater to all tribes in Nigeria.LF,The website shall not be offensive to religious or ethnic groups.LF, The website should appeal to all Africans not only Nigerians.O,The website should use Flash to provide animation on the home page.L,IzognMovies shall meet the licensing requirements set forth by the appropriate organizations in order to stream African movies.
L,The website will comply with W3C standards. L,The PHP code will comply with PEAR standards.F, The website will allow customers to search for movies by title actor or director.F, viewing a movie details the website will display the movies description actor and director entered in by the administrator.F, Website shall request and authorize credit card payment before allowing a user to stream a movie.F, shall be able to stream purchased movie via Windows Media Player for up to 2 days aftere purchase date.F,Website shall allow customers to browse movies by categories entered in by izogn administrator.F,Website shall allow customers to view reviews of selected movies by other customers.F,Website shall allow customers to add their own movie review for a selected movie.F,Website shall allow the Izogn administrator to approve a review posted by a customer.F,System will allow customers to pay for a streaming movie with a pre-paid card.
F, Website shall allow customers to purchase pre-paid cards of $5 $10 or $20 value either by credit card or mail-in payment option. F, System shall automatically update the main page of the website every Friday and show the 4 latest movies that have been added to the website.F,System shall allow users to update their billing and contact information via the My Account section of the website.F, Pre-paid cards shall be distributed via email or via a printed paper card.F,Only registered customers can purchase streaming movies.F, Movies can only be streamed if the customer has purchased a movie and is within the 2-day time period to stream the movie.F,Website must be able to support free trial periods with various parameters set by the Izogn Manager.F, All streaming movie sales will be logged in the database and accessible by the administrator.F, All registered customer information shall be stored and maintained in the database even after the customer cancels their account.
F, Daily usage statistics should be logged and accessible by the administrator. F,The owner shall have free access to all of the streaming movies.PE,Total lead cycle time must be completed in 5 minutes for 90% of lead volume between the business hours of 8AM CST to 8PM CST; then 3 minutes for 99% of lead volume during off business hours.PE,No lead evaluation will complete in a time period longer than 25 seconds for 97% of all leads processed.PE,For leads that process longer than 25 seconds the system will record the event and duration PE,The process must fit within the processing time conditions set forth by the Auto Rep Assignment process in which a lead is assigned within 3 minutes of the total life cycle.O,The lead washing process will operate within the established physical web service structure.O, The washing process must use a non binding neutral data format such as xml when receiving and processing lead data.
A,Defect reports will be available to technical units on a 24x7 basis. MN,Washing parameters will be updated during scheduled maintenance hours.F,The vendor will verify the invalid lead.F,Vendor will submit a credit validation record to be applied to the enterprise billing system.F,If a lead is scored to the high range the system will supply the lead data and score to the enterprise system.F,The leads washing functionality will validate all leads received by the web service for valid data F,The leads washing functionality will insert all leads captured by the web service.F,If the leads score falls within the high average score then it will be process by the ARA functionality.F,If the leads score falls within the medium average then it will be set for manual verification by an Enrollment Coordinator through the eleads system.
F,If the leads score falls within the low average then it will be returned to the supplying vendor F,The leads washing functionality will compile both contact and academic scoring information for a total score. F,The leads washing functionality will use the parameters supplied by corporate and schools to score part of the lead.F,The leads washing functionality will return the lead data supplied to the vendor along with the reason of rejection.F,The leads washing functionality will have an interface in which lead data parameters can be maintained.F,The leads washing functionality will verify the users authentication and rights to make parameter updates.F,The leads washing functionality will store any potential lead duplicates returned by the enterprise system.F,Prior to submitting the lead to the enterprise system the leads washing functionality will assign the lead to an admissions rep.
F,The leads washing functionality shall receive a period list of available reps from the enterprise system. LF,The product shall simulate the look of ships at sea.LF,The product shall display the grids within a circle as a view from a periscope.LF,The product shall display each type of ship in a grid using an image of the specific type of ship.LF, When the offensive player takes a shot the product shall simulate the sound of a ship at sea.LF, When a shot is marked as a miss the product shall simulate the sound of a shot missing its target.LF, When a shot is marked as a hit the product shall simulate the sound of a shot hitting its target.LF, When a ship is sunk the product shall simulate the sound of a sinking ship.A, The product shall be available 24 hours per day seven days per week.PE,The product shall able to handle a large number of games simultaneously.PE,The response time shall be fast enough to maintain the flow of the game.
The response time shall be no more than 2 seconds for 95% of responses and no more than 4% for the remaining responses. PE,The product shall protect the identity of the players.The product shall provide players no access to information that might reveal the identity of another player.US,The product shall be intuitive and self-explanatory.Nine out ten users shall be able to successfully complete a game with no training.US, The product shall be available to players who understand English Spanish and French.The product shall allow the user to select the language upon accessing the product.SE,The product shall provide players no access to information that might reveal the identity of another player PE,The response time shall be fast enough to maintain the flow of the game.The response time shall be no more than 2 seconds for 95% of responses and no more than 4% for the remaining responses.F,The product shall maintain a list of players.
F,The product shall allow a user to identify himself or herself as a player. F, Once a game has been initiated the product shall make the players assigned to the game unavailable for selection from the list of players.F,The product shall allow a user to remove himself or herself from the list of players at any time.F,The product shall make inactive players unavailable for selection from the list of players.F,The product shall remove players who have been inactive for 24 hours from the list of players.F,The product shall allow a player to initiate a game by selecting another available player from the list of players.F, Once a game is initiated the product shall display a defensive grid to each player F, Once a game is initiated the product shall display an offensive grid to each player.F,The product shall prevent a player from viewing the offensive or defensive grids of the other player.
F, Once a game is initiated the product shall allow each player to position their 5 ships on their respective defensive grids. F,The product shall allow a player to position ships horizontally on their grid.F,The product shall allow the player to position ships vertically on their grid.F,The product shall allow the player to position ships diagonally on their grid F,The product shall prevent the player from overlapping ships on their grid.F, When both players have positioned their 5 ships on their defensive grids the product shall define the status of each player for the first turn.F, For each turn the product shall identify one player as offensive status and one player as defensive status.F, The product shall maintain the status of each player for a turn either offensive or defensive.F, At the start of each turn the product shall notify each player of his or her status.F, For each turn the product shall allow the offensive player to define a shot.
F, For each shot the product shall examine the defensive grid of the defensive player to determine whether the shot was a hit or miss. F, For each shot the product shall notify the players whether the shot was a hit or miss.F, For each shot the product shall mark the shot as a hit or miss on the defensive grid of the defensive player.F, For each shot the product shall mark the shot as a hit or miss on the offensive grid of the offensive player.F,When the hit results in sinking a ship the product shall notify the players that the ship was sunk.F,When the hit results in sinking a ship the product shall mark the ship as sunk on the defensive grid of the defensive player.F,When the hit results in sinking a ship the product shall indicate the type of ship sunk on the defensive grid of the defensive player.F,When the hit results in sinking a ship the product shall mark the ship as sunk on the offensive grid of the offensive player.
F,If the shot was marked as a hit the product shall allow the offensive player to define a shot. F,If the shot was marked as a miss the product shall change the status of the offensive player to defensive F,If the shot was marked as a miss the product shall change the status of the defensive player to offensive F,The product shall continue to assign turns until the game is ended.F,The product shall end a game when a hit by the offensive player results in sinking the last of the defensive player s ships which had not been previously marked as sunk.F,The product shall allow a player to end a game at any time during the game.F,When a game is ended the product shall notify the players that the game is ended.F,The notification of the game ending will include the reason the game is ended (i.e.a player has won the game or a player has ended the game).F,When a game is ended the product shall allow each player to identify himself or herself as an available player in the list of players.
F,When a game is ended the product shall allow each player to remove himself or herself from the list of players. US,The system shall be easy to use by callers and supervisors.Callers and supervisors must be able to accomplish any system task within 2 minutes.US,The system shall be easy to learn by callers and supervisors.Callers and supervisors shall learn how to use the system within 10 minutes.PE,The system shall be responsive.At the DePaul TeleServices Center when a supervisor uses the system to view a callers schedule the system response time must be no more than 2 seconds 90% of the time and no more than 5 seconds the rest of the time.PE,On a 56k connection the system response time for all operations must be no more than 6 seconds 90% of the time.PE,On a 112k connection or faster the system response time for all operations must be no more than 3 seconds 90% of the time.
A,The system shall have high availability every day of the year.The system must be available for use between 12 A,The system must be available for use between 7 MN,The product shall be internet browser independent.The product must run using Internet Explorer and Netscape Navigator. SE,The product shall ensure that only supervisors can advertise empty time slots.The product must ensure that supervisors are allowed to access advertise empty time slots.SE,The product shall ensure that only supervisors can enter a caller s schedule.The product must ensure that supervisors are allowed to access advertise empty time slots.SE,The product shall ensure that only supervisors can view schedule of all callers.The product must ensure that supervisors are allowed to access advertise empty time slots.LF,The interface shall have a simple look.In the interface and usability survey 90% of beta testers must report that the interface is adequately simple.
LF,The interface shall be appealing to callers and supervisors. In the interface and usability survey 80% of beta testers must report that the interface is appealing.LF,The product shall provide GUI based Monitoring Services.System Administrators shall monitor message exchanges will predict and determine problems and will also provide internal and external support.LF,The product shall comply with City of Chicago Application Development Guide Framework.Application Development Framework will provide structured Graphical User Interface.US,The product shall provide Monitoring Services.The product shall be easy for System Administrators and DBAs to use after two weeks of usage.US,The product shall provide Error Tracking Services.The product shall help the System Administrators and DBAs to resolve transaction errors 99% of the time.
US,The product shall provide Portals.The product shall help System Administrators to analyze the transactions with ad-hoc reporting and KPIs (Key Performance Indicators) PE,The product shall provide Asynchronous Messaging.The product shall provide asynchronous messaging to enable parallel processing and reduce the overhead in communications PE,The product shall provide Multi-threading and Load Balancing.The product shall provide multi-threading technology as well as load balancing to overcome data load spikes and traffic. PE,The product shall process messages within 2 seconds.The product shall have a maximum message processing time of 2 seconds PE,The product shall poll the system for messages every second.The product shall poll the system for new messages at a maximum of every 1 second.
PE,The product shall provide Dynamic Change Support and Transparent resource addition.The product shall support Transparent Resource addition and Dynamic Change support to provide scalability and avoid service interruptions of more than 1 day. A,The product shall be available 99% of the time.Rationale FT,The product shall be robust.The product shall have fault avoidance based on standards adoption.FT,The product shall be robust with fault tolerance.The product shall have fault tolerance by using recovery technique compensatory transaction and routing around failures.SC,The product shall be able to process all transactions.The product shall process minimum of 1 million transactions per year.O,The product shall be platform independent.The product shall enable access to any type of development environment and platform.
O,The product shall interface CSR and Datastream.The product shall exchange messages between CSR and EAM on a real time basis without any direct interfaces between two systems. MN,The product shall continue to operate during upgrade change or new resource addition.The product shall be able to continue to operate with no interruption in service due to new resource additions.MN,The product shall be updated on a regular basis.The product will have a new major feature introduced every six months and quarterly maintenance releases every six months.MN,The product shall operate with every platform.The product shall accommodate any new technology without any change in its architecture.SE, The product shall have security.The product shall provide authentication and authorization.SE,The product shall information Security.The product shall enable privacy by encryption WS-Security and Integrity Checking.
SE,The product shall have audit capabilities.The product shall store messages for a minimum of one year for audit and transaction tracking purposes. LF,The system shall use IBM and CCR branding standards.The interface must have one IBM logo and one CCR logo.LF,The system shall be displayable on any work computer a CCR user may have.The system interface shall be formatted to display at 800 by 600 pixel resolution.LF,The system interface will use CCR nomenclature.The system must use the same nomenclature used in the existing WCS system and reports LF,The interface must summarize the data in a fashion that s appealing to CCR management.Every report type must have at least one high level summary shown in the results dashboard/interface US,The system must use a point and click interface.100% of all selection and navigation functionality must be able to be activated/selected by either a mouse or a keyboard US,The interface shall be easy to use.
90% of users should be able to successfully use the system to generate a report on their first use. The interface should adhere to IBM usability standards.US,Rreports must be easy to generate.eports must be generated within one button click and exported within one button click (after selecting the report) US,A user shall be able to use the system without instruction after a short amount of time.After one use a user should understand and remember 90% of the system usage and navigation.US,All CCR users shall receive training before the WCS system is deployed.One training session will be held for 100% of CCR users before system deployment (based on a CCR user being available) US,The WCS system shall use appropriate nomenclature and terminology as defined by the Corporate Community Grants organization.All interfaces and reports will undergo usability tests by CCR users.A,The WCS system shall be available at all times as long as IBM s internal intranet W3 is accessible.
The WCS system will be available 95% of the time IBM s internal intranet W3 is available. SC,The WCS system shall have the ability to connect other designated WCS databases currently not included in the system.SC,The WCS system shall have the ability to include data for regions outside the United States and be accessed by users outside of the United States if approved by CCR management.O,The WCS system shall be run in a Windows environment.The WCS shall be accessed by users through the Internet on an IBM computer running a Windows 2000 or above operating system.O,The Brio portion of the WCS system must be able to export files in spreadsheet form (Microsoft Excel and Lotus 1-2-3 formats).Brio will provide buttons in the user interface that produce/export reports in .xls or .123 file formats.MN,The WCS system must run queries to the WCS databases once again (on a specified day) to update the available data available to CCR users.
Queries must be made to external databases for data refresh once a week. MN, All changes updates and additions to the WCS system shall be updated in the WCS Developer and WCS User Guides.Every change must be reflected in WCS system documentation SE,The system shall only be accessed by authorized Corporate Community Grants (CCR) employees.Users must be authenticated using their IBM intranet user name and password.SE,The WCS system shall not allow automatic logins by any user.Cookies containing WCS login information about a user will not be stored on a user s computer.SE,Each team owner will need to use a username and password in order to be authenticated to reach the team owner s page SE,The Commissioner will be the only authorized user per league to adjust league settings.LF,Each page shall contain a navigation menu which shows the index into which the current page is contained.
LF,Each page shall contain a similar format with action items in one location and navigation buttons in a different location. LF,The message board shall contain the 5 most recently posted messages on the league page.The messages will contain up to the first 150 characters of the message.US,A new user shall be able to navigate through the league and team pages within 30 seconds of reaching the start-up page.US,Users shall be able to reach the add drop and trade player pages with one click from the team owner s page.US,Users shall be able to view at least 25 NFL player statistics and points on each page.US,The application shall ask the user to confirm dropping a player or trading a player before performing the action.PE,The application shall be able to complete every user transaction within one fourth of a second of receiving the request over the network.A,The application shall have a downtime of at most 10 minutes per year.O,The application shall run on a web application server.
PE,The maximum wait time for a user navigating from one screen to another within the Disputes application shall be no more than 5 seconds. PE,The Disputes application shall support 350 concurrent users without any degradation of performance in the application.O, The Disputes application shall be available 24x7x365 with the exception of the following scheduled maintenance windows.Monday Saturday 3 O,The Disputes application shall interface with the Statement Database.The Statement Database provides the transaction details to the Disputes System.All transaction details must be obtained from the Statement Database O,A primary database will be used to store cached data for active users.A,The system shall achieve 95% up time.LF,The system shall have a professional appearance LF,The product shall have a consistent color scheme and fonts.US,The system shall be used by realtors with no training.US,The system shall be intuitive and self explanatory.
US,The product shall be easy for a realtor to learn. US,The product shall use symbols and words that are naturally understandable by the realtor community.PE,The product shall produce search results in an acceptable time PE,The search results shall be returned no later 30 seconds after the user has entered the search criteria PE,The product shall generate a CMA report in an acceptable time.PE,The CMA report shall be returned no later 60 seconds after the user has entered the CMA report criteria.PE,The product shall synchronize contacts and appointments in an acceptable time.PE,The product shall synchronize with the office system every hour.A, The product shall be available for use 24 hours per day 365 days per year A, Out of 1000 accesses to the system the system is available 999 times.FT,The product shall operate in offline mode whenever internet connection is unavailable.FT, The product shall allow the user to view previously downloaded search results CMA reports and appointments.
FT,The product shall retain user preferences in the event of a failure. FT,100% of saved user preferences shall be restored when system comes back online.SC,The product shall be able to support multiple remote users SC,The product shall be able support 1000 simultaneous users.SC,The product shall be capable of handling the existing 1000 users.This number is expected to grow 5 times within the next year.US,The product shall be installed by an untrained realtor without recourse to separately-printed instructions.US,90% of untrained realtors shall be able to install the product on their device without printed instructions PO,The product is expected to run on Windows CE and Palm operating systems.SE,Only registered realtors shall be able to access the system.SE,Every user of the system shall be authenticated and authorized.SE,The product shall prevent its data from incorrect data being introduced.
F,The system shall allow a real estate agent to query MLS information F,The user shall be able to get driving directions to a selected property F,The user shall be able to download appointments and contact information for clients F,The system shall display clear property images in the search results SC,system shall be able to handle all of the user requests/usage during business hours. SC,The system shall cater to 10 simultaneous users from 9am to 5pm.SC,The system shall be capable of processing 100% of nursing students and their classes for the next 10 years.SC,The system shall be expected to manage the nursing program curriculum and class/clinical scheduling for a minimum of 5 years.O,The system shall able to operate within a business office environment typical of the Nursing Department at DePaul University.O,The system shall be used within the specifications defined by the computers used by the Program Administrators/Nursing Staff Members.
O,A secondary database will be used to store historical statistics and data. F,The system shall be able to call the seller or buyer to schedule an appointment F,The realtor shall be notified of new client appointments after automatic synchronization with office system F,The realtor shall be able to retrieve a map showing property locations for a specified area F,The system shall notify the realtor when a seller or buyer responds to an appointment request F,The product shall allow the user to save the property search results.US,The system shall be easy to use by the Program Administrators/Nursing Staff Members.US,All of the users shall be able to use the system after one day of training.US,The system shall help the user avoid making mistakes while scheduling classes and clinicals for the nursing students.US,The number of mistakes noted by the students shall be decreased by 50% in the first year.US,The system shall be intuitive to the Program Administrators/Nursing Staff Members.
US,2 out of 3 Program Administrators/Nursing Staff Members shall successfully be able to use the system to manage the scheduling of classes and clinicals. US,The system shall use symbols and words that are naturally understandable by the user community.US,The verbiage used within the system shall be consistent with Standard English and the terminology of the national Nursing Community.PE,Staff members shall be able to complete a set of tasks in a timely manner.PE,The response time of general student management tasks shall take no longer than 5 seconds and the response time of schedule generation shall take no longer than 30 seconds.A,The system shall be available for use between the hours of 8am and 6pm.O,The system shall operate within the Windows XP Professional operating system.O,The system shall interface with CampusConnect s central server O,The system shall interface with the faculty central server.O,The system shall interface with the student master server.
O,We must be able to interface with any HTML browser. SE,Only authorized users shall have access to clinical site information.SE,Program Administrators/Nursing Staff Members are the only people who shall have access to clinical site details.SE,Only authorized users shall have access to students personal information.SE,Dr Susan Poslusny and Dr Julie Donalek are the only people who shall have access to students personal information.SE,Only authorized users shall have access to the portion of the system that interfaces with CampusConnect.SE,Fit Criterion SE,The system shall have basic data integrity checking to reduce the possibility of incorrect or invalid data being introduced.SE,The system shall protect private information in accordance with the organization s information policy.SE,The system shall be built such that it is as secure as possible from malicious interference.SE,The system shall use the organization s standard virus protection software to help monitor threats to itself.
F,A cohort shall be associated to one Program of Study offered by the department. F,Program Administrators and Nursing Staff Members shall be able to add a listing of students to a cohort.F,Program Administrators and Nursing Staff Members shall be able to add a single student to a cohort F,Students added to a cohort shall be eligible for the cohort s Program of Study.F,Program Administrators and Nursing Staff Members shall be able to specify the preferred sequence of classes for a given cohort.F, For each class within a sequence for a cohort Program Administrators and Nursing Staff Members shall be able to specify which quarter that class will be offered.F,Program Administrators and Nursing Staff Members shall be able to add new classes to a sequence of classes MN,Application updates shall occur between 3AM and 6 AM CST on Wednesday morning during the middle of the NFL season.MN,New System version releases shall be released at the beginning of each NFL season.
F,The system shall display data from the Sync Matrix 1.0 and Exercise Management Tool 1.0 applications F,The system shall link Events back to either the Sync Matrix 1.0 or the Exercise Managment Tool 1.0 applications for modifications. F,The system shall allow modification of the display.F,The system shall offer a display of all the Events in the exercise.F,The system shall filter data by F,The system shall allow a user to define the time segments F,The system shall display the local and exercise time in separate clocks F,The system shall offer the ability to pause and resume the refresh of data.F,The system shall provide charts for the Activity or Event actual versus assumed time.A,The RFS system should be available 24/7 especially during the budgeting period.The RFS system shall be available 90% of the time all year and 98% during the budgeting period.2% of the time the system will become available within 1 hour of the time that the situation is reported.
O,98% of the user workstations shall meet the minimum hardware requirements prior to implementation. Minimum Hard Disk Space O,98% of the user workstations shall meet the minimum software requirements prior to implementation O,The RFS system should be able to easily interface with the BDW environment in order to load the data.7 data files shall be received from the BDW and loaded into the RFS system within 3 hours one day prior to the start of budgeting process every year.The data will be as of October month-end.Test files (7) shall be loaded up to 5 times throughout the year from the BDW.MN,Changes to revenue and expense formulae must be applied to the RFS system every year prior to the budgeting process start.Revenue and expense formulae will be updated within one week of their receipt after which 2 weeks will be allotted to testing of the budget calculations to be conducted jointly by the project development team and PCG Finance.
MN,New branches/complexes/districts/regions and revenue/expense accounts added to the BDW must be reflected in the RFS system. 100% of the entities (mentioned above) available as of October month-end will be loaded into the RFS system prior to the budget start process.SE,Only authorized user shall access the RFS system.ONLY users that exist in the UAS system with the RFS system assigned to their role(s) shall access the RFS system and use it.SE,User access should be limited to the permissions granted to their role(s) Each level in the PCG hierarchy will be assigned a role and users will be assigned to these roles.Access to functionality within RFS system is dependent on the privileges/permission assigned to the role..LF,The product must comply with the intranet page standards and requirements of ENET Securities.95% of the product look & feel will be approved by the Architecture group prior to implementation.
The remaining 5% will be corrected and approved within one month of product release. LF,The intranet pages should display appropriately in all resolutions from 800x600 and higher.The display of all the pages will be correct in 95% of the resolutions prior to implementation.The remaining 5% will be corrected within a week of notification of the incorrect display.US,The users should be able to easily use the system to successfully complete their budgets within the stipulated time for completion.With a week of training prior to product implementation 98% of the users shall complete their budgets within the stipulated time.PE,The product interface should be fast.The response time of the product interface should not exceed 2 seconds 90% of the time.For 10% of the time the response time should not be more than 10 seconds.not be more than 10 seconds.F,The system will use the stored e-mail addresses as a primary means of communicating information to affected parties.
F, The system will notify affected parties for routine communication purposes including but not limited to informing students of upcoming offered classes (timeline) and what their schedule will be/shall be. F, The system will notify affected parties when changes occur affecting clinicals including but not limited to clinical section capacity changes and clinical section cancellations.F, The system will notify affected parties when changes occur affecting classes including but not limited to class cancellations class section detail changes and changes to class offerings for a given quarter.FR,The feature of the application shall provide the optimized route for the given addresses FR,This feature provides the user to select an optimized route for the addresses given and fixing any path between the desired locations.
NFR,"Fast internet connection is not mandatory, but it would increase the performance of the application" FR,"after every user request, new maps are needed to be loaded" NFR,"As Google Maps API is being used for this application, it is mandatory that we abide by the terms of use specified by Google" FR,After installing the Application from iTunes or the App store the user will be asked to login. FR,"Upon successful login, the user can select from the following catalogue options: Spring 2013 ,Winter 2013" FR,"Based on the selection the product categories will be listed: Outerwear,Soft Shell,Fleece,Knits,Wovens,Active Wear, Safety,Accessories,Team Wear, e.c.o.Collection" FR,Once a category is selected the product thumbnail images will appear for selection.FR,A user can then select the product and view the details.FR,The detail page will contain all the same features and information as the website.
FR,From the here the user can either proceed to call a rep for an order or proceed to the online checkout process. FR,"Upon successful login, the user can select from the following catalogue options or use the search function indicated by the search icon in the top right hand corner of the application." FR,Once the icon is selected two open text field will be available to the user to search for the product name - by product ID/SKU or by keyword.FR,Following a successful query the search feature will query the application database to display the search results.Following an unsuccessful query the search feature will display an error message that no matches were found FR,A user can select the desired product from the results and proceed to the product details.
FR,From the here the user can either proceed to call a rep for an order or proceed to the online checkout process FR,"Upon successful login, the user can select from the following catalogue options, search or use the sub navigation to find the following items: Catalogue ,Videos,Contact Us" FR,Should a user select to download the catalogue PDF it will be viewed on their mobile device. FR,"Should a user select videos, the 360 video list will display.Upon selecting a video it will play on their device." FR,All users shall be able to zoom into images using the standard gestures on mobile devices.FR,All Users will be able to view the next product by swiping left to right .FR,All users should be able to go back to the product category page via the back button in the top corner.FR,All users should be able to search by keyword or product ID.
FR,"All users should be able to navigate using the sub navigation icons to access the Catalogue PDF, 360 videos, and to contact a rep representative" FR,The application itself will only have minimal logic and so there should be little to no issues with the computation required by the phone itself. FR,"The only hazard is a user using the device when they should not be, such as while driving." NFR,The application must be able to link up with the INTERNAL system in order for users to properly log in and be identified.NFR,This information must be kept secure.NFR,The data entry should not be that much more difficult than if they were at an actual computer.NFR,"Because this application will be on a phone, portability is also important.
We don’t want it to take up so much space or be too slow causing the user’s to not be able to fit it on the device." NFR,"When the user fills all necessary fields, an activation mail will be sent to users mail address" NFR,"User will be able to log in and start using the system just after activating his/her account." FR,"user enters his/her username or email and his/her password and logs in the system" FR,"user touches log out button, and logs out of the system. The user will not use the system until he/she logs in again." FR,"user is able to search items.User can search items according to their price, their location, their category." FR,"user is able to browse the profiles other users.User can see the other users’ user names, profile photos, and the users’ items." FR,"user is able to browse categories according to his/her wishes.User can also see how many items each category has." FR,"user is able browse items.
User can see the items’ pictures, prices, and other features." FR,"If the user changes his/her username or password, a notification mail will be sent to the user’s email address." FR,The changes will apply if the user clicks on the link in the email. FR,"user can buy credits from the system.He/she can use credit card or his/her PayPal account." FR,"user can add the items he/she likes to his cart by touching “Add to the cart” button." FR,"if the item the user wants is available for exchange, the user can request the exchange of items that he/she has with the item he/she wants." FR,"the user can approve the exchange that is offered by another user by clicking “Approve” button." FR,"the user can decline the exchange that is offered by another user by clicking “Decline” button." FR,the user notifies other user that the shipment of the item has started.FR,the user can add as many as items he/she wants.
FR,"The user can also set the price, set the amount, upload pictures of the item and add details of the item." FR,"the user can edit the items that he/she has already put on the system. The user can add/remove pictures, change the price and details of the item." FR,"if the user no longer wishes to sell/exchange the item, he/she can remove the item from the system by touching “Remove the item” button." FR,"if the user no longer wishes to use the system, he/she can click “Delete Profile” button.After deleting the profile, the user will no longer be able to use the system with the account details that he entered before." FR,"the user can mark an item as a favorite so that he will be notified if any change is made to the item" FR,the user can rate and comment about the users whom he traded with.FR,10000 users should be able to use the system at the same time.FR,The response time of the system should be 2 seconds at most.
NFR,Search form should be added to home page NFR,Sites should be added correctly to the search engine FR,Build the search engine based on the search form and sites available FR,"User should be able to search by site name, description" FR,System should be self capable with feature of Auto Suggest FR,System should be able to return the search results in desending order in the form of list of links to the sites FR,"System should be able to display "" No results found "" in case no search results found " FR,System should have a link back to home in case no results found FR,geo-location feature to be able to find a user FR,Use Geolocation scripts to locate user FR,check if user is within 1 mile of the campus FR,"If user within 1 mile of the campus , update location" FR,User for permission to use the location service .
FR,"If the user denies to user the location service , prompt the user to home screen " FR,"If user allows location service , create the user" FR,"if the system unable to update the location , Show failure msg and return the user to home page." FR,Update Page with link to sites nearby FR,Build Ajax Response to update home page with a link to nearby site based on user location FR,"Application should respond with a list of links to the 3 nearest sites by A-Z, sorted by name" FR,"Application should respond with the sites placed on a map with markers that includes name, description and link to site page. " FR,Application should do the distance calculations and geolocation information for user/site and nearby sites.
FR,"when user clicks on links to browse sites from home page, the system should respond with a list of links to all the sites by A-Z, sorted on site name" FR,Application should collect list of all sites sorted by name FR,Application should Build URL to each site’s page FR,"when user clicks on link to map sites from the home page, application should respond with a campus map with all the historical sites placed on it" FR,"Each historical site should have a marker for each site placed on map that include the name, shorten description, and link to the site page." FR,The site’s page should contain link to walking directions on tour FR,"Application should respond with a list of thumbnail images for the site, listed from oldest to newest by year." FR,"In case no image available , application should return ""no image"" flag." FR,"when user clicks on a thumbnail image from the site’s page, the application should replace the site’s image with the medium sized clicked on image " FR,Application should replace the site’s description with the clicked on image description FR,"when the user clicks on the main image on the site’s page, application should return with a an enlarged version of the image " FR,"when User clicks on link to Walking Tour, application should respond with walking tour page" FR,"when User clicks on link to About, application should respond with About page" FR,No personal data about a user is stored.
FR,User location information should be deleted nightly via a cron job. NFR,Project admins will need to be authorized before they can use the admin interface to manage the sites in the project.NFR,Standard web security measure should be implemented.NFR,"This application should be easily extendable, maintainable and able to be released as an open-source application" NFR,The app runs on a smart phone with operating system Android version 2.3.NFR,"All layout shall be according to the TU/e corporate identity, unless in conflict with the guidelines of requirements UCOR003-UCOR006." NFR,Icons are according to the Android Icon Guidelines.NFR,The user interfaces should be according to the Android User Interface Guidelines.NFR,The Twitter â€tweetâ€ interface should be according to the Twitter Guidelines.NFR,The Facebook â€likeâ€ interface should be according to the Facebook Guidelines.NFR,The campus map covers the campus of the TU/e.
NFR,The campus map covers the central train station of Eindhoven. NFR,A new stylistic map has the right dimensions.NFR,A new stylistic map is geographically correct.NFR,News is restricted to what is available in the feeds on the list of news RSS-feeds.NFR,Agenda is restricted to what is available in the feeds on the list of agenda RSS-feeds.NFR,Information about a certain person is restricted to what is available on the employee pages of the TU/e NFR,The information and the search methods of the Encyclopedia-app are restricted to the services provided by the MediaWiki of the TU/e Encyclopedia.NFR,The information and the search methods of the Studenthandbook-app are restricted depending on file format of the handbook-information.NFR,Response Time GIST: The fastness of the search SCALE: The response time of a search METER: Measurements obtained from 1000 searches during testing.MUST: No more than 2 seconds 100% of the time.WISH: No more than 1 second 100% of the time.
NFR,"System Dependability GIST: The fault tolerance of the system. SCALE: If the system loses the connection to the Internet or to the GPS device or the system gets some strange input, the user should be informed.METER: Measurements obtained from 1000 hours of usage during testing.MUST: 100% of the time." NFR,Hard Drive Space GIST: Hard drive space.SCALE: The applicationâ€™s need of hard drive space.METER: MB.MUST: No more than 20 MB.PLAN: No more than 15 MB.WISH: No more than 10 MB.MB: DEFINED: Megabyte NFR,Application Memory Usage GIST: The amount of Operate System memory occupied by the application.SCALE: MB.METER: Observations done from the performance log during testing MUST: No more than 20 MB.PLAN: No more than 16 MB WISH: No more than 10 MB Operate System: DEFINED: The mobile Operate System which the application is running on.MB: DEFINED: Megabyte.NFR,System Reliability GIST: The reliability of the system.
SCALE: The reliability that the system gives the right result on a search. METER: Measurements obtained from 1000 searches during testing.MUST: More than 98% of the searches.PLAN: More than 99% of the searches.WISH: 100% of the searches.NFR,System Availability GIST: The availability of the system when it is used.SCALE: The average system availability (not considering network failing).METER: Measurements obtained from 1000 hours of usage during testing.MUST: More than 98% of the time.PLAN: More than 99% of the time.WISH: 100% of the time.NFR,Internet Connection DESC: The application should be connected to the Internet.NFR,GPS Connection DESC: The application should be connected to the GPS device.NFR,"Communication Security GIST: Security of the communication between the system and server.SCALE: The messages should be encrypted for log-in communications, so others cannot get user-name and password from those messages.
METER: Attempts to get user-name and password through obtained messages on 1000 log-in session during testing. MUST: 100% of the Communication Messages in the communication of a log-in session should be encrypted.Communication Messages: Defined: Every exchanged of information between client and server." NFR,Restaurant Owner Login Account Security GIST: Security of accounts.SCALE: If a restaurant owner tries to log in to the web portal with a non-existing account then the restaurant owner should not be logged in.The restaurant owner should be notified about log-in failure.METER: 1000 attempts to log-in with a non-existing user account during testing.MUST: 100% of the time.NFR,Admin Login Account Security GIST: Security of accounts.SCALE: If an admin tries to log in to the web portal with a non-existing account then the admin should not be logged in.The admin should be notified about log-in failure.METER: 1000 attempts to log-in with a non-existing user account during testing.
MUST: 100% of the time. NFR,"Restaurant Owner Account Security GIST: Security of restaurant owners accounts.SCALE: A restaurant owner and IP address should not be able to log-in for a certain time period after three times of failed log-in attempts.METER: 1000 attempts to log-in during the lock period after user account has been locked because of failed log-in attempts of three times.MUST: The locking period should be half an hour, and during that period the log-in function is disabled." NFR,"Admin Account Security GIST: Security of admin accounts.SCALE: An admin and IP address should not be able to log-in to the web portal for a certain time period after three times of failed log-in attempts.METER: 1000 attempts to log-in during the lock period after user account has been locked because of failed log-in attempts of three times.
MUST: The locking period should be half an hour, and during that period the log-in function is disabled." NFR,"User Create Account Security GIST: The security of creating account for users of the system. SCALE: If a user wants to create an account and the desired user name is occupied, the user should be asked to choose a different user name.METER: Measurements obtained on 1000 hours of usage during testing.MUST: 100% of the time." NFR,"Restaurant Owner Create Account Security GIST: The security of creating account for restaurant owners of the system.SCALE: If a restaurant owner wants to create an account and the desired user name is occupied, the restaurant owner should be asked to choose a different user name.METER: Measurements obtained on 1000 hours of usage during testing.MUST: 100% of the time." NFR,Application extendibility DESC: The application should be easy to extend.The code should be written in a way that it favors implementation of new functions.
NFR,Application testability DESC: Test environments should be built for the application to allow testing of the applications different functions. NFR,Application portability DESC: The application should be portable with iOS and Android.NFR,Performance Requirements 10000 users should be able to use the system at the same time.The response time of the system should be 2 seconds at most.NFR,Design Constraints We will use â€œwaterfall methodâ€ and Object â€“ Oriented Programming paradigm.We will use Java as programming language.The Android version of the device should be 2.3 or higher.The system requires Internet connection all the time.NFR,â€‹ The system will be designed in such a way that the algorithms for the four main units will be able to be easily swapped out.NFR,The overall accuracy of the Web APIâ€™s response will be measured using a developer-made testing set.NFR,The overall accuracy is calculated by dividing total number of correct answers by the number of questions asked.
NFR,The accuracy of the Generic Question Constructionâ€‹ part will be close to 80% NFR,Modularity The system will be designed in such a way that the algorithms for the four main units will be able to be easily swapped out. NFR,"The overall accuracy of the Web APIâ€™s response will be measured using a developer-made testing set.The overall accuracy is calculated by dividing total number of correct answers by the number of questions asked.The accuracy of the Generic Question Constructionâ€‹ part will be close to 80%.The accuracy of the Generic Answer Constructionâ€‹ unit will be close to 70% The accuracy of the Generic Answer Populationâ€‹ unit will be close to 70% Drexel facilitiesâ€™ locations and schedules will have accuracy greater than 70% â€‹ Drexel staffâ€™s office locations, contact information, and positions will have accuracy greater than 70%.â€‹ Drexel policies including academics, admissions, information technology etc.
will have accuracy greater than 70%.â€‹ â€‹ On-campus dining locations, hours, food types, etc. will have accuracy greater than 50%.â€‹ Food trucksâ€™ general locations, hours, and food types on Drexel Campus will have accuracy greater than 40%." NFR,"Fast Response The average time for the server to respond, over the question testing set, will be less than or equal to 2 seconds" NFR,"Security The connection between the Web API and the programs will use HTTPS, for security." NFR,Ease of Useâ€‹ A new user will make less than 3 mistakes in 5 minutes after 5 minutes of use.NFR,The search feature should be prominent and easy to find for the user.NFR,"The different search options should be evident, simple and easy to understand." NFR,The results displayed in the list view should be user friendly and easy to understand.Selecting an element in the result list should only take one click.NFR,The results displayed in the map view should be user friendly and easy to understand.
Selecting a pin on the map should only take one click. NFR,The information link should be prominent and it should be evident that it is a usable link.Selecting the information link should only take one click.NFR,The user interface must be GUI based.NFR,The users are required to input the data from cell phone keypad NFR,User has the options to change the settings locally and remotely.NFR,Reduce the typing option by giving support for select menus.NFR,The primary performance requirement is speed of the network.The application itself will only have minimal logic and so there should be little to no issues with the computation required by the phone itself.NFR,"There are no safety requirements with this application, other than any normal hazards of a mobile device.The only hazard is a user using the device when they should not be, such as while driving." NFR,The application must be able to link up with the INTERNAL system in order for users to properly log in and be identified.
This information must be kept secure.
NFR,"The primary attribute of this application will be usability given the large amounts of data and information that will be presented on such a small screen, as well as the userâ€™s ability to input data into the device in a reasonable manner that should not be that much more difficult than if they were at an actual computer." NFR,"With regards to the intended number of users in the Amsterdam pilot, and the projected load scenarios, the intention is for the system to be able to serve 20.000 queries / day (in large part during the 4 peak traffic hours)." NFR,"In order to create a viable offering for the user we will have to build a simple, transparent system that can be understood and trusted by the people that are using it." NFR,"Anonimization & aggregation, so that route information may be shared safely without disclosing personal information." NFR,"Encryption, for all data that is privacy sensitive, but must be persisted on the server in order for basic functionality.
" NFR,Permit the use of unverified (anonymous) avatars / aliases. NFR,Give control to end-users over private data (at least a delete private repository option) NFR,"The system should be language agnostic, since portability is a prime goal of the overall project.English, turkish, Finnish, and Italian should be at least be supported because these are the replication partners." NFR,All text elements in the user interfaces are presented in the selected language English and Dutch.NFR,Sub-apps and their launchers can be added to the main app by a qualified programmer.NFR,Sub-apps and their launchers in the main app can be deleted by a qualified programmer.NFR,"The application maintainer should also be able to change the (lists of) information sources of the sub-applications: agenda, news, Facebook and Twitter " NFR,"Application maintainers should be able to maintain the application, without having knowledge about Java Android programming.
They should be able to modify the approximate appearance of the application. This means the colors, and GUI figures in the applications, but not the addition/deletion of GUI elements." NFR,"Application will be extendable.This means that a skilled Java Android programmer should be able to easily add a new sub-application to insApption, and add a launcher to it in the main-application." NFR,Users must be a Radford University student in order to access the automatic class schedule and assignment sharing features of this application Users can authenticate by logging in using their RU username and password.NFR,Most functionality will not require network connectivity.System components that require authentication through Radford University and network connectivity will function as long as the systems maintained by the University are available.
NFR,The development team will follow best practices for clean code and software modularity in order to make the application as maintainable as possible. NFR,Users will be able to access this application on or off campus anytime on their mobile device without the need of an internet connection.NFR,"The application will be highly extensible in terms of adding course and calendar details or views.However, the application in general has low extensibility." NFR,An application instance shall be able to be reusable every semester for a returning student.The user will be able to update their class schedule by having the application check for a registered schedule set for the next semester and if the current school semester is over.Application components will be able to be integrated into the larger RU Mobile application developed and maintained by Radford University.
NFR,The system will be designed in such a way that the algorithms for the four main units will be able to be easily swapped out. NFR,The accuracy of the Generic Question Constructionâ€‹ part will be close to 80%.NFR,The accuracy of the Generic Answer Constructionâ€‹ unit will be close to 70%.NFR,The accuracy of the Generic Answer Populationâ€‹ unit will be close to 70%.NFR,"The average time for the server to respond, over the question testing set, will be less than or equal to 2 seconds." NFR,"The connection between the Web API and the programs will use HTTPS, for security." NFR,A new user will make less than 3 mistakes in 5 minutes after 5 minutes of use.NFR,"FAU class contains requirements for recording the occurrence of security relevant events that take place under TSF control.
This family identifies the level of auditing, enumerates the types of events that shall be auditable by the TSF, and identifies the minimum set of audit related information that should be provided within various audit record types. The following extended requirement for the FAU class has been included in this ST because the operational environment is capable of providing security audit generation for TSF functions." NFR,"Performance should not be an issue because all of our server queries involve small pieces of data.Changing screens will require very little computation and thus will occur very quickly.Server updates should only take a few seconds as long as the phone can maintain a steady signal.The cost-division algorithms used by in application will be highly efficient, taking only a fraction of a second to compute." NFR,SplitPay will not affect data stored outside of its servers nor will it affect any other applications installed on the userâ€™s phone.
It cannot cause any damage to the phone or its internal components. The only potential safety concern associated with this application applies to virtually all handset apps: SplitPay should not be used while operating a vehicle or in any other situation where the userâ€™s attention must be focused elsewhere.NFR,"This application assumes that only the user or whoever he/she allows will have access to his/her Android handset.With that being said, only a Google email address is required to verify the identity of the user upon opening the app.Since it is not password protected, there is no method to authenticate the userâ€™s identity.This could only pose a thread if a user has set up PayPal functionality, however any transaction involving real currency must be authorized and confirmed before becoming final.
The PayPal API provides all of the security checks needed to ensure that no fraudulent transactions occur." NFR,The graphical user interface of SplitPay is to be designed with usability as the first priority. The app will be presented and organized in a manner that is both visually appealing and easy for the user to navigate.There will be feedbacks and visual cues such as notifications to inform users of updates and pop-ups to provide users with instructions.NFR,"To ensure reliability and correctness, there will be zero tolerance for errors in the algorithm that computes and splits expenses between group members.To maintain flexibility and adaptability, the app will take into account situations in which a user loses internet connection or for whatever reason cannot establish a connection with the server.These users will still be able to use the application, but any Bills, transactions, etc.
posted while disconnected will be cached until the connection is restored." NFR,All text elements in the user interfaces are presented in the selected language (for language selection see requirement UCAR101. The available languages are mentioned in UCAR002 and UCAR003) NFR,English is an available language.NFR,Dutch is an available language.NFR,The administrator can change the colors used in the user interfaces of the sub-apps and main-app (this does not include the external resources).NFR,The administrator can change the fonts used in the user interfaces of the sub-apps and main-app (this does not include the external resources).NFR,The administrator can change icons used in the user interfaces of the sub-apps and main-app (this does not include the external resources).NFR,The administrator can change buttons used in the user interfaces of the sub-apps and main-app (this does not include the external resources).
NFR,The administrator can change the background used in the user interfaces of the sub- apps and main-app (this does not include the external resources). FR,The administrator can change text elements used in the user interfaces of the sub-apps and main-app (this does not include the external resources).FR,The language can only be selected in the main app.
FR,The default language of insApption is English FR,The main menu contains a launcher to the Map-app.3.1.3 FR,The main menu contains a launcher to the News-app.3.1.4 FR,The main menu contains a launcher to the Agenda-app.3.1.5 FR,The main menu contains a launcher to the Facebook-app.3.1.9 FR,The main menu contains a launcher to the Twitter-app.3.1.10 FR,The main menu contains a launcher to the Employee-finder-app.3.1.6 FR,"The main menu contains a launcher to the Encyclopedia-app, only if the Dutch language is selected.3.1.7" FR,"The main menu contains a launcher to the Student handbook-app, only if the Dutch language is selected.3.1.8 " FR,It is possible to â€œlikeâ€ insApption from within the main menu. FR,It is possible to â€œtweetâ€ insApption from within the main menu.FR,There is a link to a Twitter settings menu.FR,There is a link to a Facebook settings menu.
FR,"In the settings menu for Twitter the user can save username in combination with password, regarding Twitter." FR,"In the settings menu for Facebook the user can save username in combination with password, regarding Facebook." FR,"In the settings menu for Twitter there is an option to delete the saved username in combination with the password, regarding Twitter." FR,"In the settings menu for Facebook there is an option to delete the saved username in combination with the password, regarding Facebook." FR,The application maintainer can modify the appearance of the separate launchers. FR,It is possible to search for a certain building on the campus of the TU/e by name.FR,It is possible to search for a certain building on the campus of the TU/e by address.FR,It is possible to search for a certain POI on the campus of the TU/e by name.FR,A search described in requirements UCAR201 and UCAR202 results in a displayed set of buildings that match the search input.
FR,A search described in requirements UCAR203 results in a displayed set of POIâ€™s that match the search input. FR,"It is possible to view an element from the search result described in UCAR201, UCAR202 and UCAR203 in the map view." FR,It is possible to navigate from a building on the campus to another building on the campus.FR,It is possible to navigate from a building on the campus to a POI on the campus.FR,It is possible to navigate from a POI on the campus to a building on the campus.FR,It is possible to navigate from a POI on the campus to a POI on the campus.FR,It is possible to navigate from an POI on the campus to another location on the campus.FR,It is possible to navigate from a street on the campus to another location on the campus.FR,It is possible to switch between different means of transportation in the navigator.FR,The from-location can be searched by the name of a building.FR,The from-location can be searched by the address of a building.
FR,The from-location can be searched by the name of a POI. FR,The from-location is selected from the search result described in UCAR211 and UCAR212.FR,The from-location can be selected on the map.FR,The to-location can be searched by the name of a building.FR,The to-location can be searched by the address of a building.FR,The to-location can be searched by the name of a POI.FR,The to-location is selected from the search result described in UCAR215 and UCAR216.FR,The to-location can be selected on the map.FR,The route planned by the navigator is shown in the map view.FR,The application has a map view in stylistic style.FR,The application has a map view in satellite style.FR,It is possible to switch between stylistic map view and satellite map view.FR,It is possible to zoom when in the map view.FR,It is possible to pan when in the map view.FR,Current location is shown in the map view.FR,POIâ€™s of a building are indicated.FR,It is possible to â€œtweetâ€ the userâ€™s current location.
FR,It is possible to change the status on Facebook. The new status will indicate the userâ€™s current location.FR,The administrator can change the stylistic map.FR,The administrator can add buildings.FR,The administrator can remove buildings.FR,The administrator can add vertices of a path which is used in the route planner.FR,The administrator can remove vertices of a path which is used in the route planner.FR,The administrator can remove edges of a path which is used in the route planner.FR,The administrator can add edges of a path which is used in the route planner.FR,"The administrator can add POIâ€™s.A POI should belong to a certain building, floor and room number." FR,"The administrator can remove POIâ€™s.
A POI should belong to a certain building, floor and room number." FR,"The administrator can link a POI to a certain building, floor and room." FR,"The administrator can unlink a POI from a certain building, floor and room." FR,Titles of all news items available through the news RSS-feeds are shown on the title screen. FR,"From the first titles of the news articles, the first lines are shown." FR,Full news report is shown after choosing the corresponding news title.FR,It is possible to â€œlikeâ€ a certain news item.FR,It is possible to â€œtweetâ€ a certain news item.FR,The administrator can modify the list of news RSS-feeds that are available in English.FR,The administrator can modify the list of news RSS-feeds that are available in Dutch.FR,"When the agenda-app is started, upcoming events are shown on the title screen." FR,There is a calender view.FR,It is possible to open calendar view.FR,It is possible to close calendar view.
FR,The user can change the calender to a different month. FR,The user is able to see on which days events take place in calendar view.FR,The current day is indicated on the calendar.FR,By selecting a certain day in the calendar the events for that day are shown.FR,It is possible to search for events by date.FR,It is possible to search for events by subject.FR,A search described in requirements UCAR406 and UCAR407 results in a displayed set of events that match the search input.FR,It is possible to â€œlikeâ€ a certain event.FR,It is possible to â€œtweetâ€ a certain event.FR,The only events that are available in the agenda-app are extracted from the agenda RSS-feeds in the selected language.FR,The administrator can modify the list of agenda RSS-feeds that are available in English.FR,The administrator can modify the list of agenda RSS-feeds that are available in Dutch.FR,It is possible to search a person by name.FR,It is possible to search a person by department.
FR,A search described in requirements UCAR501 and UCAR502 results in a displayed set of employees that match the search input. FR,It is possible to view all personal info available through the employee pages of a person in the search result set of requirement UCAR503.FR,It is possible to show the location of the employeeâ€™s office in the map view (see 3.1.3).FR,It is possible to plan a route to the location of the employeeâ€™s office (see 3.1.3).FR,When the Encyclopedia-app is started the most searched items are shown on the title screen.FR,When the Encyclopedia-app is started the most recent items are shown on the title screen.FR,It is possible to search the book by a certain word or phrase.FR,A search described in requirement UCAR602 results in a displayed set of topics.FR,The user can select a topic from the search result list.FR,By clicking on a topic the full text regarding that topic is shown.FR,It is possible to â€œlikeâ€ a certain topic.
FR,It is possible to â€œtweetâ€ a certain topic. FR,When the student handbook-app is started the most searched items are shown on the title screen.FR,A search described in requirement UCAR702 results in a displayed set of topics.FR,The user can select a topic from the search result described in requirement UCAR703.FR,When the Facebook-app is started the page of TU/e on Facebook is shown on the title screen.FR,Other TU/e related Facebook-sites can be selected from a list (see for contents of list requirement UCAR803).FR,The administrator can modify the list of TU/e related Facebook-sites.FR,When the Twitter-app is started the page of TU/e on Twitter is shown on the title screen.FR,Other TU/e related Twitter-sites can be selected from a list (see for contents of list requirement UCAR903).FR,The administrator can modify the list of TU/e related Twitter-sites.
FR,Download mobile application DESC: A user should be able to download the mobile application through either an application store or similar service on the mobile phone. The application should be free to download.FR,"Download and notify users of new releases DESC: When a new/updated version or release of the software is released, the user should check for these manually.The download of the new release should be done through the mobile phone in the same way as downloading the mobile application." FR,"User registration - Mobile application DESC: Given that a user has downloaded the mobile application, then the user should be able to register through the mobile application.The user must provide user-name, password and e-mail address.The user can choose to provide a regularly used phone number." FR,"User log-in - Mobile application DESC: Given that a user has registered, then the user should be able to log in to the mobile application.
The log-in information will be stored on the phone and in the future the user should be logged in automatically." FR,"Retrieve password DESC: Given that a user has registered, then the user should be able to retrieve his/her password by e-mail." FR,"Mobile application - Search DESC: Given that a user is logged in to the mobile application, then the first page that is shown should be the search page. The user should be able to search for a restaurant, according to several search options.The search options are Price, Destination, Restaurant type and Specific dish.There should also be a free text search option.A user should be able to select multiple search options in one search." FR,"Mobile application - Search result in a map view DESC: Search results can be viewed on a map.On the map, the relevant and closest restaurants according to the userâ€™s position are shown.A specific pin will represent a specific restaurant location.On each pin there should be an information link.
There should be maximally 100 results displayed. The map view should have a default zoom.The map view should include a button that, when selected, should display different filtering options in a filtering menu." FR,"Mobile application - Search result in a list view Search results can be viewed in a list.Each element in the list represents a specific restaurant.Each element should include the restaurant name, telephone number, type of food, distance according to the userâ€™s position, average price, a short two-line description, a link to the restaurantâ€™s web-page and an information link.There should be maximally 100 results displayed.If the result contains more restaurants than what can be displayed on the screen at one time, the user should be able to scroll through them.When searching by price the restaurants should be sorted according to the following order: 1.average price 2.distance 3.restaurant type 4.
specific dish When searching by a search option, other than price, the restaurants should be sorted according to the following order: 1. distance 2.average price 3.restaurant type 4.specific dish The list view should include a header with different selectable sorting options.The list view should include a button that, when selected, should display different filtering options in a filtering menu." FR,"Mobile application - Navigation to restaurant DESC: A user should be able to select a pin on a map or an element on a list.When a selection is made, the location of the restaurant should be sent to the mobile phoneâ€™s GPS-navigation program.The user should then be navigated to the destination.When the destination is reached, a user should be able to go back to the search page on the mobile application." FR,Mobile application - Switch result view DESC: A user should be able to switch between a map view and a list view for all search options.
FR,"Mobile application - Selecting the information link DESC: A user should be able to select the information link, which is included on all result items. The link will direct the user to an information page, which includes a picture of the restaurant, the restaurant name, address, phone number, e-mail address, type of food, average price, restaurant description and a menu with name, description and price of the different dishes." FR,Mobile application - Search by price DESC: A user should be able to input a maximum and a minimum price range.The result is displayed in a list view by default.FR,"Mobile application - Search by destination DESC: A user should be able to input a maximum and a minimum distance, according to his/her position.By default the minimum distance is set to 0 km and the maximum to 10 km.The user should be able to input a higher or lower maximum distance and a higher minimum distance than set by default.
The result is displayed in a map view by default." FR,Accepted input for price and destination search DESC: Integers should be accepted as input when a user searches by price or destination. If the system receives an invalid input the user should be informed and prompted to insert an accepted input.FR,Mobile application - Search by restaurant type DESC: A user should be able to select a restaurant type in a given list as input.The result is displayed in a map view by default.FR,Mobile application - Search by specific dish DESC: A user should be able to select a specific dish in a given list as input.The result is displayed in a map view by default.FR,"Mobile application - Free-text search DESC: A user should be able to conduct a search by providing either restaurant name, restaurant description, restaurant address, restaurant type or restaurant menu in the free-text search field.
The result is displayed in a map view by default." FR,Mobile application - No match found DESC: If no match is found the user should be informed but kept on the search page in order to get the possibility to conduct a new search right away. FR,"Mobile application - Sorting results DESC: When viewing the results in a list, a user should be able to sort the results according to price, distance, restaurant type, specific dish or restaurant name.When sorting by restaurant name, specific dish or restaurant type the results should be ordered alphabetically.When sorting by price the results should be ordered from cheapest to most expensive.When sorting by distance the results should be ordered from closets to furthest distance according to the userâ€™s position.When the sort button for a specific search option is clicked, then the order should be reversed and ordered in a descending matter.
If the sort button is clicked again the order of the results should be reversed." FR,"Mobile application - Filtering results DESC: When viewing the results in a list or a map, a user should be able to filter the results in a filtering menu. The filtering options include: increasing or decreasing the maximum distance increasing or decreasing the maximum price choosing a restaurant type choosing a specific dish" FR,"Mobile application - Profile page DESC: On the mobile application, a user should have a profile page.On the profile page a user can edit his/her information, which includes the password, e-mail address and phone number.A user should also be able to choose what language the mobile application should be set to.
The different language choices are Swedish, English, Spanish and French." FR,Required information for registration Given the restaurant owner wants to create an account And the restaurant owner does not have an account When the restaurant owner registers on the web-portal by providing user-name And password And address And e-mail address And phone number Then the restaurant owner should be able to apply for verification FR,Full information for registration Given the restaurant owner wants to create an account And the restaurant owner does not have an account When the restaurant owner registers on the web-portal by providing user name And password And address And e-mail address And phone number And mobile number Then the restaurant owner should be able to apply for verification FR,Confirmed registration Given the restaurant owner has applied for verification And has not received a confirmation e-mail after registration When the restaurant owner receives a confirmation e-mail Then the restaurant owner should be able to log in FR,Successful log-in Given the restaurant owner wants to log in When the restaurant owner logs in with his/her account Then the restaurant owner should be logged in as a restaurant owner FR,Retrieve password Given the restaurant owner wants to log in And has lost the password When the restaurant owner enters his/her email address in the â€œRetrieve passwordâ€ form And submits the form Then the restaurant owner should receive an email containing the password FR,Show fields for managing information Given the restaurant owner is logged in When the restaurant owner wants to manage information Then the restaurant owner should be able to manage information in a form FR,Filling in mandatory fields Given the restaurant owner wants to fill in the mandatory fields of the form When the restaurant owner provides average price And address And e-mail address And phone number And restaurant name Then the restaurant owner has filled the mandatory fields of the form FR,Filling in optional fields Given the restaurant owner of a restaurant wants to fill in optional fields in the form When the restaurant owner provides restaurant description And menu And type of restaurant And picture of restaurant And mobile phone Then the restaurant owner has filled in optional fields in the form FR,Filling in menu field Given the restaurant owner wants to fill in the menu field in the form When the restaurant owner provides dish name And dish description And dish price Then the restaurant owner has filled in the menu field in the form FR,Adding information with mandatory fields Given the restaurant owner has filled in the mandatory fields of the form When the restaurant owner submits the form Then the information about the restaurant should be added FR,Adding information with mandatory and optional fields Given the restaurant owner has filled in the mandatory fields of the form And filled in one or more optional fields of the form When the restaurant owner submits the form Then the information about the restaurant should be added FR,Deleting information Given the restaurant owner is logged in And information exists When the restaurant owner deletes information Then the information should be deleted FR,Editing information Given the restaurant owner is logged in And information exists When the restaurant owner edits information Then the information should be edited FR,Select English as preferred language Given the restaurant owner wants to select a preferred language When the restaurant owner selects English as a new language Then the web-portal will show all text in English FR,Select Swedish as preferred language Given the restaurant owner wants to select a preferred language When the restaurant owner selects Swedish as a new language Then the web-portal will show all text in Swedish FR,Select French as preferred language Given the restaurant owner wants to select a preferred language When the restaurant owner selects French as a new language Then the web-portal will show all text in French FR,Select Spanish as preferred language Given the restaurant owner wants to select a preferred language When the restaurant owner selects Spanish as a new language Then the web-portal will show all text in Spanish FR,Successful log-in Given the administrator wants to log in When the administrator logs in with an administrator account Then the administrator should be logged in as an administrator FR,Verify a restaurant owner Given the administrator is logged in When the administrator verifies a restaurant owner Then the restaurant owner should be able to log in And the restaurant owner should be notified by a confirmation email FR,Reject a restaurant owner Given the administrator is logged in When the administrator rejects a restaurant owner Then the restaurant owner should not be able to log in And the restaurant owner should be notified by a rejection email FR,Add a new restaurant type Given the administrator is logged in When the administrator creates a new restaurant type Then the new restaurant type should be added to the list of restaurant types FR,Editing an existing restaurant type Given the administrator is logged in When the administrator edits an existing restaurant type Then the restaurant type should be updated in the list of restaurant types FR,Delete a restaurant type Given the administrator is logged in When the administrator deletes a restaurant type Then the deleted restaurant type should be removed from the list of restaurant types FR,Add a new dish Given the administrator is logged in When the administrator creates a new dish Then the new dish should be added to the list of dishes FR,Editing an existing dish Given the administrator is logged in When the administrator edits an existing dish Then the dish should be updated in the list of dishes FR,Delete a dish Given the administrator is logged in When the administrator deletes a dish Then the deleted dish should be removed from the list of dishes FR,Add restaurant information Given the administrator is logged in When the administrator adds restaurant information Then the information should be added to the restaurant FR,Delete restaurant information Given the administrator is logged in And information about a restaurant exists When the administrator deletes the information Then the information about the restaurant should be deleted FR,Edit restaurant information Given the administrator is logged in And information about a restaurant exists When the administrator edits the information Then the information about the restaurant should be edited FR,Edit an existing userâ€™s information Given the administrator is logged in When the administrator edits an existing user Then the user information should be updated FR,Delete/Inactivate an existing user Given the administrator is logged in When the administrator deletes an existing user Then the user should be deleted FR,Add a new restaurant owner Given the administrator is logged in When the administrator creates a new restaurant owner Then the new restaurant owner should be added FR,Edit an existing restaurant owner Given the administrator is logged in When the administrator edits an existing restaurant owner Then the restaurant owner information should be updated FR,Delete an existing restaurant owner Given the administrator is logged in When the administrator deletes an existing restaurant owner Then the restaurant owner should be deleted And the restaurant information should be deleted FR,Select English as preferred language Given the administrator wants to select a preferred language When the administrator selects English as a new language Then the web-portal will show all text in English FR,Select Swedish as preferred language Given the administrator wants to select a preferred language When the administrator selects Swedish as a new language Then the web-portal will show all text in Swedish FR,Select French as preferred language Given the administrator wants to select a preferred language When the administrator selects French as a new language Then the web-portal will show all text in French FR,Select Spanish as preferred language Given the administrator wants to select a preferred language When the administrator selects Spanish as a new language Then the web-portal will show all text in Spanish FR,Prominent search feature DESC: The search feature should be prominent and easy to find for the user.
FR,"Usage of the search feature DESC: The different search options should be evident, simple and easy to understand." FR,Usage of the result in the list view DESC: The results displayed in the list view should be user friendly and easy to understand. Selecting an element in the result list should only take one click.FR,Usage of the result in the map view DESC: The results displayed in the map view should be user friendly and easy to understand.Selecting a pin on the map should only take one click.FR,Usage of the information link DESC: The information link should be prominent and it should be evident that it is a usable link.Selecting the information link should only take one click.FR,"In this case, user enters his/her username, name, surname, password, mail address sets a profile photo.When the user fills all necessary fields, an activation mail will be sent to users mail address.
User will be able to log in and start using the system just after activating his/her account." FR,"In this case, user enters his/her username or email and his/her password and logs in the system." FR,"In this case, user touches log out button, and logs out of the system. The user will not use the system until he/she logs in again." FR,"In this case, user is able to search items.User can search items according to their price, their location, their category." FR,"In this case, user is able to browse the profiles other users.User can see the other users user names, profile photos, and the usersâ€™ items." FR,"In this case, user is able to browse categories according to his/her wishes.User can also see how many items each category has." FR,"In this case, user is able browse items.User can see the itemsâ€™ pictures, prices, and other features." FR,"In this case, user can change his/her username, email address, profile picture and password.
If the user changes his/her username or password, a notification mail will be sent to the userâ€™s email address. The changes will apply if the user clicks on the link in the email." FR,"In this case, user can buy credits from the system.
He/she can use credit card or his/her PayPal account." FR,"In this case, user can add the items he/she likes to his cart by touching â€œAdd to the cartâ€ button." FR,"In this case, the user can buy items he/she has added to cart with credits, if the items are available for buying with credits." FR,"In this case, if the item the user wants is available for exchange, the user can request the exchange of items that he/she has with the item he/she wants." FR,"In this case, the user can approve the exchange that is offered by another user by clicking â€œApproveâ€ button." FR,"In this case, the user can decline the exchange that is offered by another user by clicking â€œDeclineâ€ button." FR,"In this case, the user notifies other user that the shipment of the item has started." FR,"In this case, the user can add as many as items he/she wants.
The user can also set the price, set the amount, upload pictures of the item and add details of the item." FR,"In this case, the user can edit the items that he/she has already put on the system. The user can add/remove pictures, change the price and details of the item." FR,"In this case, if the user no longer wishes to sell/exchange the item, he/she can remove the item from the system by touching â€œRemove the itemâ€ button." FR,"In this case, if the user no longer wishes to use the system, he/she can click â€œDelete Profileâ€ button.
After deleting the profile, the user will no longer be able to use the system with the account details that he entered before." FR,"In this case, the user can mark an item as a favorite so that he will be notified if any change is made to the item" FR,"In this case, the user can rate and comment about the users whom he traded with." FR,The client will send a GET request to the Web API with the question as a URL parameter The client will specify the header Content-Type: application/json in their requests as convention. A valid API query is a single URL parameter containing one sentence that is a question in standard English.FR,"The server will reply with either data or an error, see R4.1.3.5.The client will be able to parse the JSON and determine if there was an error.The server will send all API data in JSON response documents with the header Content-Type: application/json.
The server will respond with a 200 OK status code if a request has the header Content-Type: application/json and is a valid API query. The server will respond with a 400 Bad Request status code if a request does not specify the header Content-Type: application/json OR is a malformed API query." FR,"API responses are defined in JSON, specified by [RFC 7159].A JSON object will be the root of every API response.The response document will contain at least one of the following top-level members: Data: the documentâ€™s â€œprimary data,â€ in this case, the response to the clientâ€™s query.Errors: an array of error objects stating what went wrong with the clientâ€™s request, should any issues arise.The top-level members specified in R4.1.3.3 will not coexist in the same JSON document.
If data is present, errors will be absent and vice versa." FR,"This unit will receive a text string from the URL parameter.â€‹ This unit will identify important words in the sentence and replace them with generic representations preceded by an escape character. (example: â€œPISBâ€ becomes â€œ$buildingâ€) â€‹This unit will output the sentence as a string, as described in R4.â€‹This unit will output a map of generic representations to the words they replaced." FR,"This unit will have a list of generic words related specifically to potential queries (examples: building, professor)" FR,"An error during this process means there was a problem parsing the sentence and creating the generic question.In this case, return a message such as â€œSorry, I didnâ€™t understand that.â€" FR,This unit will receive the output sentence from the Generic Question Construction unit as input (R4.2.1.3).FR,This unit will generate a generic answer sentence using the input.
(example: â€œWhat time does $building close?â€ becomes â€œ$building closes at $closetime.â€). This unit will output the generic answer sentence described FR,"If there was an error here, then the unit failed to create a generic answer given a generic sentence.In this case, simply fallback to the error handling described in R4.4.2.1" FR,This unit will receive as input a mapping from the Generic Question Construction unit FR,This unit will receive as input a generic answer from the Generic Answer Construction unit FR,This unit will query the database for data about the elements in the mapping FR,This unit will replace the representations in the generic answer with data.FR,"This unit will output the answer to the original question, as described in R4.4.1.3." FR,"If querying the database did not provide an answer, the system will say that it does not have an answer and provide appropriate website link where the user could find the answer.
Example message, â€œI donâ€™t know when Rush Building closes, but here is the website of the building." FR,"If the system could not find appropriate website associated with the question, the system will return a generic error message such as â€œSorry, I couldnâ€™t find an answer to that.â€" FR,A MySQL database will be used to store all information required to answer questions. FR,"The database is populated by the information extraction unit before the rest of the system is available, so that all information is readilyaccessible." FR,The database will use the generic representations from R4.2.2.1 as table names or column headers for easier retrieval of data.FR,The database will be updated periodically and the API will be unavailable during the update.
FR,Structural data sources will have their data stored in our database FR,"For data with enough structuring, web scraping will be used to programmatically extract all the data needed and store it in our database." FR,"For totally unstructured information, AI libraries will be used to programmatically extract and store any information possible" FR,"Data that is especially hard to extract, for whatever reason, will be manually extracted and added to the database by a developer." FR,The GUI will have a textbox that will accept inputs from a keyboard. FR,"Text box will originally contain a suggestive text question, to guide the user to the format of an appropriate question." FR,The GUI will have a â€œSendâ€ button which sends text from the textbox to the API when clicked FR,The GUI will have a chat window displaying questions sent to the system and responses from the API.
FR,"The chat window will contain all questions and answers from the current session, with a scroll bar if all messages canâ€™t fit on the screen." FR,"If there is a network issue, the chat window will display an error message." FR,There will be a designated phone number that users can send texts to. â€‹ FR,"Texts sent to that number will be sent to the API, then the system will reply to the user with the answer as another text message." FR,"If a question is not understood by our API, the system will send a text containing an example question after the text with the API response." FR,Upon the USB being plugged in the system shall be able to be deployed and operational in less than 1 minute FR,The system shall be able to handle 1000 customers logged in concurrently at the same time.
NIST Trustworthy and Responsible AI NIST AI 600 -1 Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.600 -1 NIST Trustworthy and Responsible AI NIST AI 600 -1 Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.600 -1 July 2024 U.S.Department of Commerce Gina M.Raimondo, Secretary National Institute of Standards and Technology Laurie E.
Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology About AI at NIST : The National Institute of Standards and Technology (NIST) develops measurements, technology, tools, and standards to advance reliable, safe, transparent, explainable, privacy -enhanced, and fair artiﬁcial intelligence (AI) so that its full commercial and societal beneﬁts can be realized without harm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for more than a decade, is also helping to fulﬁll the 2023 Executive Order on Safe, Secure, and Trus tworthy AI.NIST established the U.S.AI Safety Institute and the companion AI Safety Institute Consortium to continue the eﬀorts set in motion by the E.O.to build the science necessary for safe, secure, and trustworthy development and use of AI.
Acknowledgments: This report was accomplished with the many helpful comments and contributions from the community, including the NIST Generative AI Public Working Group, and NIST staﬀ and guest researchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin Stanley, and Elham Tabassi. NIST Technical Series Policies Copyright, Use, and Licensing Statements NIST Technical Series Publication Identifier Syntax Publication History Approved by the NIST Editorial Review Board on 07- 25-2024 Contact Information ai-inquiries@nist.gov National Institute of Standards and Technology Attn: NIST AI Innovation Lab, Information Technology Laboratory 100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899- 8900 Additional Information Additional information about this publication and other NIST AI publications are available at https://airc.nist.gov/Home .
Disclaimer : Certain commercial entities, equipment, or materials may be identiﬁed in this document in order to adequately describe an experimental procedure or concept. Such identiﬁcation is not intended to imply recommendation or endorsement by the National Instit ute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose.Any mention of commercial, non- proﬁt, academic partners, or their products, or references is for information only; it is not intended to imply endorsement or recommendation by any U.S.Government agency.Table of Contents 1.Introduction ..............................................................................................................................................1 2.Overview of Risks Unique to or Exacerbated by GAI .....................................................................2 3.
Suggested Actions to Manage GAI Risks ......................................................................................... 12 Appendix A.Primary GAI Considerations ...............................................................................................47 Appendix B.References ................................................................................................................................54 1 1.Introduction This document is a cross -sectoral proﬁle of and companion resource for the AI Risk Management Framework (AI RMF 1.0) for Generative AI ,1 pursuant to President Biden’s Executive Order (EO) 14110 on Safe, Secure, and Trustworthy Artiﬁcial Intelligence.2 The AI RMF was released in January 2023, and is intended for voluntary use and to improve the ability of organizations to incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems.
A proﬁle is an implementation of the AI RMF functions, categories, and subcategories for a speciﬁc setting , application , or technology – in this case, Generative AI (GAI) – based on the requirements, risk tolerance, and resources of the Framework user. AI RMF proﬁle s assist organizations in deciding how to best manage AI risks in a manner that is well -aligned with their goals, considers legal/regulatory requirements and best practices, and reﬂects risk management priorities.Consistent with other AI RMF proﬁles , this proﬁle oﬀers insights into how risk can be managed across various stages of the AI lifecycle and for GAI as a technology.As GAI covers risks of models or applications that can be used across use cases or sectors, this document is an AI RMF cross -sectoral proﬁ le.
Cross -sectoral proﬁles can be used to govern, map, measure, and manage risks associated with activities or business processes common across sectors, such as the use of large language models (LLMs) , cloud -based services, or acquisition. This document deﬁnes risks that are novel to or exacerbated by the use of GAI.After introducing and describing these risks, the document provides a set of suggested actions to help organizations govern, map, measure, and manage these risks.1 EO 14110 deﬁnes Generative AI as “the class of AI models that emulate the structure and characteristics of input data in order to generate derived synthetic content.This can include images, videos, audio, text, and other digital content.” While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers to generative foundation models .
The foundation model subcategory of “dual -use foundation models” is deﬁned by EO 14110 as “an AI model that is trained on broad data; generally uses self -supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts.” 2 This proﬁle was developed per S ection 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting through the Director of the National Institute of Standards and Technology (NIST), to develop a companion resource to the AI RMF, NIST AI 100 –1, for generative AI. 2 This work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST’s Generative AI Public Working Group (GAI PWG).The GAI PWG was an open, transparent, and collaborative process , facilitated via a virtual workspace , to obtain multistakeholder input on GAI risk management and to inform NIST’s approach.
The focus of the GAI PWG was limited to four primary considerations relevant to GAI : Governance, Content Provenance, Pre -deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the suggested actions in this document primarily address these considerations.Future revisions of this proﬁle will include additional AI RMF subcategories, risks, and suggested actions based on additional considerations of GAI as the space evolve s and empirical evidence indicates additional risks .A glossary of terms pertinent to GAI risk management will be developed and hosted on NIST’s Trustworthy & Responsible AI Resource Center (AIRC), and added to The Language of Trustworthy AI: An In- Depth Glossary of Terms .This document was also informed by public comments and consultations from several Request s for Information.2.
Overview of Risks Unique to or Exacerbated by GAI In the context of the AI RMF, risk refers to the composite measure of an event’s probability (or likelihood) of occurring and the magnitude or degree of the consequences of the corresponding event. Some risks can be assessed as likely to materialize in a given context, particularly those that have been empirically demonstrated in similar contexts.Other risks may be unlikely to materialize in a given context, o r may be more speculative and therefore uncertain.AI risks can diﬀer from or intensify traditional software risks.Likewise, GAI can exacerbate existing AI risks, and creates unique risks.GAI risks can vary along many dimensions: • Stage of the AI lifecycle: Risks can arise during design, development , depl oyment , operation, and/or decommission ing.
• Scope: Risks may exist at individual model or system levels , at the application or implementation level s (i.e., for a speciﬁc use case), or at the ecosystem level – that is, beyond a single system or organizational context . Examples of the latter include the expansion of “ algorithmic monocultures ,3” resulting from repeated use of the same model, or impacts on access to opportunity, labor markets , and the creative economies .4 • Source of risk: Risks may emerge from factors related to the de sign, training, or operation of the GAI model itself, stemming in some cases from GAI model or system inputs , and in other cases , from GAI system outputs.
Many GAI risks, however, originate from human behavior , including 3 “Algorithmic monocultures” refers to the phenomenon in which repeated use of the same model or algorithm in consequential decision- making settings like employment and lending can result in increased susceptibility by systems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm. 4 Many studies have projected the impact of AI on the workforce and labor markets.Fewer studies have examined the impact of GAI on the labor market , though some industry surveys indicate that that both emp loyees and employers are pondering this disruption.3 the abuse, misuse, and unsafe repurposing by humans (adversarial or not ), and others result from interactions between a human and an AI system.• Time scale: GAI risks may materialize abruptly or across extended periods .
Example s include immediate (and/or prolonged) emotional harm and potential risks to physical safety due to the distribution of harmful deepfake images , or the lo ng-term eﬀect of disinformation on soci etal trust in public institutions . The presence of risks and where they fall along the dimensions above will vary depending on the characteristics of the GAI model , system, or use case at hand.These characteristics include but are not limited to GAI model or system architecture, training mechanisms and libraries , data types used for training or ﬁne -tuning , levels of model access or availability of model weights, and application or use case context.Organizations may choose to tailor how they measure GAI risks based on these characteristics .
They may additionally wish to allocate risk management resources relative to the severity and likelihood of negative impact s, including where and how these risks manifest , and their direct and material impacts harms in the context of GAI use. Mitigations for model or system level risks may diﬀer from mitigations for use-case or ecosystem level risks.Importantly, some GAI risks are un known , and are therefore diﬃcult to properly scope or evaluate given the uncertaint y about potential GAI scale, complexity, and capabilities.Other risks may be known but diﬃcult to estimate given the wide range of GAI stakeholders, uses, inputs, and outputs .Challenges with risk estimation are aggravated by a lack of visibility into GAI training data, and the generally immature state of the science of AI measurement and safety today .
This document focuses on risks for which there is an existing empirical evidence base at the time this proﬁle was written ; for example, speculative risks that may potentially arise in more advanced, future GAI systems are not considered . Future updates may incorporate additional risks or provide further details on the risks identiﬁed below.To guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by the development and use of GAI are deﬁned below.5 Each risk is labeled according to the outcome , object, or source of the risk (i.e., some are risks “to ” a subject or domain and others are risks “of” or “from” an issue or theme ).These risks provide a lens through which organizations can frame and execute risk management eﬀorts.To help streamline risk management eﬀorts, each risk is mapped in Section 3 (as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identiﬁed in the AI RMF .
5 These risks can be further categorized by organizations depending on their unique approaches to risk deﬁnition and management. One possible way to further categorize these risks, derived in part from the UK’s International Scientiﬁc Report on the Safety of Advanced AI , could be: 1 ) Technical / Model risks (or risk from malfunction): Confabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; Harmful Bias, and Homogenization ; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities ; Data Privacy; Human -AI Conﬁguration; Obscene, Degrading, and/or Abusive Content; Information Integrity; Information Security; 3) Ecosystem / societal risks (or systemic risks) : Data Privacy; Environmental; Intellectual Property .We also note that some risks are cross -cutting between these categories.4 1.
CBRN Information or Capabilities : Eased access to or synthesis of materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons or other dangerous materials or agents . 2.Confabulation: The production of c onﬁdently stated but erroneous or false content ( known colloquially as “hallucinations” or “fabrications”) by which users may be misled or deceived .6 3.Dangerous , Violent , or Hateful Content : Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self -harm or conduct illegal activities.Includes diﬃculty controlling public exposure to hate ful and disparaging or stereotyping content .4.Data Privacy: Impacts due to l eakage and unauthorized use, disclosure , or de -anonymization of biometric, health, location , or other personally identiﬁable information or sensitive data .7 5.
Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems. 6.Harmful Bias or Homogenization: Ampliﬁcation and exacerbation of historical, societal, and systemic biases ; performance disparities8 between sub- groups or languages , possibly due to non- representative training data , that result in discrimination, ampliﬁcation of biases, or incorrect presumptions about performance ; undesired homogeneity that skews system or model outputs , which may be erroneous, lead to ill- founded decision- making, or amplify harmful biases .7.Human -AI Conﬁguration: Arrangement s of or interactions between a human and an AI system which can result in the human inappropriately anthropomorphizing GAI systems or experiencing algorithmic aversion , automation bias , over -reliance , or emotional entanglement with GAI systems .8.
Information Integrity: Lowered barrier to entry to generate and support the exchange and consumption of content which may not distinguish fact from opinion or ﬁction or acknowledge uncertainties, or could be leveraged for large -scale dis - and mis -information campaigns. 9.Information Security: Lowered barriers for oﬀensive cyber capabilities, including via automated discovery and exploitation of vulnerabilities to ease hacking, malware, phishing, oﬀensive cyber 6 Some commenters have noted that the terms “hallucination” and “fabrication” anthropomorphize GAI, which itself is a risk related to GAI systems as it can inappropriately attribute human characteristics to non -human entities.
7 What is categorized as sensitive data or sensitive PII can be highly contextual based on the nature of the information, but examples of sensitive information include information that relates to an information subject’s most intimate sphere , including political opinions, sex life, or criminal convictions . 8 The notion of harm presumes some baseline scenario that the harmful factor (e.g., a GAI model) makes worse .When the mechanism for potential harm is a disparity between groups, it can be diﬃcult to establish what the most appropriate baseline is to compare against, which can result in divergent views on when a disparity between AI behaviors for diﬀerent subgroups constitutes a harm.In discussing harms from disparities such as biased behavior, t his document highlights examples where someone’s situation is worsened relative to what it would have been in the absence of any AI system , making the outcome unambiguously a harm of the system .
5 operations , or other cyberattacks ; increas ed attack surface for targeted cyberattacks , which may compromise a system’s availability or the conﬁdentiality or integrity of training data, code, or model weights. 10.Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or licensed content without authorization (possibly in situations which do not fall under fair use ); eased exposure of trade secrets; or plagiari sm or illegal replication .11.Obscen e, Degrading, and/or A busive Content : Eased production of and access to obscene , degrading, and/or abusive imagery which can cause harm , including synthetic child sexual abuse material (CSAM) , and nonconsensual intimate images (NCII) of adults .12.
Value Chain and Component Integration : Non-transparent or untraceable integration of upstream third- party components, including data that has been improperly obtained or not processed and cleaned due to increased automation from GAI; improper supplier vetting across the AI lifecycle ; or other issues that diminish transparency or accountability for downstream users. 2.1.CBRN Information or Capabilities In the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant knowledge, information , materials, tools, or technologies that could be misused to assist in the design, development, production, or use of CBRN weapons or other dangerous materials or agents .While relevant biological and chemical threat knowledge and information is often publicly accessible , LLMs could facilitate its analysis or synthesis , particularly by individuals without formal scientiﬁc training or expertise.
Recent research on this topic found that LLM outputs regarding biological threat creation and attack planning pr ovided minima l assistance beyond traditional search engine queries, suggesting that state -of- the-art LLMs at the time these studies were conducted do not substantially increase the operational likelihood of such an attack. The physical synthesis development, production, and use of chemical or biological agents will continue to require both applicable expertise and supporting materials and infrastructure .The impact of GAI on chemical or biological agent misuse will depend on what the key barriers for malicious actors are (e.g., whether information access is one such barrier ), and how well GAI can help actors address those barriers .
Furthermore , chemical and biological design tools (BDTs) – highly specialized AI systems trained on scientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry and biology beyond what text -based LLMs are able to provide . As these models become more eﬃcacious , including for beneﬁcial uses, it will be important to assess their potential to be used for harm, such as the ideation and design of novel harmful chemical or biological agents .While some of these described capabilities lie beyond the reach of existing GAI tools, ongoing assessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN weapons planning and GAI systems’ connection or access to relevant data and tools .Trustworthy AI Characteristic : Safe , Explainable and Interpretable 6 2.2.
Confabulation “Confabulation” refers to a phenomenon in which GAI systems generate and conﬁdently present erroneous or false content in response to prompts . Confabulations also include generated outputs that diverge from the prompts or other input or that contradict previously generated statements in the same context.Th ese phenomena are colloquially also referred to as “hallucination s” or “fabrication s.” Confabulations can occur across GAI outputs and contexts .9,10 Confabulations are a natural result of the way generative models are designed : they generate outputs that approximate the statistical distribution of their training data ; for example, LLMs predict the next token or word in a sentence or phrase .While such statistical prediction can produce factual ly accurate and consistent outputs , it can also produce outputs that are factually inaccurat e or internally inconsistent .
This dynamic is particularly relevant when it comes to open -ended prompts for long- form responses and in domains which require highly contextual and/or domain expertise. Risks from confabulations may arise when users believe false content – often due to the conﬁdent nature of the response – leading users to act upon or promote the false information.This poses a challenge for many real -world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments.Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.GAI outputs may also include confabulated logic or citations that purport to justify or explain the system’s answer , which may further mislead humans into inappropriately trusting the system’s output.
For instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the answer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, potentially deceiv ing humans into believing they are speaking with another human.The extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the potential risks from adversarial prompting of such behavior are emerging areas of study .Given the wide range of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of confabulations .Trustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable , Explainable and Interpretable 2.3.Dangerous , Violent , or Hateful Content GAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁ es violence , with greater ease and scale than other technologies .
LLMs have been reported to generate dangerous or violent recommendations , and s ome models have generated actionable instructions for dangerous or 9 Confabulations of falsehoods are most commonly a problem for text -based outputs; for audio, image, or video content, creative generation of non- factual content can be a desired behavior. 10 For example, legal confabulations have been shown to be pervasive in current state -of-the-art LLMs.See also, e.g., 7 unethical behavior .Text -to-image models also make it easy to create images that could be used to promote dangerous or violent message s.Similar concerns are present for other GAI media, including video and audio.GAI may also produce content that recommends self -harm or criminal/illegal activities.
Many current systems restrict model outputs to limit certain content or in response to certain prompts, but this approach may still produce harmful recommendations in response to other less -explicit, novel prompts (also relevant to CBRN Information or Capabilities , Data Privacy , Information Security , and Obscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known a s “jailbreaking ,” or, manipulating prompts to circumvent output controls .Limitations of GAI system s can be harmful or dangerous in certain contexts.Studies have observed that users may disclose mental health issues i n conversations with chatbots – and that users exhibit negative reactions to unhelpful responses from these chatbot s during situations of distress .This risk encompasses diﬃculty controlling creation of and public exposure to oﬀensive or hateful language , and denigrating or stereotypical content generated by AI.
This kind of speech may contribute to downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or stereotypical content can also further exacerbate representational harms (see Harmful Bias and Homogenization below).Trustworthy AI Characteristics: Safe, Secure and Resilient 2.4.Data Privacy GAI systems raise several risks to privacy.GAI system training requires large volumes of data, which in some cases may include personal data.The use of personal data for GAI training raises risks to widely accepted privacy principles , including to transparency, individual participation (including consent), and purpose speciﬁcation.For example, most model developers do not disclose speciﬁc data sources on which models were trained, limiting user awareness of whether personally identiﬁably information (PII) was trained on and, if so, how it was collected .Models may leak, generate, or correctly infer sensitive information about individuals .
For example, during adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was included in their training data. This problem has been referred to as data memorization , and may pose exacerbated privacy risks even for data present only in a small number of training samples .In addition to revealing sensitive information in GAI training data , GAI models may be able to correctly infer P II or sensitive data that was not in their training data nor disclosed by the user by stitching together information from disparate sources .These i nferences can have negative impact on an individual even if the inferences are not accurate (e.g., confabulations), and especially if they reveal information that the individual considers sensitive or that is used to disadvantage or harm them.Beyond harms from information exposure (such as extortion or dignitary harm ), wrong or inappropriate inferences of PII can contribute to downstream or secondary harmful impacts .
For example, predictive inferences made by GAI models based on PII or protected attributes c an contribute to adverse decisions , leading to representational or allocative harms to individuals or groups (see Harmful Bias and Homogenization below). 8 Trustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and Resilient 2.5.Environmental Impacts Training, maint aining, and operating (running inference on) GAI systems are resource -intensive activities , with potentially large energy and environmental footprints.Energy and carbon emissions vary based on what is being done with the GAI model (i.e., pre -training, ﬁne -tuning, inference), the modality of the content , hardware used, and type of task or application .Current e stimates suggest that training a single transformer LLM can emit as much carbon as 300 round- trip ﬂights between San Francisco and New York.
In a study comparing energy consumption and carbon emissions for LLM inference, generative tasks ( e.g., text summarization) were found to be more energy - and carbon -i ntensive th an discriminative or non- generative tasks (e.g., text classiﬁcation). Methods for creating smaller versions of train ed models, such as model distillation or compression, could reduce environmental impacts at inference time, but training and tuning such models may still contribute to their environmental impacts .Currently there is no agreed upon method to estimate environmental impacts from GAI .Trustworthy AI Characteristics: Accountable and Transparent, Safe 2.6.Harmful Bias and Homogenization Bias exists in many forms and can become ingrained in automated systems.AI systems , including GAI systems, can increase the speed and scale at which harmful biases manifest and are acted upon, potentially perpetuati ng and amplify ing harms to individuals, groups, communities, organizations, and society .
For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current text-to-image models underrepresent women and/or racial minorities , and people with disabilities . Image generator models have also produce d biased or stereotyped output for various demographic groups and have diﬃculty producing non- stereotyped content even when the prompt speciﬁcally requests image features that are inconsistent with the stereotypes.Harmful b ias in GAI models , which may stem from their training data , can also cause representational harm s or perpetuate or exacerbate bias based on race, gender, disability, or other protected classes .Harmful b ias in GAI systems can also lead to harms via disparities between how a model performs for diﬀerent subgroups or languages (e.g., an LLM may perform less well for non- English languages or certain dialects ).Such disparities can contribute to discriminatory decision -making or ampliﬁcation of existing societal biases.
In addition, GAI systems may be inappropriately trusted to perform similarly across all subgroups , which could leave the groups facing underperformance with worse outcomes than if no GAI system were used. Disparate or r educed performance for low er-resource languages also presents challenges to model adoption, inclusion, and accessibility , and may make preservation of endangered languages more diﬃcult if GAI systems become embedded in everyday processes that would otherwise have been opportunities to use the se languages .Bias is mutually reinforcing with the problem of undesired homogenization , in which GAI systems produce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles 9 and reduce d content diversity ).Overly homogenized outputs can themselves be incorrect, or they may lead to unreliable decision -making or amplify harmful biases.
These phenomena can ﬂow from foundation models to downstream models and systems , with the foundation models acting as “bottlenecks ,” or single points of failure. Overly homogenized content can contribute to “model collapse .” Model collapse can occur when model training over -relies on synthetic data , resulting in data points disappearing from the distribution of the new model’s outputs.In addition to threat ening the robustness of the model overall, m odel collapse could lead to homogenized outputs, including by amplifying any homogenization from the model used to generate the synthetic training data .Trustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable 2.7.Human -AI Conﬁguration GAI system use can involve varying risks of misconﬁgurations and poor interactions between a system and a human who is interacti ng with it.
Humans bring their unique perspectives , experiences , or domain - speciﬁc expertise to interactions with AI systems but may not have detailed knowledge of AI systems and how they work. As a result, h uman experts may be unnecessarily “averse ” to GAI systems , and thus deprive themselves or others of GAI’s beneﬁcial uses .Conversely , due to the complexity and increasing reliability of GAI technology, over time, human s may over -rely on GAI systems or may unjustiﬁably perceive GAI content to be of higher quality than that produced by other sources .This phenomenon is an example of automation bias , or excessive deference to automated systems .Automation bias can exacerbate other risks of GAI, such as risks of confabulation or risks of bias or homogenization.There may also be concerns about emotional entanglement between humans and GAI systems, which could lead to negative psychological impacts .
Trustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with Harmful Bias Managed, Privacy Enhanced, Safe , Valid and Reliable 2.8. Information Integrity Information integrity describes the “ spectrum of information and associated patterns of its creation, exchange, and consumption in society .” High-integrity information can be trusted; “distinguishes fact from ﬁction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of vetting.This information can be linked to the original source(s) with appropriate evidence.High- integrity information is also accurate and reliable, can be veriﬁed and authenticated, has a clear chain of custody, and creates reasonable expectations about when its validity may expire.”11 11 This deﬁnition of information integrity is derived from the 2022 White House Roadmap for Researchers on Priorities Related to Information Integrity Research and Development.
10 GAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading content (misinformation) at scale , particularly if the content stems from confabulations. GAI systems can also ease the deliberate production or dis semination of false or misleading information (disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others.Even very subtle changes to text or images can manipulate human and machine perception.Similarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce disinformation that is targeted towards speciﬁc demographics.Current and e merging multimodal models make it possible to generate both text-based disinformation and highly realistic “ deepfakes ” – that is, synthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be enabled by future GAI models trained on new data modalities.
Disinformation and misinformation – both of which may be facilitated by GAI – may erode public trust in true or valid evidence and information, with downstream eﬀects. For example, a synthetic image of a Pentagon blast went viral and brieﬂy caused a drop in the stock market.Generative AI models can also assist malicious actors in creating compelling imagery and propaganda to support disinformation campaigns, which may not be photorealistic, but could enable these campaigns to ga in more reach and engagement on social media platforms.Additionally, g enerative AI models can assist malicious actors in creating fraudulent content intended to impersonate others.Trustworthy AI Characteristics: Accountable and Transparent, Safe , Valid and Reliable , Interpretable and Explainable 2.9.Information Security Information security for computer systems and data is a mature ﬁeld with widely accepted and standardized practices for oﬀensive and defensive cyber capabilities .
GAI -based systems present two primary information security risks: GAI could potentially discover or enable new cybersecurity risks by lowering the barriers for or easing automated exercise of oﬀensive capabilities ; simultaneously , it expands the available attack surface , as GAI itself is vulnerable to attacks like prompt injection or data poisoning. Oﬀensive cyber capabilities advanced by GAI systems may augment cyber security attacks such as hacking, malware, and phishing.Reports have indicated that LLMs are already able to discover some vulnerabilities in systems (hardware, software, data) and write code to exploit them .Sophisticated threat actors might further these risks by developing GAI- powered security co -pilots for use in several parts of the attack chain, including informing attackers on how to proactively evade threat detection and escalate privilege s after gaining system access.
Information security for GAI models and systems also includes maintaining availability of the GAI system and the integrity and (when applicable ) the conﬁdentiality of the GAI code, training data, and model weights. To identify and secur e potential attack points in AI systems or speciﬁc components of the AI 12 See also https://doi.org/10.6028/NIST.AI.100-4 , to be published.11 value chain (e.g., data inputs , processing, GAI training, or deployment environments ), conventional cybersecurity practices may need to adapt or evolve .For instance , prompt injection involves modifying what input is provided to a GAI system so that it behave s in unintended ways.In direct prompt injections, attackers might craft malicious prompts and input them directly to a GAI system , with a variety of downstream negative consequences to interconnected systems.
Indirect prompt injection attacks occur when adversaries remotely (i.e., without a direct interface) exploit LLM -integrated applications by injecting prompts into data likely to be retrieved. Security researchers have already demonstrated how indirect prompt injections can exploit vulnerabilities by steal ing proprietary data or running malicious code remotely on a machine.Merely querying a closed production model can elicit previously undisclosed information about that model .Another cybersecurity risk to GAI is data poisoning , in which an adversary compromises a training dataset used by a model to manipulate its output s or operation.Malicious tampering with data or parts of the model could exacerbate risks associated with GAI system outputs.Trustworthy AI Characteristics: Privacy Enhanced, Safe , Secure and Resilient , Valid and Reliable 2.10.
Intellectual Property Intellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair use under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI outputs displaying instances of training data memorization (see Data Privacy above) could infringe on copyright.How GAI relates to copyright, including the status of generated content that is similar to but does not strictly copy w ork protected by copyright , is currently being debated in legal fora.Similar discussions are taking place regard ing the use or emulation of personal identit y, likeness , or voice without permission.Trustworthy AI Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy Enhanced 2.11.Obscene, Degrading, and/or A busive Content GAI can ease the production of and access to illegal non- consensual intimate imagery (NCII) of adult s, and/or child sexual abuse material (CSAM) .
GAI- generated obscene, abusive or degrading content can create privacy, psychological and emotional, and even physical harms , and in some cases may be illegal . Generated explicit or obscene AI content may include highly realistic “deepfakes” of real individuals, including children.The spread of this kind of material can have downstream negative consequences : in the context of CSAM, e ven if the generated images do not resemble speciﬁc individuals, the prevalence of such images c an divert time and re sources from eﬀorts to ﬁnd real -world victims.Outside of CSAM, the creation and spread of NCII disproportionately impact s women and sexual minorities , and can have subsequent negative consequences including decline in overall mental health, substance abuse, and even suicidal thoughts .Data used for training G AI models may unintentional ly inclu de CSAM and NCII.
A recent report noted that several commonly used GAI training datasets were found to contain hundreds of known images of 12 CSAM. Even when trained on “clean” data, increasingly capable GAI models can synthesize or produce synthetic NCII and CSAM.Websites, mobile apps, and custom -built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.Trustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe , Privacy Enhanced 2.12.Value Chain and Component Integration GAI value chains involve many third -party components such as procured datasets, pre -trained models, and software libraries.These components might be improperly obtained or not properly vetted, leading to diminished transparency or accountability for downstream users.
While this is a risk for traditional AI systems and some other digital technologies , the risk is exacerbated for GAI due to the scale of the training data, which may be too large for humans to vet; the diﬃculty of training foundation models, which leads to extensive reuse of limited numbers of models; an d the extent to which GAI may be integrat ed into other devices and services. As GAI systems often involve many distinct third -party components and data sources , it may be diﬃcult to attribute issues in a system’s behavior to any one of these sources.Errors in t hird-party GAI components can also have downstream impacts on accuracy and robustness .For example, test datasets commonly used to benchmark or validate models can contain label errors .Inaccuracies in these labels can impact the “stability” or robustness of these benchmarks, which many GAI practitioners consider during the model selection process.
Trustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with Harmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable 3. Suggested Actions to Manage GAI Risks The following suggested actions target risks unique to or exacerbated by GAI.In addition to the suggested actions below , AI risk management activities and actions set forth in the AI RMF 1.0 and Playbook are already applicable for managing GAI risks .Organizations are encouraged to apply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems .Implementation of the suggested actions will vary depending on the type of risk, characteristics of GAI systems, stage of the GAI lifecycle, and relevant AI actors involved.
Suggested actions to manage GAI risks can be found in the tables below : • The suggested actions are organized by relevant AI RMF subcategor ies to streamline these activities alongside implementation of the AI RMF. • Not every subcategory of the AI RMF is included in this document.13 Suggested actions are listed for only some subcategories.13 As this document was focused on the GAI PWG eﬀorts and primary considerations (see Appendix A) , AI RMF subcategories not addressed here may be added later .13 • Not every suggested action appl ies to every AI Actor14 or is relevant to every AI Actor Task .For example, suggested actions relevant to GAI developers may not be relevant to GAI deployers.The applicability of suggested actions to relevant AI actors should be determined based on organizational considerations and their unique uses of GAI systems .
Each table of suggest ed actions includes: • Action ID: Each Action ID corresponds to the relevant AI RMF function and subcatego ry (e.g., GV - 1.1-001 corresponds to the ﬁrst suggested action for Govern 1.1, GV-1.1-002 corresponds to the second suggested action for Govern 1.1). AI RMF functions are tagged as follows: GV = Govern; MP = Map; MS = Measure; MG = Manage .• Suggested Action: Steps an organization or AI actor can take to manage GAI risks .• GAI Risks: Tags linking suggested action s with relevant GAI risks .• AI Actor Tasks: Pertinent AI Actor Tasks for each subcategory .Not every AI Actor Task listed will apply to every suggested action in the subcategory (i.e., some apply to AI development and others apply to AI deployment).The tables below begin with the AI RMF subcategory, shaded in blue, followed by suggested actions.GOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.
Action ID Suggested Action GAI Risks GV-1.1-001 Align GAI development and use with applicable laws and regulations, including those related to data privacy , copyright and intellectual property law. Data Privacy ; Harmful Bias and Homogenization ; Intellectual Property AI Actor Tasks : Governance and Oversight 14 AI Actors are deﬁned by the OECD as “those who play an active role in the AI system lifecycle, including organizations and individuals that deploy or operate AI .” See Appendix A of the AI RMF for additional descriptions of AI Actors and AI Actor Tasks.14 GOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices.Action ID Suggested Action GAI Risks GV-1.2-001 Establish transparency policies and processes for documenting the origin and history of training data and generated data for GAI applications to advance digital content transparency , while balancing the proprietary nature of training approaches .
Data Privacy ; Information Integrity ; Intellectual Property GV-1.2-0 02 Establish policies to evaluate risk -relevant capabilities of GAI and robustness of safety measures, both prior to deployment and on an ongoing basis, through internal and external evaluations. CBRN Information or Capabilities ; Information Security AI Actor Tasks: Governance and Oversight GOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization’s risk tolerance.
Action ID Suggested Action GAI Risks GV-1.3-001 Consider the following factors when updating or deﬁning risk tiers for GAI: Abuses and impacts to information integrity; Dependencies between GAI and other IT or data systems; Harm to fundamental rights or public safety ; Presentation of obscene, objectionable, oﬀensive, discrimina tory, invalid or untruthful output; Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use ; Whether the system introduces signiﬁcant new security vulnerabilities ; Anticipated system impact on some groups compared to others ; Unreliable decision making capabilities, validity, adaptability, and variability of GAI system performance over time.
Information Integrity ; Obscene, Degrading, and/or Abusive Content ; Value Chain and Component Integration; Harmful Bias and Homogenization ; Dangerous , Violent , or Hateful Content ; CBRN Information or Capabilities GV-1.3-002 Establish minimum thresholds for performance or assurance criteria and review as part of deployment approval (“go/”no -go”) policies, procedures, and processes, with reviewed processes and approval thresholds reﬂecting measurement of GAI capabilities and risks. CBRN Information or Capabilities ; Confabulation ; Dangerous , Violent, or Hateful Content GV-1.3-003 Establish a test plan and response policy, before developing highly capable models, to periodically evaluate whether the model may misuse CBRN information or capabilities and/or oﬀensive cyber capabilities.
CBRN Information or Capabilities ; Information Security 15 GV-1.3-004 Obtain input from stakeholder communities to identify unacceptable use , in accordance with activities in the AI RMF Map function . CBRN Information or Capabilities ; Obscene, Degrading, and/or Abusive Content ; Harmful Bias and Homogenization ; Dangerous, Violent, or Hateful Content GV-1.3-005 Maintain an updated hierarch y of identiﬁed and expected GAI risks connected to contexts of GAI model advancement and use, potentially including specialized risk levels for GAI systems that address issues such as model collapse and algorithmic monoculture.
Harmful Bias and Homogenization GV-1.3-006 Reevaluate organizational risk tolerances to account for unacceptable negative risk (such as where signiﬁcant negative impacts are imminent, severe harms are actually occurring, or large -scale risks could occur); and broad GAI negative risks, including: Immature safety or risk cultures related to AI and GAI design, development and deployment, public information integrity risks, including impacts on democratic processes, unknown long -term performance characteristics of GAI. Information Integrity ; Dangerous , Violent, or Hateful Content ; CBRN Information or Capabilities GV-1.3-007 Devise a plan to halt development or deployment of a GAI system that poses unacceptable negative risk.
CBRN Information and Capability ; Information Security ; Information Integrity AI Actor Tasks: Governance and Oversight GOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities. Action ID Suggested Action GAI Risks GV-1.4-001 Establish policies and mechanisms to prevent GAI systems from generating CSAM, NCII or content that violates the law.Obscene, Degrading, and/or Abusive Content ; Harmful Bias and Homogenization ; Dangerous, Violent, or Hateful Content GV-1.4-002 Establish transparent acceptable use policies for GAI that address illegal use or applications of GAI.
CBRN Information or Capabilities ; Obscene, Degrading, and/or Abusive Content ; Data Privacy ; Civil Rights violations AI Actor Tasks: AI Development, AI Deployment, Governance and Oversight 16 GOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and organizational roles and responsibilities are clearly deﬁned, including determining the frequency of periodic review. Action ID Suggested Action GAI Risks GV-1.5-001 Deﬁne organizational responsibilities for periodic review of content provenance and incident monitoring for GAI systems.Information Integrity GV-1.5-002 Establish organizational policies and procedures for after action reviews of GAI system incident response and incident disclosures, to identify gaps; Update incident response and incident disclosure processes as required.
Human -AI Conﬁguration ; Information Security GV-1.5-003 Maintain a document retention policy to keep history for test, evaluation, validation, and veriﬁcation (TEVV) , and digital content transparency methods for GAI. Information Integrity ; Intellectual Property AI Actor Tasks: Governance and Oversight, Operation and Monitoring GOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.Action ID Suggested Action GAI Risks GV-1.6-001 Enumerate organizational GAI systems for incorporation into AI system inventory and adjust AI system inventory requirements to account for GAI risks.Information Security GV-1.6-002 Deﬁne any inventory exemptions in organizational policies for GAI systems embedded into application software .
Value Chain and Component Integration GV-1.6-003 In addition to general model, governance, and risk information, consider the following items in GAI system inventory entries: Data provenance information (e.g., source, signatures, versioning, watermarks); Known issues reported from internal bug tracking o r external information sharing resources (e.g., AI incident database , AVID , CVE, NVD , or OECD AI incident monitor ); Human oversight roles and responsibilities; Special rights and considerations for intellectual property, licensed works, or personal, privileged, proprietary or sensitive data; Underlying foundation models, versions of underlying models, and access modes .
Data Privacy ; Human -AI Conﬁguration; Information Integrity ; Intellectual Property ; Value Chain and Component Integration AI Actor Tasks: Governance and Oversight 17 GOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization’s trustworthiness. Action ID Suggested Action GAI Risks GV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when necessary .Information Security ; Value Chain and Component Integration GV-1.7-002 Consider the following factors when decommissioning GAI systems: Data retention requirements; Data security, e.g., containment, protocols, Data leakage after decommissioning; Dependencies between upstream, downstream, or other data, internet of things (IOT) or AI systems; Use of open -source data or models; Users’ emotional entanglement with GAI functions.
Human -AI Conﬁguration ; Information Security ; Value Chain and Component Integration AI Actor Tasks: AI Deployment, Operation and Monitoring GOVERN 2.1: Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization. Action ID Suggested Action GAI Risks GV-2.1-001 Establish organizational roles, policies, and procedures for communicating GAI incidents and performance to AI Actors and downstream stakeholders (including those potentially impacted ), via community or oﬃcial resources (e.g., AI incident database , AVID , CVE, NVD , or OECD AI incident monitor ).Human -AI Conﬁguration ; Value Chain and Component Integration GV-2.1-002 Establish procedures to engage teams for GAI system incident response with diverse composition and responsibilities based on the particular incident type.
Harmful Bias and Homogenization GV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training. Human -AI Conﬁguration GV-2.1-004 When systems may raise national security risks, involve national security professionals in mapping, measuring, and managing those risks .CBRN Information or Capabilities ; Dangerous, Violent, or Hateful Content ; Information Security GV-2.1-005 Create mechanisms to provide protections for whis tleblowers who report, based on reasonable belief, when the organization violates relevant laws or poses a speciﬁc and empirically well-substantiated negative risk to public safety (or has already caused harm) .
CBRN Information or Capabilities ; Dangerous , Violent, or Hateful Content AI Actor Tasks: Governance and Oversight 18 GOVERN 3.2: Policies and procedures are in place to deﬁne and diﬀerentiate roles and responsibilities for human -AI conﬁgurations and oversight of AI systems. Action ID Suggested Action GAI Risks GV-3.2-001 Policies are in place to bolster oversight of GAI systems with independent evaluations or assessments of GAI models or systems where the type and robustness of evaluations are proportional to the identiﬁed risks.CBRN Information or Capabilities ; Harmful Bias and Homogenization GV-3.2-002 Consider adjustment of organizational roles and components across lifecycle stages of large or complex GAI systems, including: Test and evaluation, validation, and red- teaming of GAI systems; GAI content moderation; GAI system development and engineering; Increased accessibility of GAI tools, interfaces, and systems, Incident response and containment.
Human -AI Conﬁguration ; Information Security ; Harmful Bias and Homogenization GV-3.2-003 Deﬁne acceptable use policies for GAI interfaces, modalities, and human -AI conﬁgurations (i.e., for chatbots and decision -making tasks) , including criteria for the kinds of queries GAI applications should refuse to respond to. Human -AI Conﬁguration GV-3.2-004 Establish policies for user feedback mechanisms for GAI systems which include thorough instructions and any mechanisms for recourse .Human -AI Conﬁguration GV-3.2-005 Engage in threat modeling to anticipate potential risks from GAI systems.CBRN Information or Capabilities ; Information Security AI Actors: AI Design GOVERN 4.1: Organizational policies and practices are in place to foster a critical thinking and safety -ﬁrst mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts.
Action ID Suggested Action GAI Risks GV-4.1-001 Establish policies and procedures that address continual improvement processes for GAI risk measurement . Address general risks associated with a lack of explainability and transparency in GAI systems by using ample documentation and techniques such as: application of gradient -based attributions, occlusion/term reduction, counterfactual prompts and prompt eng ineering, and analysis of embeddings; Assess and update risk measurement approaches at regular cadences.Confabulation GV-4.1-002 Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red -teaming or independent external evaluations .
CBRN Information and Capability ; Value Chain and Component Integration 19 GV-4.1-003 Establish policies, procedures, and processes for oversight functions (e.g., senior leadership, legal, compliance, including internal evaluation ) across the GAI lifecycle, from problem formulation and supply chains to system decommission. Value Chain and Component Integration AI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring GOVERN 4.2: Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.Action ID Suggested Action GAI Risks GV-4.2-001 Establish terms of use and terms of service for GAI systems .Intellectual Property ; Dangerous , Violent, or Hateful Content ; Obscene, Degrading, and/or Abusive Content GV-4.2-002 Include relevant AI Actors in the GAI system risk identiﬁcation process.
Human -AI Conﬁguration GV-4.2-0 03 Verify that downstream GAI system impacts (such as the use of third -party plugins) are included in the impact documentation process. Value Chain and Component Integration AI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring GOVERN 4.3: Organizational practices are in place to enable AI testing, identiﬁcation of incidents, and information sharing.Action ID Suggested Action GAI Risks GV4.3-- 001 Establish policies for measuring the eﬀectiveness of employed content provenance methodologies (e.g., cryptography, watermarking, steganography, etc.) Information Integrity GV-4.3-002 Establish o rganizational practices to identify the minimum set of criteria necessary for GAI system incident reporting such as: System ID (auto -generated most likely), Title, Reporter, System/Source, Data Reported, Date of Incident, Description, Impact(s), Stakeholder(s) Impacted.
Information Security 20 GV-4.3-003 Verify information sharing and feedback mechanisms among individuals and organizations regarding any negative impact from GAI systems. Information Integrity ; Data Privacy AI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight GOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.Action ID Suggested Action GAI Risks GV-5.1-001 Allocate time and resources for outreach, feedback, and recourse processes in GAI system development.Human -AI Conﬁguration ; Harmful Bias and Homogenization GV-5.1-002 Document interactions with GAI systems to users prior to interactive activities , particularly in contexts involving more signiﬁcant risks.
Human -AI Conﬁguration ; Confabulation AI Actor Tasks: AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight GOVERN 6.1: Policies and procedures are in place that address AI risks associated with third -party entities, including risks of infringement of a third -party’s intellectual property or other rights. Action ID Suggested Action GAI Risks GV-6.1-001 Categorize diﬀerent types of GAI content with associated third -party rights (e.g., copyright, intellectual property, data privacy).Data Privacy ; Intellectual Property ; Value Chain and Component Integration GV-6.1-002 Conduct joint educational activities and events in collaboration with third parties to promote best practices for managing GAI risks.Value Chain and Component Integration GV-6.1-003 Develop and validate approaches for measuring the success of content provenance management eﬀorts with third parties (e.g., incidents detected and response times).
Information Integrity ; Value Chain and Component Integration GV-6.1-004 Draft and maintain well -deﬁned contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, security requirements, and content provenance expectations for GAI systems . Information Integrity ; Information Security ; Intellectual Property 21 GV-6.1-0 05 Implement a use-cased based supplier risk assessment framework to evaluate and monitor third -party entities’ performance and adherence to content provenance standards and technologies to detect anomalies and unauthorized changes; services acquisition and value chain risk management; and legal compliance .Data Privacy ; Information Integrity ; Information Security ; Intellectual Property ; Value Chain and Component Integration GV-6.1-0 06 Include clauses in contracts which allow an organization to evaluate third -party GAI processes and standards.
Information Integrity GV-6.1-0 07 Inventory all third -party entities with access to organizational content and establish approved GAI technology and service provider lists. Value Chain and Component Integration GV-6.1-0 08 Maintain records of changes to content made by third parties to promote content provenance, including sources, timestamps, metadata .Information Integrity ; Value Chain and Component Integration; Intellectual Property GV-6.1-0 09 Update and integrate due diligence processes for GAI acquisition and procurement vendor assessments to include intellectual property, data privacy, security, and other risks.
For example, update p rocesses to: Address solutions that may rely on embedded GAI technologies; Address ongoing monitoring , assessments, and alerting, dynamic risk assessments, and real -time reporting tools for monitoring third -party GAI risks; Consider policy adjustments across GAI modeling libraries, tools and APIs, ﬁne -tuned models, and embedded tools; Assess GAI vendors, open -source or proprietary GAI tools, or GAI service providers against incident or vulnerability databases . Data Privacy ; Human -AI Conﬁguration; Information Security ; Intellectual Property ; Value Chain and Component Integration ; Harmful Bias and Homogenization GV-6.1-0 10 Update GAI acceptable use policies to address proprietary and open -source GAI technologies and data, and contractors, consultants, and other third -party personnel.
Intellectual Property ; Value Chain and Component Integration AI Actor Tasks: Operation and Monitoring, Procurement, Third -party entities GOVERN 6.2: Contingency processes are in place to handle failures or incidents in third -party data or AI systems deemed to be high -risk. Action ID Suggested Action GAI Risks GV-6.2-001 Document GAI risks associa ted with system value chain to identify over -reliance on third -party data and to identify fallbacks.Value Chain and Component Integration GV-6.2-002 Document incidents involving third -party GAI data and systems, including open - data and open- source software.
Intellectual Property ; Value Chain and Component Integration 22 GV-6.2-003 Establish incident response plans for third -party GAI technologies: Align incident response plans with impacts enumerated in MAP 5.1; Communicate third- party GAI incident response plans to all relevant AI Actors ; Deﬁne ownership of GAI incident response functions; Rehearse third- party GAI incident response plans at a regular cadence; Improve incident response plans based on retrospective learning; Review incident response plans for alignment with relevant breach reporting, data protection, data privacy, or other laws. Data Privacy ; Human -AI Conﬁguration; Information Security ; Value Chain and Component Integration; Harmful Bias and Homogenization GV-6.2-0 04 Establish policies and procedures for continuous monitoring of third -party GAI systems in deployment.
Value Chain and Component Integration GV-6.2-0 05 Establish policies and procedures that address GAI data redundancy, including model weights and other system artifacts. Harmful Bias and Homogenization GV-6.2-0 06 Establish policies and procedures to test and manage risks related to rollover and fallback technologies for GAI systems, acknowledging that rollover and fallback may include manual processing.Information Integrity GV-6.2-0 07 Review vendor contracts and avoid arbitrary or capricious termination of critical GAI technologies or vendor services and non-standard terms that may amplify or defer liability in unexpected ways and /or contribute to u nauthorized data collection by vendors or third -parties (e.g., secondary data use) .
Consider: Clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., ﬁne -tuning, drift, decay); Request: Notiﬁcation and disclosure for serious incidents arising from third -party data and system s; Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support. Human -AI Conﬁguration ; Information Security ; Value Chain and Component Integration AI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third -party entities MAP 1.1: Intended purposes, potentially beneﬁcial uses, context speciﬁc laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented.
Considerations include: the speciﬁc set or types of user s along with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics. Action ID Suggested Action GAI Risks MP-1.1-001 When identifying intended purposes, c onsider factors such as internal vs.external use, narrow vs.broad application scope, ﬁne -tuning , and varieties of data sources ( e.g., grounding, retrieval- augmented generation ).
Data Privacy ; Intellectual Property 23 MP-1.1-002 Determine and document the expected and acceptable GAI system context of use in collaboration with socio -cultural and other domain experts, by assessing: Assumptions and limitations; Direct value to the organization; Intended operational environment and observed usage patterns; Potential positive and negative impacts to individuals, public safety, groups, communities, organizations, democratic institutions, and the physical environment; Social norms and expectations. Harmful Bias and Homogenization MP-1.1-003 Document risk measurement plans t o address identiﬁed risks.
Plans may include, as applicable: Individual and group cognitive biases (e.g., conﬁrmation bias, funding bias, groupthink) for AI Actors involved in the design, implementation, and use of GAI systems; Known past GAI system incidents and failure modes; In -context use and foreseeable misuse, abuse, and oﬀ -label use; Over reliance on quantitative metrics and methodologies without suﬃcient awareness of their limitations in the context(s) of use; Standard measurement and structured human f eedback approaches; Anticipated human- AI conﬁgurations. Human -AI Conﬁguration ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content MP-1.1-0 04 Identify and document foreseeable illegal uses or applications of the GAI system that surpass organizational risk tolerances.
CBRN Information or Capabilities ; Dangerous , Violent, or Hateful Content ; Obscene, Degrading, and/or Abusive Content AI Actor Tasks: AI Deployment MAP 1.2: Interdisciplinary AI Actors , competencies, skills, and capacities for establishing context reﬂect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized.Action ID Suggested Action GAI Risks MP-1.2-001 Establish and empower interdisciplinary teams that reﬂect a wide range of capabilities, competencies, demographic groups, domain expertise, educational backgrounds, lived experiences, professions, and skills across the enterprise to inform and conduct risk measurement and management functions.
Human -AI Conﬁguration ; Harmful Bias and Homogenization MP-1.2-002 Verify that data or benchmarks used in risk measurement, and users, participants, or subjects involved in structured GAI public feedback exercises are representative of diverse in -context user populations. Human -AI Conﬁguration ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment 24 MAP 2.1: The speciﬁc tasks and methods used to implement the tasks that the AI system will support are deﬁned (e.g., classiﬁers, generative models, recommenders).Action ID Suggested Action GAI Risks MP-2.1-001 Establish known assumptions and practices for determining data origin and content lineage, for documentation and evaluation purposes.Information Integrity MP-2.1-002 Institute test and evaluation for data and content ﬂows within the GAI system, including but not limited to, original data sources, data transformations, and decision -making criteria.
Intellectual Property ; Data Privacy AI Actor Tasks: TEVV MAP 2.2: Information about the AI system’s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides suﬃcient information to assist relevant AI Actors when making decisions and taking subsequent actions.Action ID Suggested Action GAI Risks MP-2.2-001 Identify and document how the system relies on upstream data sources , including for content provenance , and if it serves as an upstream dependency for other systems.Information Integrity ; Value Chain and Component Integration MP-2.2-0 02 Observe and analyze how the GAI system interacts with external networks, and identify any potential for negative externalities, particularly where content provenance might be compromised.
Information Integrity AI Actor Tasks: End Users MAP 2.3: Scientiﬁc integrity and TEVV considerations are identiﬁed and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and cons truct validati on Action ID Suggested Action GAI Risks MP-2.3-001 Assess the accuracy, quality, reliability, and authenticity of GAI output by comparing it to a set of known ground truth data and by using a variety of evaluation methods (e.g., human oversight and automated evaluation , proven cryptographic techniques , review of content inputs ). Information Integrity 25 MP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data used at diﬀerent stages of AI life cycle.
Harmful Bias and Homogenization ; Intellectual Property MP-2.3-003 Deploy and document fact -checking techniques to verify the accuracy and veracity of information generated by GAI systems, especially when the information comes from multiple (or unknown) sources. Information Integrity MP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human -generated content.Information Integrity MP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.Information Security AI Actor Tasks: AI Development, Domain Experts, TEVV MAP 3.4: Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant technical standards and certiﬁcations – are deﬁned, assessed, and documented.
Action ID Suggested Action GAI Risks MP-3.4-001 Evaluate whether GAI operators and end -users can accurately understand content lineage and origin. Human -AI Conﬁguration ; Information Integrity MP-3.4-002 Adapt existing training programs to include modules on digital content transparency.Information Integrity MP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and interpreting content provenance, relevant to speciﬁc industry and context.Information Integrity MP-3.4-004 Delineate human proﬁciency tests from tests of GAI capabilities.Human -AI Conﬁguration MP-3.4-005 Implement systems to continually monitor and track the outcomes of human- GAI conﬁgurations for future reﬁnement and improvements .Human -AI Conﬁguration ; Information Integrity MP-3.4-006 Involve the end -users, practitioners, and operators in GAI system in prototyping and testing activities.
Make sure these tests cover various scenarios , such as crisis situations or ethically sensitive contexts. Human -AI Conﬁguration ; Information Integrity ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content AI Actor Tasks: AI Design, AI Development, Domain Experts, End -Users, Human Factors, Operation and Monitoring 26 MAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third -party data or software – are in place, followed, and documented, as are risks of infringement of a third -party’s intellectual property or other rights.Action ID Suggested Action GAI Risks MP-4.1-001 Conduct periodic monitor ing of AI-generated content for privacy risks; address any possible instances of PII or sensitive data exposure.Data Privacy MP-4.1-002 Implement processes for respondi ng to potential intellectual property infringement claims or other rights .
Intellectual Property MP-4.1-003 Connect new GAI policies, procedures, and processes to existing model, data, software development, and IT governance and to legal, compliance, and risk management activities . Information Security ; Data Privacy MP-4.1-004 Document training data curation policies, to the extent possible and according to applicable laws and policies .Intellectual Property ; Data Privacy ; Obscene, Degrading, and/or Abusive Content MP-4.1-005 Establish policies for collection, retention, and minimum quality of data, in consideration of the following risks: Disclosure of inappropriate CBRN information ; Use of Illegal or dangerous content; Oﬀensive cyber capabilities; Training data imbalances that could give rise to harmful biases ; Leak of personally identiﬁable information, including facial likenesses of individuals.
CBRN Information or Capabilities ; Intellectual Property ; Information Security ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content ; Data Privacy MP-4.1-0 06 Implement policies and practices deﬁning how third -party intellectual property and training data will be used, stored, and protected. Intellectual Property ; Value Chain and Component Integration MP-4.1-0 07 Re-evaluate models that were ﬁne -tuned or enhanced on top of third -party models.Value Chain and Component Integration MP-4.1-0 08 Re-evaluate risks when adapting GAI models to new domains.Additionally, establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to cont ext of use or mapp ed risks such as security, and safety) may no longer hold.
CBRN Information or Capabilities ; Intellectual Property ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content ; Data Privacy MP-4.1-0 09 Leverage approaches to detect the presence of PII or sensitive data in generated output text, image, video, or audio . Data Privacy 27 MP-4.1-0 10 Conduct appropriate diligence on training data use to assess intellectual property, and privacy, risks, including to examine whether use of proprietary or sensitive training data is consistent with applicable laws.Intellectual Property ; Data Privacy AI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third -party entities MAP 5.1: Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or d eployed the AI system, or other data are identiﬁed and documented.
Action ID Suggested Action GAI Risks MP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic data generation capabilities for potential misuse or vulnerabilities . Information Integrity ; Information Security MP-5.1-002 Identify potential content provenance harms of GAI, such as misinformation or disinformation, deepfakes, including NCII, or tampered content.Enumerate and rank risks based on their likelihood and potential impact, and determine how well provenance solutions address speciﬁc risks and/or harms.Information Integrity ; Dangerous , Violent, or Hateful Content ; Obscene, Degrading, and/or Abusive Content MP-5.1-003 Consider d isclos ing use of GAI to end user s in relevant contexts, while considering the objective of disclosure, the context of use, the likelihood and magnitude of the risk posed, the audience of the disclosure, as well as the frequency of the disclosures.
Human -AI Conﬁguration MP-5.1-004 Prioritize GAI structured public feedback processes based on risk assessment estimates. Information Integrity ; CBRN Information or Capabilities ; Dangerous , Violent, or Hateful Content ; Harmful Bias and Homogenization MP-5.1-005 Conduct adversarial role -playing exercises, GAI red -teaming, or chaos testing to identify anomalous or unforeseen failure modes.Information Security MP-5.1-0 06 Proﬁle threats and negative impacts arising from GAI systems interacting with, manipulating, or generating content, and outlining known and potential vulnerabilities and the likelihood of their occurrence.
Information Security AI Actor Tasks: AI Deployment, AI Design, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, End - Users, Operation and Monitoring 28 MAP 5.2: Practices and personnel for supporting regular engagement with relevant AI Actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented. Action ID Suggested Action GAI Risks MP-5.2-001 Determine context -based measures to identify if new impacts are present due to the GAI system, including regular engagements with downstream AI Actors to identify and quantify new contexts of unanticipated impacts of GAI systems.Human -AI Conﬁguration ; Value Chain and Component Integration MP-5.2-002 Plan regular engagements with AI Actors responsible for inputs to GAI systems, including third- party data and algorithms, to review and evaluate unanticipated impacts.
Human -AI Conﬁguration ; Value Chain and Component Integration AI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End - Users, Human Factors, Operation and Monitoring MEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for implementation starting with the most signiﬁcant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be measured are pro perly documented.Action ID Suggested Action GAI Risks MS-1.1-001 Employ methods to trace the origin and modiﬁcations of digital content .Information Integrity MS-1.1-0 02 Integrate tools designed to analyze content provenance and detect data anomalies, verify the authenticity of digital signatures, and identify patterns associated with misinformation or manipulation.
Information Integrity MS-1.1-003 Disaggregate evaluation metrics by demographic factors to identify any discrepancies in how content provenance mechanisms work across diverse populations. Information Integrity ; Harmful Bias and Homogenization MS-1.1-004 Develop a suite of metrics to evaluate structured public feedback exercises informed by representative AI Actors .Human -AI Conﬁguration ; Harmful Bias and Homogenization ; CBRN Information or Capabilities MS-1.1-0 05 Evaluate novel methods and technologies for the measurement of GAI-related risks in cluding in content provenance , oﬀensive cy ber, and CBRN , while maintaining the models’ ability to produce valid, reliable, and factually accurate outputs.Information Integrity ; CBRN Information or Capabilities ; Obscene, Degrading, and/or Abusive Content 29 MS-1.1-006 Implement continuous monitoring of GAI system impacts to identify whether GAI outputs are equitable across various sub- populations.
Seek active and direct feedback from aﬀected communities via structured feedback mechanisms or red - teaming to monitor and improve outputs. Harmful Bias and Homogenization MS-1.1-007 Evaluate the quality and integrity of data used in training and the provenance of AI-generated content , for example by e mploying techniques like chaos engineering and seeking stakeholder feedback.Information Integrity MS-1.1-008 Deﬁne use cases, contexts of use, capabilities, and negative impacts where structured human feedback exercises, e.g., GAI red- teaming, would be most beneﬁcial for GAI risk measurement and management based on the context of use.Harmful Bias and Homogenization ; CBRN Information or Capabilities MS-1.1-0 09 Track and document risks or opportunities related to all GAI risks that cannot be measured quantitatively, including explanations as to why some risks cannot be measured (e.g., due to technological limitations, resource constraints, or trustworthy considerations).
Include unmeasured risks in marginal risks. Information Integrity AI Actor Tasks: AI Development, Domain Experts, TEVV MEASURE 1.3: Internal experts who did not serve as front -line developers for the system and/or independent assessors are involved in regular assessments and updates.Domain experts, users, AI Actors external to the team that developed or deployed the AI system, and aﬀected communities are consulted in support of assessments as necessary per organizational risk tolerance .Action ID Suggested Action GAI Risks MS-1.3-001 Deﬁne relevant groups of interest (e.g., demographic groups, subject matter experts, experience with GAI technology) within the context of use as part of plans for gathering structured public feedback.
Human -AI Conﬁguration ; Harmful Bias and Homogenization ; CBRN Information or Capabilities MS-1.3-002 Engage in internal and external evaluations , GAI red -teaming, impact assessments, or other structured human feedback exercises in consultation with representative AI Actors with expertise and familiarity in the context of use, and/or who are representative of the populations associated with the context of use. Human -AI Conﬁguration ; Harmful Bias and Homogenization ; CBRN Information or Capabilities MS-1.3-0 03 Verify those conducting structured human feedback exercises are not directly involved in system development tasks for the same GAI model.
Human -AI Conﬁguration ; Data Privacy AI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Operation and Monitoring, TEVV 30 MEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population. Action ID Suggested Action GAI Risks MS-2.2-001 Assess and manage statistical biases related to GAI content provenance through techniques such as re -sampling, re -weighting, or adversarial training.Information Integrity ; Information Security ; Harmful Bias and Homogenization MS-2.2-002 Document how content provenance data is tracked and how that data interact s with privacy and security .Consider : Anonymiz ing data to protect the privacy of human subjects; Leverag ing privacy output ﬁlters; Remov ing any personally identiﬁable information (PII) to prevent potential harm or misuse.
Data Privacy ; Human AI Conﬁguration; Information Integrity ; Information Security ; Dangerous , Violent, or Hateful Content MS-2.2-0 03 Provide human subjects with options to withdraw participation or revoke their consent for present or future use of their data in GAI applications . Data Privacy ; Human -AI Conﬁguration; Information Integrity MS-2.2-0 04 Use techniques such as anonymization , diﬀerential privacy or other privacy - enhancing technologies to minimize the risks associated with linking AI -generated content back to individual human subjects.Data Privacy ; Human -AI Conﬁguration AI Actor Tasks: AI Development, Human Factors, TEVV MEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s).Measures are documented.
Action ID Suggested Action GAI Risks MS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a model for ﬁne tuning or enhancement with retrieval- augmented generation. Information Security ; Confabulation MS-2.3-002 Evaluate claims of model capabilities using empirically validated methods.Confabulation ; Information Security MS-2.3-003 Share results of pre -deployment testing with relevant GAI Actors , such as those with system release approval authority.Human -AI Conﬁguration 31 MS-2.3-004 Utilize a purpose -built testing environment such as NIST Dioptra to empirically evaluate GAI trustworthy characteristics.CBRN Information or Capabilities ; Data Privacy ; Confabulation ; Information Integrity ; Information Security ; Dangerous , Violent, or Hateful Content ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, TEVV MEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable.
Limitations of the generalizability beyond the conditions under which the technology was developed are documented. Action ID Suggested Action Risks MS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non - systematic, and anecdotal assessments.Human -AI Conﬁguration ; Confabulation MS-2.5-002 Document the extent to which human domain knowledge is employed to improve GAI system performance, via, e.g., RLHF, ﬁne -tuning, retrieval- augmented generation, content moderation, business rules.Human -AI Conﬁguration MS-2.5-003 Review and verify sources and citations in GAI system outputs during pre - deployment risk measurement and ongoing monitoring activities.Confabulation MS-2.5-004 Track and document instances of anthropomorphization (e.g., human images, mentions of human feelings, cyborg imagery or motifs) in GAI system interfaces.
Human -AI Conﬁguration MS-2.5-0 05 Verify GAI system training data and TEVV data provenance, and that ﬁne -tuning or retrieval- augmented generation data is grounded. Information Integrity MS-2.5-0 06 Regularly review security and safety guardrails, especially if the GAI system is being operated in novel circumstances.This includes reviewing reasons why the GAI system was initially assessed as being safe to deploy.Information Security ; Dangerous , Violent, or Hateful Content AI Actor Tasks: Domain Experts, TEVV 32 MEASURE 2.6: The AI system is evaluated regularly for safety risks – as identiﬁed in the MAP function.The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, p articularly if made to operate beyond its knowledge limits.Safety metrics reﬂect system reliability and robustness, real- time monitoring, and response times for AI system failures.
Action ID Suggested Action GAI Risks MS-2.6-001 Assess adverse impacts , including health and wellbeing impacts for value chain or other AI Actors that are exposed to sexually explicit, oﬀensive , or violent information during GAI training and maintenance. Human -AI Conﬁguration ; Obscene, Degrading, and/or Abusive Content ; Value Chain and Component Integration; Dangerous , Violent, or Hateful Content MS-2.6-002 Assess existence or levels of harmful bias , intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data.Data Privacy ; Intellectual Property ; Obscene, Degrading, and/or Abusive Content ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content ; CBRN Information or Capabilities MS-2.6-003 Re-evaluate safety features of ﬁne -tuned models when the negative risk exceeds organizational risk tolerance.
Dangerous , Violent, or Hateful Content MS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to assess risks that may arise from unreliable downstream decision -making. Value Chain and Component Integration ; Dangerous , Violent, or Hateful Content MS-2.6-005 Verify that GAI system architecture can monitor outputs and performance, and handle, recover from, and repair errors when security anomalies, threats and impacts are detected.Confabulation ; Information Integrity ; Information Security MS-2.6-0 06 Verify that systems properly handle queries that may give rise to inappropriate, malicious, or illegal usage, including facilitating manipulation, extortion, targeted impersonation, cyber -attacks , and weapons creation.CBRN Information or Capabilities ; Information Security MS-2.6-007 Regularly evaluate GAI system vulnerabilities to possible circumventi on of safety measures .
CBRN Information or Capabilities ; Information Security AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV 33 MEASURE 2.7: AI system security and resilience – as identiﬁed in the MAP function – are evaluated and documented. Action ID Suggested Action GAI Risks MS-2.7-001 Apply established security measures to: Assess likelihood and magnit ude of vulnerabilities and threat s such as backdoors, compromised dependencies, data breaches, eavesdropping, man- in-the-middle attacks, reverse engineering , autonomous agents, model theft or exposure of model weights, AI inference, bypass, extraction, and other baseline security concerns .Data Privacy ; Information Integrity ; Information Security ; Value Chain and Component Integration MS-2.7-002 Benchmark GAI system security and resilience related to content provenance against industry standards and best practices.
Compare GAI system security features and content provenance methods against industry state -of-the-art. Information Integrity ; Information Security MS-2.7-003 Conduct user surveys to gather user satisfaction with the AI -generated content and user perceptions of content authenticity.Analyze user feedback to identify concerns and/or current literacy levels related to content provenance and understanding of labels on content .Human -AI Conﬁguration ; Information Integrity MS-2.7-004 Identify metrics that reﬂect the eﬀectiveness of security measures, such as data provenance, the number of unauthorized access attempts, inference, bypass, extraction, penetrations, or provenance veriﬁcation.
Information Integrity ; Information Security MS-2.7-005 Measure reliability of content authentication methods, such as watermarking, cryptographic signatures, digital ﬁngerprints, as well as access controls, conformity assessment, and model integrity veriﬁcation , which can help support the eﬀective implementation of content provenance techniques. Evaluate the rate of false positives and false negatives in content provenance, as well as true positives and true negatives for veriﬁcation.Information Integrity MS-2.7-0 06 Measure the rate at which recommendations from security checks and incidents are implemented.Assess how quickly the AI system can adapt and improve based on lessons learned from security incidents and feedback .
Information Integrity ; Information Security MS-2.7-0 07 Perform AI red -teaming to assess resilience against: Abuse to facilitate attacks on other systems (e.g., malicious code generation, enhanced phishing content), GAI attacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, data poisoning, membership inference, model extraction, sponge examples). Information Security ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content MS-2.7-0 08 Verify ﬁne -tuning does not compromise safety and security controls.Information Integrity ; Information Security ; Dangerous , Violent, or Hateful Content 34 MS-2.7-0 09 Regularly assess and verify that security measures remain eﬀective and have not been compromised.
Information Security AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV MEASURE 2.8: Risks associated with transparency and accountability – as identiﬁed in the MAP function – are examined and documented. Action ID Suggested Action GAI Risks MS-2.8-001 Compile statistics on actual policy violations, take -down requests, and intellectual property infringement for organizational GAI systems: Analyze transparency reports across demographic groups, languages groups .Intellectual Property ; Harmful Bias and Homogenization MS-2.8-002 Document the instructions given to data annotators or AI red -teamers.Human -AI Conﬁguration MS-2.8-0 03 Use digital content transparency solutions to enable the documentation of each instance where content is generated, modiﬁed, or shared to provide a tamper - proof history of the content, promote transparency, and enable traceability.
Robust version control systems can also be applied to track chang es across the AI lifecycle over time. Information Integrity MS-2.8-004 Verify adequacy of GAI system user instructions through user testing.Human -AI Conﬁguration AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV 35 MEASURE 2.9: The AI model is explained, validated, and documented, and AI system output is interpreted within its context – as identiﬁed in the MAP function – to inform responsible use and governance.Action ID Suggested Action GAI Risks MS-2.9-001 Apply and document ML explanation results such as: Analysis of embeddings, Counterfactual prompts, Gradient -based attributions, Model compression/surrogate models, Occlusion/term reduction.
Confabulation MS-2.9-002 Document GAI model details including: Proposed use and organizational value; Assumptions and limitations, Data collection methodologies; Data provenance; Data quality; Model architecture (e.g., convolutional neural network, transformers, etc.); Optimizatio n objectives; Training algorithms; RLHF approaches; Fine -tuning or retrieval- augmented generation approaches; Evaluation data; Ethical considerations; Legal and regulatory requirements. Information Integrity ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End -Users, Operation and Monitoring, TEVV MEASURE 2.10: Privacy risk of the AI system – as identiﬁed in the MAP function – is examined and documented.
Action ID Suggested Action GAI Risks MS-2.10- 001 Conduct AI red -teaming to assess issues such as: Outputting of training data samples, and subsequent reverse engineering, model extraction, and membership inference risks; Revealing biometric, conﬁdential, copyrighted, licensed, patented, personal, proprietary, sensitive, or trade -marked information ; Tracking or revealing location information of users or members of training datasets. Human -AI Conﬁguration ; Information Integrity; Intellectual Property MS-2.10- 002 Engage directly with end -users and other stakeholders to understand their expectations and concerns regarding content provenance.Use this feedback to guide the design of provenance data -tracking techniques .Human -AI Conﬁguration ; Information Integrity MS-2.10-0 03 Verify deduplication of GAI training data samples , particularly regarding synthetic data .
Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End -Users, Operation and Monitoring, TEVV 36 MEASURE 2.11: Fairness and bias – as identiﬁed in the MAP function – are evaluated and results are documented. Action ID Suggested Action GAI Risks MS-2.11- 001 Apply use -case appropriate benchmarks (e.g., Bias Benchmark Questions, Real Hateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, stereotyping, denigration, and hateful content in GAI system outputs; Document assumptions and limitations of benchmarks, including any actual or possible training/test data cross contamination, relative to in -context deployment environment.Harmful Bias and Homogenization MS-2.11- 002 Conduct fairness assessments to measure systemic bias.Measure GAI system performance across demographic groups and subgroups, addressing both quality of service and any allocation of services and resources.
Quantify harms using: ﬁeld testing with sub- group populations to determine likelihood of exposure to generated content exhibiting harmful bias, AI red- teaming with counterfactual and low -context (e.g., “leader,” “bad guys”) prompts. For ML pipelines or business processes with categorical or numeric out comes that rely on GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, equal opportunity, statistical hypothesis tests), to the pipeline or business outcome where appropriate; Custom, context -speciﬁc metrics developed in collabo ration with domain experts and aﬀected communities; Measurements of the prevalence of denigration in generated content in deployment (e.g., sub-sampling a fraction of traﬃc and manually annotating denigrating content) .
Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content MS-2.11- 003 Identify the classes of individuals, groups, or environmental ecosystems which might be impacted by GAI systems through direct engagement with potentially impacted communities.
Environmental ; Harmful Bias and Homogenization MS-2.11- 004 Review, document, and measure sources of bias in GAI training and TEVV data: Diﬀerences in distributions of outcomes across and within groups, including intersecting groups; Completeness, representativeness, and balance of data sources; demographic group and subgroup coverage in GAI system training data; Fo rms of latent systemic bias in images, text, audio, embeddings, or other complex or unstructured data; Input data features that may serve as proxies for demographic group membership (i.e., image metadata, language dialect) or otherwise give rise to emergen t bias within GAI systems; The extent to which the digital divide may negatively impact representativeness in GAI system training and TEVV data; Filtering of hate speech or content in GAI system training data; Prevalence of GAI -generated data in GAI system training data.
Harmful Bias and Homogenization 15 Winogender Schemas is a sample set of paired sentences which diﬀer only by gender of the pronouns used, which can be used to evaluate gender bias in natural language processing coreference resolution systems. 37 MS-2.11-0 05 Assess the proportion of synthetic to non -synthetic training data and verify training data is not overly homogenous or GAI-produced to mitigate concerns of model collapse.Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End -Users, Operation and Monitoring, TEVV MEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identiﬁed in the MAP function – are assessed and documented.Action ID Suggested Action GAI Risks MS-2.12- 001 Assess safety to physical environments when deploying GAI systems.
Dangerous , Violent, or Hateful Content MS-2.12- 002 Document anticipated environmental impacts of model development, maintenance, and deployment in product design decisions. Environmental MS-2.12- 003 Measure or estimate environmental impacts (e.g., energy and water consumption) for training, ﬁne tuning, and deploying models: Verify tradeoﬀs between resources used at inference time versus additional resources required at training time.Environmental MS-2.12- 004 Verify eﬀectiveness of carbon capture or oﬀset programs for GAI training and applications , and address green -washing concerns .Environmental AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV 38 MEASURE 2.13: Eﬀectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.
Action ID Suggested Action GAI Risks MS-2.13- 001 Create measurement error models for pre -deployment metrics to demonstrate construct validity for each metric (i.e., does the metric eﬀectively operationalize the desired concept): Measure or estimate, and document, biases or statistical variance in applie d metrics or structured human feedback processes; Leverage domain expertise when modeling complex societal constructs such as hateful content . Confabulation ; Information Integrity ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, Operation a nd Monitoring, TEVV MEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are diﬃcult to assess using currently available measurement techniques or where metrics are not yet available.Action ID Suggested Action GAI Risks MS-3.2-001 Establish processes for identifying emergent GAI system risks including consulting with external AI Actors .
Human -AI Conﬁguration ; Confabulation AI Actor Tasks: AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV MEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics. Action ID Suggested Action GAI Risks MS-3.3-001 Conduct impact assessments on how AI -generated content might aﬀect diﬀerent social, economic, and cultural groups.Harmful Bias and Homogenization MS-3.3-002 Conduct studies to understand how end users perceive and interact with GAI content and accompanying content provenance within context of use.Assess whether the content aligns with their expectations and how they may act upon the information presented.
Human -AI Conﬁguration ; Information Integrity MS-3.3-003 Evaluate potential biases and stereotypes that could emerge from the AI - generated content using appropriate methodologies including computational testing methods as well as evaluating structured feedback input. Harmful Bias and Homogenization 39 MS-3.3-004 Provide input for training materials about the capabilities and limitations of GAI systems related to digital content transparency for AI Actors , other professionals, and the public about the societal impacts of AI and the role of diverse and inclusive content generation.Human -AI Conﬁguration ; Information Integrity ; Harmful Bias and Homogenization MS-3.3-005 Record and integrate structured feedback about content provenance from operators, users, and potentially impacted communities through the use of methods such as user research studies, focus groups, or community forums.Actively seek feedback on generated c ontent quality and potential biases.
Assess the general awareness among end users and impacted communities about the availability of these feedback channels. Human -AI Conﬁguration ; Information Integrity ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End -Users, Operation and Monitoring, TEVV MEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as intended.Results are documented.Action ID Suggested Action GAI Risks MS-4.2-001 Conduct adversarial testing at a regular cadence to map and measure GAI risks , including tests to address attempts to deceive or manipulate the application of provenance techniques or other misuses .Identify vulnerabilities and understand potential misuse scenarios and unintended outputs.
Information Integrity ; Information Security MS-4.2-002 Evaluate GAI system performance in real -world scenarios to observe its behavior in practical environments and reveal issues that might not surface in controlled and optimized testing environments. Human -AI Conﬁguration ; Confabulation ; Information Security MS-4.2-003 Implement interpretability and explainability methods to evaluate GAI system decisions and verify alignment with intended purpose.Information Integrity ; Harmful Bias and Homogenization MS-4.2-004 Monitor and document instances where human operators or other systems override the GAI's decisions.Evaluate these cases to understand if the overrides are linked to issues related to content provenance.Information Integrity MS-4.2-0 05 Verify and document the incorporation of results of structured public feedback exercises into design, implementation, deployment approval (“go”/“no -go” decisions), monitoring, and decommission decisions.
Human -AI Conﬁguration ; Information Security AI Actor Tasks: AI Deployment, Domain Experts, End -Users, Operation and Monitoring, TEVV 40 MANAGE 1.3: Responses to the AI risks deemed high priority, as identiﬁed by the MAP function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting.Action ID Suggested Action GAI Risks MG-1.3-001 Document trade -oﬀs, decision processes, and relevant measurement and feedback results for risks that do not surpass organizational risk tolerance , for example, in the context of model release : Consider diﬀerent approaches for model release, for example , leveraging a staged release approach.Consider release approaches in the context of the model and its projected use cases.Mitigate, transfer, or avoid risks that surpass organizational risk tolerances.
Information Security MG-1.3-002 Monitor the robust ness and eﬀectiveness of risk controls and mitigation plans (e.g., via red-teaming, ﬁeld testing, participatory engagements, performance assessments, user feedback mechanisms). Human -AI Conﬁguration AI Actor Tasks: AI Develop ment, AI Deployment, AI Impact Assessment, Operation and Monitoring MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems.Action ID Suggested Action GAI Risks MG-2.2-001 Compare GAI system outputs against pre-deﬁned organization risk tolerance, guidelines , and principles , and review and test AI-generated content against these guidelines.CBRN Information or Capabilities ; Obscene, Degrading, and/or Abusive Content ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content MG-2.2-002 Document training data sources to trace the origin and provenance of AI - generated content.
Information Integrity MG-2.2-003 Evaluate feedback loops between GAI system content provenance and human reviewers, and update where needed. Implement real -time monitoring systems to aﬃrm that cont ent provenance protocols remain eﬀective.Information Integrity MG-2.2-004 Evaluate GAI content and data for representational biases and employ techniques such as re -sampling, re -ranking, or adversarial training to mitigate biases in the generated content.Information Security ; Harmful Bias and Homogenization MG-2.2-005 Engage in due diligence to analyze GAI output for harmful content, potential misinformation , and CBRN -related or NCII content .CBRN Information or Capabilities ; Obscene, Degrading, and/or Abusive Content ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content 41 MG-2.2-0 06 Use feedback from internal and external AI Actors , users, individuals, and communities, to assess impact of AI -generated content.
Human -AI Conﬁguration MG-2.2-0 07 Use real -time auditing tools where they can be demonstrated to aid in the tracking and validation of the lineage and authenticity of AI -generated data. Information Integrity MG-2.2-0 08 Use structured feedback mechanisms to solicit and capture user input about AI - generated content to detect subtle shifts in quality or alignment with community and societal values.Human -AI Conﬁguration ; Harmful Bias and Homogenization MG-2.2-009 Consider opportunities to responsibly use synthetic data and other privacy enhancing techniques in GAI development, where appropriate and applicable , match the statistical properties of real- world data without disclosing personally identiﬁable information or contributing to homogenization .
Data Privacy ; Intellectual Property; Information Integrity ; Confabulation ; Harmful Bias and Homogenization AI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring MANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identiﬁed. Action ID Suggested Action GAI Risks MG-2.3-001 Develop and update GAI system incident response and recovery plans and procedures to address the following: Review and maintenance of policies and procedures to account for newly encountered uses; Review and maintenance of policies and procedures for detec tion of unanticipated uses; Verify response and recovery plans account for the GAI system value chain; Verify response and recovery plans are updated for and include necessary details to communicate with downstream GAI system Actors: Points -of-Contact (POC), Contact information, notiﬁcation format.
Value Chain and Component Integration AI Actor Tasks: AI Deployment, Operation and Monitoring MANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. Action ID Suggested Action GAI Risks MG-2.4-001 Establish and maintain communication plans to inform AI stakeholders as part of the deactivation or disengagement process of a speciﬁc GAI system (including for open -source models) or context of use, including r easons, workarounds, user access removal, alternative processes, contact information, etc.Human -AI Conﬁguration 42 MG-2.4-002 Establish and maintain procedures for escalating GAI system incidents to the organizational risk management authority when speciﬁc criteria for deactivation or disengagement is met for a particular context of use or for the GAI system as a whole.
Information Security MG-2.4-003 Establish and maintain procedures for the remediation of issues which trigger incident response processes for the use of a GAI system, and provide stakeholders timelines associated with the remediation plan. Information Security MG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of GAI systems in accordance with set risk tolerances and appetites.Information Security AI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring MANAGE 3.1: AI risks and beneﬁts from third -party resources are regularly monitored, and risk controls are applied and documented.
Action ID Suggested Action GAI Risks MG-3.1-001 Apply organizational risk tolerances and controls (e.g., acquisition and procurement processes; assessing personnel credentials and qualiﬁcations, performing background checks; ﬁltering GAI input and outputs, grounding, ﬁne tuning , retrieval- augmented generation) to third -party GAI resources: Apply organizational risk tolerance to the utilization of third -party datasets and other GAI resources; Apply organizational risk tolerances to ﬁne -tuned third -party models; Apply organizational risk tolerance to existing t hird-party models adapted to a new domain; Reassess risk measure ments after ﬁne -tuning third- party GAI models. Value Chain and Component Integration ; Intellectual Property MG-3.1-002 Test GAI system value chain risks (e.g., data poisoning, malware, other software and hardware vulnerabilities; labor practices; data privacy and localization compliance; geopolitical alignment).
Data Privacy ; Information Security ; Value Chain and Component Integration ; Harmful Bias and Homogenization MG-3.1-003 Re-assess model risks after ﬁne -tuning or retrieval -augmented generation implementation and for any third -party GAI models deployed for applications and/or use cases that were not evaluated in initial testing. Value Chain and Component Integration MG-3.1-004 Take reasonable measures to review training data for CBRN information, and intellectual property, and where appropriate, remove it.Implement reasonable measures to prevent, ﬂag, or take other action in response to outputs that reproduce particular training data (e.g., plagiarized, trademarked, patented, licensed content or trade secret material).Intellectual Property ; CBRN Information or Capabilities 43 MG-3.1-005 Review various transparency artifacts (e.g., system cards and model cards) for third -party models.
Information Integrity ; Information Security ; Value Chain and Component Integration AI Actor Tasks: AI Deployment, Operation and Monitoring, Third -party entities MANAGE 3.2: Pre-trained models which are used for development are monitored as part of AI system regular monitoring and maintenance. Action ID Suggested Action GAI Risks MG-3.2-001 Apply explainable AI (XAI) techniques (e.g., analysis of embeddings, model compression/distillation, gradient -based attributions, occlusion/term reduction, counterfactual prompts, word clouds) as part of ongoing continuous improvement processes to mitigate risks related to unexplainable GAI systems.Harmful Bias and Homogenization MG-3.2-002 Document how pre -trained models have been adapted ( e.g., ﬁne-tuned , or retrieval- augmented generation) for the speciﬁc generative task, including any data augmentations, parameter adjustments, or other modiﬁcations.
Access to un-tuned (baseline) models support s debugging the relative inﬂuence of the pre - trained weights compared to the ﬁne -tuned model weights or other system updates . Information Integrity ; Data Privacy MG-3.2-003 Document sources and types of training data and their origins, potential biases present in the data related to the GAI application and its content provenance, architecture, training process of the pre -trained model including information on hyperparameters, training duration, and any ﬁne -tuning or retrieval- augmented generation processes applied.Information Integrity ; Harmful Bias and Homogenization ; Intellectual Property MG-3.2-004 Evaluate user reported problematic content and integrate feedback into system updates.
Human -AI Conﬁguration , Dangerous , Violent, or Hateful Content MG-3.2-005 Implement content ﬁlters to prevent the generation of inappropriate, harmful, false, illegal, or violent content related to the GAI application, including for CSAM and NCII. These ﬁlters can be rule -based or leverage additional machine learning models to ﬂag problematic inputs and outputs.Information Integrity ; Harmful Bias and Homogenization ; Dangerous , Violent, or Hateful Content ; Obscene, Degrading, and/or Abusive Content MG-3.2-006 Implement real -time monitoring processes for analyzing generated content performance and trustworthiness characteristics related to content provenance to identify deviations from the desired standards and trigger alerts for human intervention.Information Integrity 44 MG-3.2-007 Leverage feedback and recommendations from organizational boards or committees related to the deployment of GAI applications and content provenance when using third -party pre -trained models.
Information Integrity ; Value Chain and Component Integration MG-3.2-0 08 Use human moderation systems where appropriate to review generated content in accordance with human- AI conﬁguration policies established in the Govern function, aligned with socio -cultural norms in the context of use, and for settings where AI models are demonstrated to perform poorly. Human -AI Conﬁguration MG-3.2-0 09 Use organizational risk tolerance to evaluate acceptable risks and performance metrics and decommission or retrain pre -trained models that perform outside of deﬁned limits.CBRN Information or Capabilities ; Confabulation AI Actor Tasks: AI Deployment, Operation and Monitoring, Third -party entities MANAGE 4.1: Post -deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI Actors , appeal and override, decommissioning, incident response, recovery, and change management.
Action ID Suggested Action GAI Risks MG-4.1-001 Collaborate with external researchers, industry experts, and community representatives to maintain awareness of emerging best practices and technologies in measuring and managing identiﬁed risks. Information Integrity ; Harmful Bias and Homogenization MG-4.1-002 Establish, maintain, and evaluate eﬀectiveness of organizational processes and procedures for post -deployment monitoring of GAI systems, particularly for potential confabulation, CBRN, or cyber risks.CBRN Information or Capabilities ; Confabulation ; Information Security MG-4.1-003 Evaluate the use of sentiment analysis to gauge user sentiment regarding GAI content performance and impact, and work in collaboration with AI Actors experienced in user research and experience.Human -AI Conﬁguration MG-4.1-004 Implement active learning techniques to identify instances where the model fails or produces unexpected outputs.
Confabulation MG-4.1-0 05 Share transparency reports with internal and external stakeholders that detail steps taken to update the GAI system to enhance transparency and accountability. Human -AI Conﬁguration ; Harmful Bias and Homogenization MG-4.1-0 06 Track dataset modiﬁcations for provenance by monitoring data deletions, rectiﬁcation requests, and other changes that may impact the veriﬁability of content origins.Information Integrity 45 MG-4.1-0 07 Verify that AI Actors responsible for monitoring reported issues can eﬀectively evaluate GAI system performance including the application of content provenance data tracking techniques, and promptly escalate issues for response.
Human -AI Conﬁguration ; Information Integrity AI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End -Users, Human Factors, Operation and Monitoring MANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI Actors . Action ID Suggested Action GAI Risks MG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the performance, feedback received, and improvements made.Harmful Bias and Homogenization MG-4.2-002 Practice and follow incident response plans for addressing the generation of inappropriate or harmful content and adapt processes based on ﬁndings to prevent future occurrences.Conduct post -mortem analyses of incidents with relevant AI Actors , to understand the root causes and implement preventive measures.
Human -AI Conﬁguration ; Dangerous , Violent, or Hateful Content MG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease non-technical stakeholders understanding of GAI system functionality. Human -AI Conﬁguration AI Actor Tasks: AI Deployment, AI Design, AI Development, Aﬀected Individuals and Communities, End -Users, Operation and Monitoring, TEVV MANAGE 4.3: Incidents and errors are communicated to relevant AI Actors , including aﬀected communities.Processes for tracking, responding to, and recovering from incidents and errors are followed and documented.Action ID Suggested Action GAI Risks MG-4.3-001 Conduct after -action assessments for GAI system incidents to verify incident response and recovery processes are followed and eﬀective, including to follow procedures for communicating incidents to relevant AI Actors and where applicable, relevant legal and regulatory bodies.
Information Security MG-4.3-002 Establish and maintain policies and procedures to record and track GAI system reported errors, near -misses, and negative impacts. Confabulation ; Information Integrity 46 MG-4.3-003 Report GAI incidents in compliance with legal and regulatory requirements (e.g., HIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle crash reporting requirements.Information Security ; Data Privacy AI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End -Users, Human Factors, Operation and Monitorin g 47 Appendix A.Primary GAI Considerations The following primary considerations were derived as overarching themes from the GAI PWG consultation process.These considerations (Governance, Pre- Deployment Testing, Content Provenance, and Incident Disclosure) are relevant for volun tary use by any organization designing, developing, and using GAI and also inform the Actions to Manage GAI risks.
Information included about the primary considerations is not exhaustive , but highlights the most relevant topics derived from the GAI PWG. Acknowledgments: These considerations could not have been surfaced without the helpful analysis and contributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, Mat Heyman, Yoo young Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee.A.1.Governance A.1.1.Overview Like any other technology system, governance principles and techniques can be used to manage risks related to generative AI models, capabilities, and applications.Organizations may choose to apply their existing risk tiering to GAI systems, or they may op t to revis e or update AI system risk levels to address these unique GAI risks.This section describes how organizational governance regimes may be re - evaluated and adjusted for GAI contexts.It also addresses third -party considerations for governing across the AI value chain.A.1.2.
Organizational Governance GAI opportunities, risks and long- term performance characteristics are typically less well -understood than non- generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. Accordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human- AI conﬁgurations in order to manage their risks eﬀectively.Organizations’ use of GAI systems may also warrant additional human review, tracking and documentation, and greater management oversight.AI technology can produce varied outputs in multiple modalities and present many classes of user interfaces.This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering applications and contexts of use.These can include data labeling and preparation, development of GAI models, content moderation, code generation and review, text generation and editing, image and video generation, summarization, search, and chat.
These activities can take place within organizational settings or in the public domain. Organizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict with their tolerances or values.Governance tools and protocols that are applied to other types of AI systems can be applied to GAI systems.
These p lans and actions include: • Accessibility and reasonable accommodations • AI actor credentials and qualiﬁcations • Alignment to organizational values • Auditing and assessment • Change -management controls • Commercial use • Data provenance 48 • Data protection • Data retention • Consistency in use of deﬁning key terms • Decommissioning • Discouraging anonymous use • Education • Impact assessments • Incident response • Monitoring • Opt-outs • Risk-based controls • Risk mapping and measurement • Science -backed TEVV practices • Secure software development practices • Stakeholder engagement • Synthetic content detection and labeling tools and techniques • Whistleblower protections • Workforce diversity and interdisciplinary teams Establishing acceptable use policies and guidance for the use of GAI in formal human- AI teaming settings as well as diﬀerent levels of human -AI conﬁgurations can help to decrease risks arising from misuse, abuse, inappropriate repurpose, and misalignment between systems and users.
These practices are just one example of adapting existing governance protocols for GAI contexts. A.1.3.Third -Party Considerations Organizations may seek to acquire, embed, incorporate, or use open- source or proprietary third- party GAI models, systems, or generated data for various applications across an enterprise.Use of these GAI tools and inputs has implications for all functions of the organization – including but not limited to acquisition, human resources, legal, compliance, and IT services – regardl ess of whether they are carried out by employees or third parties.Many of the actions cited above are relevant and options for addressing third- party considerations.Third party GAI integrations may give rise to increased intellectual property, data privacy, or information security risks, pointing to the need for clear guidelines for transparency and risk management regarding the collection and use of third -party data for model inputs.
Organizations may consider varying risk controls for foundation models, ﬁne -tuned models, and embedded tools, enhanced processes for interacting with external GAI technologies or service providers. Organizations can apply standard or existing risk controls and processes to proprietary or open -source GAI technologies, data, and third -party service providers, including acquisition and procurement due diligence, requests for software bills of materials (SBOMs), application of service level agreements (SLAs), and statement on standards for attestation en gagement (SSAE) reports to help with third -party transparency and risk management for GAI systems.A.1.4.Pre-Deployment Testing Overview The diverse ways and contexts in which GAI systems may be developed, used, and repurposed complicates risk mapping and pre -deployment measurement eﬀorts.
Robust test, evaluation, validation, and veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI lifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF ). Until new and rigorous 49 early lifecycle TEVV approaches are developed and matured for GAI, organizations may use recommended “pre- deployment testing” practices to measure performance, capabilities, limits, risks, and impacts.This section describes risk measurement and estimation as part of pre -deployment TEVV, and examines the state of play for pre -deployment testing methodologies.Limitations of Current Pre -deployment Test Approaches Currently available pre -deployment TEVV processes used for GAI applications may be inadequate, non- systematically applied, or fail to reﬂect or mismatched to deployment contexts.
For example, the anecdotal testing of GAI system capabilities through video games or standardized tests designed for humans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or reliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically asse ss validity or reliability risks.Measurement gaps can arise from mismatches between laboratory and real -world settings.Current testing approaches often remain focused on laboratory conditions or restricted to benchmark test datasets and in silico techniques that may not extrapolate well to —or directly assess GAI impacts in real - world conditions.For example, current measurement gaps for GAI make it diﬃcult to precisely estimate its potential ecosystem -level or longitudinal risks and related political, social, and economic impacts.
Gaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt sensitivity and broad heterogeneity of contexts of use. A.1.5.Structured Public Feedback Structured public feedback can be used to evaluate whether GAI systems are performing as intended and to calibrate and verify traditional measurement methods.Examples of structured feedback include, but are not limited to: • Participatory Engagement Methods : Methods used to solicit feedback from civil society groups, aﬀected communities, and users, including focus groups, small user studies, and surveys.• Field Testing : Methods used to determine how people interact with, consume, use, and make sense of AI -generated information, and subsequent actions and eﬀects, including UX, usability, and other structured, randomized experiments.
• AI Red -teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers. Information gathered from structured public feedback can inform design, implementation, deployment approval , maintenance, or decommissioning decisions.Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices.When implementing feedback activities, organizations should follow human subjects research requirements and best practices such as informed consent and subject compensation.50 Participatory Engagement Methods On an ad hoc or more structured basis, organizations can design and use a variety of channels to engage external stakeholders in product development or review.
Focus groups with select experts can provide feedback on a range of issues. Small user studies c an provide feedback from representative groups or populations.Anonymous surveys can be used to poll or gauge reactions to speciﬁc features.Participatory engagement methods are often less structured than ﬁeld testing or red teaming, and are more commonl y used in early stages of AI or product development.Field Testing Field testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed.Field style tests can be adapted from a focus on user preferences and experiences towards AI risks and impacts – both negative and positive.When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.
Organizations may also collect feedback on outcomes, harms, and user experience directly from users in the production environment after a model has been released, in accordance with human subject standards such as informed consent and compensation. Organiz ations should follow applicable human subjects research requirements, and best practices such as informed consent and subject compensation, when implementing feedback activities.AI Red -teaming AI red -teaming is an evolving practice that references exercises often conducted in a controlled environment and in collaboration with AI developers building AI models to identify potential adverse behavior or outcomes of a GAI model or system, how they could occur, an d stress test safeguards” .AI red-teaming can be performed before or after AI models or systems are made available to the broader public; this section focuses on red- teaming in pre -deployment contexts.
The quality of AI red- teaming outputs is related to the background and expertise of the AI red team itself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the varying contexts where GAI will be used.For best results, AI red teams should demonstrate domain expertise, and awareness of socio -cultural aspects within the deployment context.AI red -teaming results should be given additional analysis before they are incorporated into organizational governance and decision making, policy and procedural updates, and AI risk management eﬀorts.Various types of AI red -teaming may be appropriate, depending on the use case: • General Public: Performed by general users (not necessarily AI or technical experts) who are expected to use the model or interact with its outputs, and who bring their own lived experiences and perspectives to the task of AI red -teaming .
These individuals may have been provided instructions and material to complete tasks which may elicit harmful model behaviors. This type of exercise can be more eﬀective with large groups of AI red-teamers.• Expert: Performed by specialists with expertise in the domain or speciﬁc AI red -teaming context of use (e.g., medicine, biotech, cybersecurity).• Combination: In scenarios when it is diﬃcult to identify and recruit specialists with suﬃcient domain and contextual expertise, AI red -teaming exercises may leverage both expert and 51 general public participants.For example, expert AI red- teamers could modify or verify the prompts written by general public AI red- teamers.These approaches may also expand coverage of the AI risk attack surface.• Human / AI: Performed by GAI in combinatio n with specialist or non -specialist human teams.GAI- led red -teaming can be more cost eﬀective than human red- teamers alone.
Human or GAI- led AI red -teaming may be better suited for eliciting diﬀerent types of harms. A.1.6.Content Provenance Overview GAI technologies can be leveraged for many applications such as content generation and synthetic data.Some aspects of GAI output s, such as the production of deepfake content, can challenge our ability to distinguish human- generated content from AI -generated synthetic content.To help manage and mitigate these risks, digital transparency mechanisms like provenance data tracking can trace the origin and history of content.Provenance data tracking and synthetic content detection can help facilitate greater information access about both authentic and synthetic content to users, enabling better knowledge of trustworthiness in AI systems.
When combined with other organizational accountability mechanisms, digital content transparency approaches can enable processes to trace negative outcomes back to their source, improve information integrity, and uphold public trust. Provenance data tracking and synthetic content detection mechanisms provide information about the origin and history of content to assist in GAI risk management eﬀorts.Provenance metad ata can include information about GAI model developers or creators of GAI content , date/time of creation, location, modiﬁcations, and sources.Metadata can be tracked for text, images, videos, audio, and underlying datasets.The implementation of p rovenance data tracking techniques can help assess the authenticity, integrity, intellectual property rights , and potential manipulations in digital content .Some well -known techniques for provenance data tracking include digital watermarking , metadata recording , digital ﬁngerprinting, and human authentication, among others .
Provenance Data Tracking Approaches Provenance data tracking techniques for GAI systems can be used to track the history and origin of data inputs, metadata, and synthetic content. Provenance data tracking records the origin and history for digital content, allowing its authenticity to be determined.It consists of techniques to record metadata as well as overt and covert digital watermarks on content.Data provenance refers to tracking the origin and history of input data through metadata and digital watermarking techniques.Provenance data tracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or control over the various trade -oﬀs and cascading impacts of early -stage model decisions on downstream performance and synthetic outputs.
For example, by selecting a watermarking model to prioritize robustness (the durability of a watermark) , an AI actor may inadvertently diminis h computational complexity ( the resources required to implement watermarking). Organizational risk management eﬀorts for enhancing content provenance include: • Tracking provenance of training data and metadata for GAI systems; • Documenting provenance data limitations within GAI systems; 52 • Monitoring system capabilities and limitations in deployment through rigorous TEVV processes; • Evaluati ng how humans engage, interact with, or adapt to GAI content (especially in decision making tasks informed by GAI content), and how they react to applied provenance techniques such as overt disclosures.Organizations can document and delineate GAI system objectives and limitations to identify gaps where provenance data may be most useful.
For instance, GAI systems used for content creation may require robust watermarking techniques and corresponding detectors to identify the source of content or metadata recording techniques and metadata management tools and repositories to trace content origins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can enable organizations to maximize the utility of provenance data and risk management eﬀorts.A.1.7.Enhancing Content Provenance through Structured Public Feedback While indirect feedback methods such as automated error collection systems are useful, they often lack the context and depth that direct input from end users can provide.Organizations can leverage feedback approaches described in the Pre-Deployment Testing section to capture input from external sources such as through AI red- teaming.
Integrating pre - and post -deployment external feedback into the monitoring process for GAI models and corresponding applications can help enhance awareness of performance changes and mitigate potential risks and harms from outputs . There are many ways to capture and make use of user feedback – before and after GAI systems and digital content transparency approaches are deployed – to gain insights about authentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques , and unintended consequences resulting from the utilization of content provenance approaches on users and communities .Furthermore, organizations can track and document the provenance of datasets to identify instances in which AI -generated data is a potential root cause of performance issues with the GAI system.A.1.8.
Incident Disclosure Overview AI incidents can be deﬁned as an “ event, circumstance, or series of events where the development, use, or malfunction of one or more AI systems directly or indirectly contribute s to one of the following harms: injury or harm to the health of a person or group s of people (including psychological harms and harms to mental health); disruption of the management and operation of critical infrastructure; violations of human rights or a breach of obligations under applicable law intended to protect fundamental, labor, and intellectual property rights ; or harm to property, communities, or the environment. ” AI incidents can occur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual).State of AI Incident Tracking and Disclosure Formal channels do not currently exist to report and document AI incidents.However, a number of publicly available databases have been created to document their occurrence.
These reporting channels make decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by amount of media coverage .53 Documenting, reporting, and sharing information about GAI incidents can help mitigate and prevent harmful outcomes by assisting relevant AI Actors in tracing impacts to their source .Greater awareness and standardization of GAI incident reporting could promote this transparency and improve GAI risk management across the AI ecosystem.Documentation and Involvement of AI Actors AI Actors should be aware of their roles in reporting AI incidents.To better understand previous incidents and implement measures to prevent similar ones in the future, organizations could consider developing guidelines for publicly available incident reporting which include information about AI actor responsibilities.These guidelines would help AI system operators identify GAI incidents across the AI lifecycle and with AI Actors regardless of role .
Documentation and review of third -party inputs and plugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM inputs and content delivered through these plugins is often distributed, with inconsistent or insuﬃcient access control. Documentation practices including logging, recording, and analyzing GAI incidents can facilitate smoother sharing of information with relevant AI Actors .Regular information sharing, change management records, version history and metadata can also empower AI Actors responding to and managing AI incidents.54 Appendix B.References Acemoglu, D.(2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 AI Incident Database.https://incidentdatabase.ai/ Atherton, D.(2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses.AI Incident Database.https://incidentdatabase.ai/blog/deepfakes -and-child -safety/ Badyal, N.et al.(2023) Intentional Biases in LLM Responses.arXiv .
https://arxiv.org/pdf/2311.07611 Bing Chat: Data Exﬁltration Exploit Explained. Embrace The Red.https://embracethered.com/blog/posts/2023/bing- chat -data -exﬁltration -poc- and-ﬁx/ Bommasani, R.et al.(2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?arXiv .https://arxiv.org/pdf/2211.13972 Boyarskaya, M.et al.(2020) Overcoming Failures of Imagination in AI Infused System Development and Deployment.arXiv .https://arxiv.org/pdf/2011.13416 Browne, D.et al.(2023) Securing the AI Pipeline.Mandiant .https://www.mandiant.com/resources/blog/securing- ai-pipeline Burgess, M.(2024) Generative AI’s Biggest Security Flaw Is Not Easy to Fix.WIRED.https://www.wired.com/story/generative -ai-prompt -injection -hacking/ Burtell, M.et al.(2024) The Surprising Power of Next Word Prediction: Large Language Models Explained, Part 1.Georgetown Center for Security and Emerging Technology .
https://cset.georgetown.edu/article/the -surprising- power -of-next -word -prediction- large -language - models -explained -part-1/ Canadian Centre for Cyber Security (2023) Generative artiﬁcial intelligence (AI) - ITSAP .00.041. https://www.cyber.gc.ca/en/guidance/generative -artiﬁcial -intelligence -ai-itsap00041 Carlini, N., et al.(2021) Extracting Training Data from Large Language Models.Usenix .https://www.usenix.org/conference/usenixsecurity21/presentation/carlini -extracting Carlini, N.et al.(2023) Quantifying Memorization Across Neural Language Models.ICLR 2023.https://arxiv.org/pdf/2202.07646 Carlini, N.et al.(2024) Stealing Part of a Production Language Model.arXiv .https://arxiv.org/abs/2403.06634 Chandra, B.et al.(2023) Dismantling the Disinformation Business of Chinese Inﬂuence Operations.RAND.https://www.rand.org/pubs/commentary/2023/10/dismantling- the-disinformation -business -of- chinese.html Ciriello , R.et al.
(2024) Ethical Tensions in Human- AI Companionship: A Dialectical Inquiry into Replika. ResearchGate.https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human- AI_Companionship_A_Dialectical_Inquiry_into_Replika Dahl, M.et al.(2024) Large Legal Fictions: Proﬁling Legal Hallucinations in Large Language Models.arXiv .https://arxiv.org/abs/2401.01301 55 De Angelo, D.(2024) Short, Mid and Long- Term Impacts of AI in Cybersecurity.Palo Alto Networks .https://www.paloaltonetworks.com/blog/2024/02/impacts -of-ai-in-cybersecurity/ De Freitas, J.et al.(2023) Chatbots and Mental Health: Insights into the Safety of Generative AI.Harvard Business School .https://www.hbs.edu/ris/Publication%20Files/23- 011_c1bdd417- f717- 47b6 -bccb - 5438c6e65c1a_f6fd9798- 3c2d- 4932- b222- 056231fe69d7.pdf Dietvorst, B.et al.(2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them Err.Journal of Experimental Psychology .
https://marketing.wharton.upenn.edu/wp- content/uploads/2016/10/Dietvorst -Simmons -Massey -2014.pdf Duhigg, C. (2012) How Companies Learn Your Secrets.New York Times .https://www.nytimes.com/2012/02/19/magazine/shopping- habits.html Elsayed, G.et al.(2024) Images altered to trick machine vision can inﬂuence humans too.Google DeepMind.https://deepmind.google/discover/blog/images -altered -to-trick -machine -vision -can- inﬂuence -humans -too/ Epstein, Z.et al.(2023).Art and the science of generative AI.Science .https://www.science.org/doi/10.1126/science.adh4451 Feﬀer, M.et al.(2024) Red- Teaming for Generative AI: Silver Bullet or Security Theater?arXiv.https://arxiv.org/pdf/2401.15897 Glazunov, S.et al.(2024) Project Naptime: Evaluating Oﬀensive Security Capabilities of Large Language Models.Project Zero.https://googleprojectzero.blogspot.com/2024/06/project -naptime.html Greshake, K.et al.
(2023) Not what you've signed up for: Compromising Real -World LLM -Integrated Applications with Indirect Prompt Injection. arXiv .https://arxiv.org/abs/2302.12173 Hagan, M.(2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to people’s legal problem stories.SSRN.https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 Haran, R.(2023) Securing LLM Systems Against Prompt Injection.NVIDIA.https://developer.nvidia.com/blog/securing- llm-systems -against -prompt -injection/ Information Technology Industry Council (2024) Authenticating AI -Generated Content .https://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf Jain, S.et al.(2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity.arXiv .https://arxiv.org/pdf/2305.08157 Ji, Z.et al ( 2023) Survey of Hallucination in Natural Language Generation.ACM Comput.Surv.55, 12, Article 248.https://doi.org/10.1145/3571730 Jones -Jang, S.et al.
(2022) How do people react to AI failure? Automation bias, algorithmic aversion, and perceived controllability.Oxford.https://academic.oup.com/jcmc/article/28/1/zmac029/6827859 ] Jussupow, E.et al.(2020) Why Are We Averse Towards Algorithms?A Comprehensive Literature Review on Algorithm Aversion.ECIS 2020.https://aisel.aisnet.org/ecis2020_rp/168/ Kalai, A., et al.(2024) Calibrated Language Models Must Hallucinate.arXiv.https://arxiv.org/pdf/2311.14648 56 Karasavva, V.et al.(2021) Personality, Attitudinal, and Demographic Predictors of Non- consensual Dissemination of Intimate Images.NIH.https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ Katzman, J., et al.(2023) Taxonomizing and measuring representational harms: a look at image tagging.AAAI.https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 Khan, T.et al.(2024) From Code to Consumer: PAI’s Value Chain Analysis Illuminates Generative AI’s Key Players.AI.
https://partnershiponai.org/from -code -to-consumer -pais-value -chain -analysis -illuminates - generative- ais-key-players/ Kirchenbauer, J. et al.(2023) A Watermark for Large Language Models.OpenReview .https://openreview.net/forum?id=aX8ig9X2a7 Kleinberg, J.et al.(May 2021) Algorithmic monoculture and social welfare.PNAS .https://www.pnas.org/doi/10.1073/pnas.2018340118 Lakatos, S.(2023) A Revealing Picture.Graphika.https://graphika.com/reports/a -revealing -picture Lee, H.et al.(2024) Deepfakes, Phrenology, Surveillance, and More!A Taxonomy of AI Privacy Risks.arXiv.https://arxiv.org/pdf/2310.07879 Lenaerts -Bergmans, B.(2024) Data Poisoning: The Exploitation of Generative AI.Crowdstrike .https://www.crowdstrike.com/cybersecurity -101/cyberattacks/data -poisoning/ Liang, W.et al.(2023) GPT detectors are biased against non -native English writers.arXiv .https://arxiv.org/abs/2304.02819 Luccioni, A.et al.(2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment?
arXiv . https://arxiv.org/pdf/2311.16863 Mouton, C.et al.(2024) The Operational Risks of AI in Large -Scale Biological Attacks.RAND .https://www.rand.org/pubs/research_reports/RRA2977- 2.html .Nicoletti, L.et al.(2023) Humans Are Biased.Generative Ai Is Even Worse.Bloomberg.https://www.bloomberg.com/graphics/2023 -generative- ai-bias/ .National Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/ﬁnal National Institute of Standards and Technology (2023) AI Risk Management Framework .https://www.nist.gov/itl/ai -risk-management -framework National Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI Risks and Trustworthiness.https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3- sec-characteristics National Institute of Standards and Technology (2023) AI Risk Management Framework , Chapter 6 : AI RMF Proﬁles.
https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Proﬁles/6 -sec-proﬁle National Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: Descriptions of AI Actor Tasks . https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors% 20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product 57 National Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: How AI Risks Diﬀer from Traditional Software Risks .https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B National Institute of Standards and Technology (2023) AI RMF Playbook .
https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook National Institue of Standards and Technology (2023) Framing Risk https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1- sec-risk National Institu te of Standards and Technology (2023) The Language of Trustworthy AI: An In- Depth Glossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary National Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing Bias in Artiﬁcial Intelligence https://www.nist.gov/publications/towards -standard -identifying -and- managing- bias-artiﬁcial -intelligence Northcutt, C. et al.(2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.arXiv .https://arxiv.org/pdf/2103.14749 OECD (2023) "Advancing accountability in AI: Governing and managing risks throughout the lifecycle for trustworthy AI", OECD Digital Economy Papers , No.349, OECD Publishing, Paris .
https://doi.org/10.1787/2448f04b- en OECD (2024) "Deﬁning AI incidents and related terms" OECD Artiﬁcial Intelligence Papers , No. 16, OECD Publishing, Paris .https://doi.org/10.1787/d1a8d965- en OpenAI (2023) GPT-4 System Card .https://cdn.openai.com/papers/gpt -4-system -card.pdf OpenAI (2024) GPT-4 Technical Report.https://arxiv.org/pdf/2303.08774 Padmakumar, V.et al.(2024) Does writing with language models reduce content diversity?ICLR .https://arxiv.org/pdf/2309.05196 Park, P.et.al.(2024) AI deception: A survey of examples, risks, and potential solutions.Patterns, 5(5).arXiv .https://arxiv.org/pdf/2308.14752 Partnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect Disclosure .https://partnershiponai.org/glossary -for-synthetic -media- transparency -methods -part-1- indirect -disclosure/ Qu, Y .et al.(2023) Unsafe Diﬀusion: On the Generation of Unsafe Images and Hateful Memes From Text - To-Image Models.arXiv .
https://arxiv.org/pdf/2305.13873 Rafat, K. et al.(2023) Mitigating carbon footprint for knowledge distillation based deep learning model compression.PLOS One .https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 Said, I.et al.(2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes in Victimization and Perpetration.Sage.https://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47- 08862605221122834 Sandbrink, J.(2023) Artiﬁcial intelligence and biological misuse: Diﬀerentiating risks of language models and biological design tools.arXiv .https://arxiv.org/pdf/2306.13952 58 Satariano, A.et al.(2023) The People Onscreen Are Fake.The Disinformation Is Real.New York Times .https://www.nytimes.com/2023/02/07/technology/artiﬁcial -intelligence -training- deepfake.html Schaul, K.et al.(2024) Inside the secret list of websites that make AI like ChatGPT sound smart.Washington Post .
https://www.washingtonpost.com/technology/interactive/2023/ai -chatbot -learning/ Scheurer , J. et al.(2023) Technical report: Large language models can strategically deceive their users when put under pressure.arXiv.https://arxiv.org/abs/2311.07590 Shelby, R.et al.(2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction.arXiv .https://arxiv.org/pdf/2210.05791 Shevlane, T.et al.(2023) Model evaluation for extreme risks.arXiv .https://arxiv.org/pdf/2305.15324 Shumailov, I.et al.(2023) The curse of recursion: training on generated data makes models forget.arXiv .https://arxiv.org/pdf/2305.17493v2 Smith, A.et al.(2023) Hallucination or Confabulation?Neuroanatomy as metaphor in Large Language Models.PLOS Digital Health.https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 Soice, E.et al.(2023) Can large language models democratize access to dual -use biotechnology?arXiv .https://arxiv.org/abs/2306.03809 Solaiman, I.et al.
(2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv.https://arxiv.org/abs/2302.04844 Staab, R.et al.(2023) Beyond Memorization: Violating Privacy via Inference With Large Language Models.arXiv .https://arxiv.org/pdf/2310.07298 Stanford, S.et al.(2023) Whose Opinions Do Language Models Reﬂect?arXiv .https://arxiv.org/pdf/2303.17548 Strubell, E.et al.(2019) Energy and Policy Considerations for Deep Learning in NLP .arXiv .https://arxiv.org/pdf/1906.02243 The White House (2016) Circular No.A -130, Managing Information as a Strategic Resource .https://www.whitehouse.gov/wp- content/uploads/legacy_drupal_ﬁles/omb/circulars/A130/a130revised.pdf The White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artiﬁcial Intelligence .
https://www.whitehouse.gov/brieﬁng- room/presidential - actions/2023/10/30/executive -order -on-the-safe-secure -and-trustworthy -development -and-use-of- artiﬁcial -intelligence/ The White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity Research and Development. https://www.whitehouse.gov/wp- content/uploads/2022/12/Roadmap- Information -Integrity -RD-2022.pdf?Thiel, D.(2023) Investigation Finds AI Image Generation Models Trained on Child Abuse.Stanford Cyber Policy Center .https://cyber.fsi.stanford.edu/news/investigation- ﬁnds -ai-image -generation -models - trained -child -abuse 59 Tirrell, L.(2017) Toxic Speech: Toward an Epidemiology of Discursive Harm.Philosophical Topics, 45(2) , 139- 162.https://www.jstor.org/stable/26529441 Tufekci, Z.(2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of Computational Agency .Colorado Technology Law Journal .
https://ctlj.colorado.edu/wp- content/uploads/2015/08/Tufekci -ﬁnal.pdf Turri, V. et al.(2023) Why We Need to Know More: Exploring the State of AI Incident Documentation Practices.AAAI/ACM Conference on AI, Ethics, and Society .https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 Urbina, F.et al.(2022) Dual use of artiﬁcial -intelligence -powered drug discovery.Nature Machine Intelligence.https://www.nature.com/articles/s42256- 022-00465- 9 Wang, X.et al.(2023) Energy and Carbon Considerations of Fine -Tuning BERT.ACL Anthology .https://aclanthology.org/2023.ﬁndings -emnlp.607.pdf Wang, Y .et al.(2023) Do -Not- Answer: A Dataset for Evaluating Safeguards in LLMs.arXiv .https://arxiv.org/pdf/2308.13387 Wardle, C.et al.(2017) Information Disorder: Toward an interdisciplinary framework for research and policy making.Council of Europe.https://rm.coe.int/information- disorder -toward -an-interdisciplinary - framework -for-researc/168076277c Weatherbed, J.
(2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge.https://www.theverge.com/2024/1/25/24050334/x -twitter -taylor -swift -ai-fake-images -trending Wei, J.et al.(2024) Long Form Factuality in Large Language Models.arXiv.https://arxiv.org/pdf/2403.18802 Weidinger, L.et al.(2021) Ethical and social risks of harm from Language Models.arXiv .https://arxiv.org/pdf/2112.04359 Weidinger, L.et al.(2023) Sociotechnical Safety Evaluation of Generative AI Systems.arXiv .https://arxiv.org/pdf/2310.11986 Weidinger, L.et al.(2022) Taxonomy of Risks posed by Language Models.FAccT ’22.https://dl.acm.org/doi/pdf/10.1145/3531146.3533088 West, D.(2023) AI poses disproportionate risks to women.Brookings.https://www.brookings.edu/articles/ai -poses -disproportionate -risks -to-women/ Wu, K.et al.(2024) How well do LLMs cite relevant medical references?An evaluation framework and analyses.arXiv .https://arxiv.org/pdf/2402.02008 Yin, L.et al.
(2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias.Bloomberg.https://www.bloomberg.com/graphics/2024 -openai -gpt-hiring- racial -discrimination/ Yu, Z.et al.( March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models.arXiv .https://arxiv.org/html/2403.17336v1 Zaugg, I.et al.(2022) Digitally -disadvantaged languages.Policy Review.https://policyreview.info/pdf/policyreview -2022-2- 1654.pdf 60 Zhang, Y .et al.(2023) Human favoritism, not AI aversion: People’s perceptions (and bias) toward generative AI, human experts, and human –GAI collaboration in persuasive content generation.Judgment and Decision Making.https://www.cambridge.org/core/journals/judgment -and-decision - making/article/human -favoritism -not- ai-aversion -peoples -perceptions -and-bias-toward -generative -ai- human- experts -and-humangai -collaboration -in-persuasive -content - generation/419C4BD9CE82673EAF1D8F6C350C4FA8 Zhang, Y .et al.
(2023) Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. arXiv .https://arxiv.org/pdf/2309.01219 Zhao, X.et al.(2023) Provable Robust Watermarking for AI -Generated Text.Semantic Scholar .https://www.semanticscholar.org/paper/Provable -Robust -Watermarking -for-AI-Generated -Text-Zhao - Ananth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc
